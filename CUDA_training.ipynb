{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CUDA_training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPARWvHqAZMF4iHHD0E4kuf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DNjRVavm5RGZ","colab_type":"text"},"source":["# Connect to colab"]},{"cell_type":"code","metadata":{"id":"zk26N0eZ5Xp7","colab_type":"code","outputId":"137a256b-5361-4860-dd0a-b9aba5b1af94","executionInfo":{"status":"ok","timestamp":1590132906195,"user_tz":360,"elapsed":54338,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd ../gdrive/'My Drive'/PARMA/SoilColor/GitRepo"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive/My Drive/PARMA/SoilColor/GitRepo\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dZ3I_ve0mPGS","colab_type":"text"},"source":["# Train Image model"]},{"cell_type":"code","metadata":{"id":"bMKOFr_hmN4y","colab_type":"code","outputId":"bc8ef5e4-eb58-4a1f-9d40-71b123c45695","executionInfo":{"status":"ok","timestamp":1590134531225,"user_tz":360,"elapsed":1679355,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python3 train.py -m image -a train -d ../Images/o_fused_definitive/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Selected image model for train, using the data stored in ../Images/o_fused_definitive/\n","Ingrese el número de epochs que desea entrenar: 1000\n","Ingrese el batch size: 64\n","Ingrese el lr: 0.0001\n","Image_Model(\n","  (loss): L1Loss()\n","  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n","  (relu1): ELU(alpha=1.0)\n","  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (relu2): ELU(alpha=1.0)\n","  (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=2048, out_features=500, bias=True)\n","  (sigm1): Sigmoid()\n","  (fc2): Linear(in_features=500, out_features=3, bias=True)\n","  (sigm2): Sigmoid()\n",")\n","Executing model on: cuda:0\n","Epoch #0\tTraining loss: 0.21811590451372562 \tValidation loss: 0.2867414085485625\n","Epoch #1\tTraining loss: 0.15918261698592914 \tValidation loss: 0.28716875690278354\n","Epoch #2\tTraining loss: 0.13260200413183987 \tValidation loss: 0.28350379508922946\n","Epoch #3\tTraining loss: 0.11420922666506705 \tValidation loss: 0.27617991302685846\n","Epoch #4\tTraining loss: 0.10162233564213694 \tValidation loss: 0.2699411926832558\n","Epoch #5\tTraining loss: 0.0959430102267765 \tValidation loss: 0.2613909720985675\n","Epoch #6\tTraining loss: 0.09286023785984757 \tValidation loss: 0.24470428954412166\n","Epoch #7\tTraining loss: 0.09027160914924957 \tValidation loss: 0.2238090180848556\n","Epoch #8\tTraining loss: 0.08723248300457317 \tValidation loss: 0.1999452969014734\n","Epoch #9\tTraining loss: 0.08492594086074241 \tValidation loss: 0.18431630689079676\n","Epoch #10\tTraining loss: 0.08317667193547265 \tValidation loss: 0.18077105040259184\n","Epoch #11\tTraining loss: 0.08173043101959603 \tValidation loss: 0.182069678886274\n","Epoch #12\tTraining loss: 0.08006160071604332 \tValidation loss: 0.1880671700949891\n","Epoch #13\tTraining loss: 0.07917056697001454 \tValidation loss: 0.19126195198286033\n","Epoch #14\tTraining loss: 0.07763962007734165 \tValidation loss: 0.19740722797594532\n","Epoch #15\tTraining loss: 0.07669413105412182 \tValidation loss: 0.20070809482054738\n","Epoch #16\tTraining loss: 0.07600132740498602 \tValidation loss: 0.21181718455670248\n","Epoch #17\tTraining loss: 0.07528633113654683 \tValidation loss: 0.21676800524917783\n","Epoch #18\tTraining loss: 0.07459867183797597 \tValidation loss: 0.22148251256658488\n","Epoch #19\tTraining loss: 0.07364146121562092 \tValidation loss: 0.22815315659198615\n","Epoch #20\tTraining loss: 0.07316330593650076 \tValidation loss: 0.2328029382036893\n","Epoch #21\tTraining loss: 0.07232852867549537 \tValidation loss: 0.23352715058460147\n","Epoch #22\tTraining loss: 0.07170803675207846 \tValidation loss: 0.23969083613647868\n","Epoch #23\tTraining loss: 0.07044878572143716 \tValidation loss: 0.2361374783145449\n","Epoch #24\tTraining loss: 0.07020045794459133 \tValidation loss: 0.24209304820796898\n","Epoch #25\tTraining loss: 0.06938144364867106 \tValidation loss: 0.24735009414804401\n","Epoch #26\tTraining loss: 0.0693748696894554 \tValidation loss: 0.2502228830313923\n","Epoch #27\tTraining loss: 0.06877004585556822 \tValidation loss: 0.2503337024755946\n","Epoch #28\tTraining loss: 0.06822240463450283 \tValidation loss: 0.255579256020335\n","Epoch #29\tTraining loss: 0.06811662732053894 \tValidation loss: 0.2511224468592793\n","Epoch #30\tTraining loss: 0.06804564941666946 \tValidation loss: 0.25016783168286894\n","Epoch #31\tTraining loss: 0.0674016307050161 \tValidation loss: 0.26543871829447435\n","Epoch #32\tTraining loss: 0.06689501352256687 \tValidation loss: 0.2633122792528247\n","Epoch #33\tTraining loss: 0.06733694199735465 \tValidation loss: 0.26491325812161354\n","Epoch #34\tTraining loss: 0.06714524986936889 \tValidation loss: 0.25361557625511355\n","Epoch #35\tTraining loss: 0.06683140318720057 \tValidation loss: 0.25502487203564844\n","Epoch #36\tTraining loss: 0.06622614970829001 \tValidation loss: 0.25684923554085465\n","Epoch #37\tTraining loss: 0.06554244174910007 \tValidation loss: 0.2515446976634828\n","Epoch #38\tTraining loss: 0.06545539151191809 \tValidation loss: 0.2534815190706204\n","Epoch #39\tTraining loss: 0.06437764195579196 \tValidation loss: 0.247857679347947\n","Epoch #40\tTraining loss: 0.06459655307943615 \tValidation loss: 0.24147171264774975\n","Epoch #41\tTraining loss: 0.06402006767102417 \tValidation loss: 0.23839683041577567\n","Epoch #42\tTraining loss: 0.06346118617217511 \tValidation loss: 0.2507672138739937\n","Epoch #43\tTraining loss: 0.06338566731884453 \tValidation loss: 0.17720679299992326\n","Epoch #44\tTraining loss: 0.0628507688095616 \tValidation loss: 0.19511747584878994\n","Epoch #45\tTraining loss: 0.06290431126528842 \tValidation loss: 0.2104827530537144\n","Epoch #46\tTraining loss: 0.06271746967804794 \tValidation loss: 0.22436947086777706\n","Epoch #47\tTraining loss: 0.0624717253866087 \tValidation loss: 0.23046793972361568\n","Epoch #48\tTraining loss: 0.06255216089717053 \tValidation loss: 0.23312282413595256\n","Epoch #49\tTraining loss: 0.06231067503508823 \tValidation loss: 0.23434963210444126\n","Epoch #50\tTraining loss: 0.06247710263708911 \tValidation loss: 0.22689188433094332\n","Epoch #51\tTraining loss: 0.06195958891122851 \tValidation loss: 0.22460306470114022\n","Epoch #52\tTraining loss: 0.06090886992366208 \tValidation loss: 0.23119173831572976\n","Epoch #53\tTraining loss: 0.06111691399857731 \tValidation loss: 0.2254689111349935\n","Epoch #54\tTraining loss: 0.06151430223987937 \tValidation loss: 0.23288669002361545\n","Epoch #55\tTraining loss: 0.061099671072041215 \tValidation loss: 0.23962372149078914\n","Epoch #56\tTraining loss: 0.06056558101174062 \tValidation loss: 0.2513311200261653\n","Epoch #57\tTraining loss: 0.0609891346335472 \tValidation loss: 0.25440362126073884\n","Epoch #58\tTraining loss: 0.061163258872143786 \tValidation loss: 0.24740600988729305\n","Epoch #59\tTraining loss: 0.060978465037285476 \tValidation loss: 0.2450505726974364\n","Epoch #60\tTraining loss: 0.060764356220973315 \tValidation loss: 0.25271244717676444\n","Epoch #61\tTraining loss: 0.059764755782890734 \tValidation loss: 0.25507360558380526\n","Epoch #62\tTraining loss: 0.05996341498601528 \tValidation loss: 0.25722884276150787\n","Epoch #63\tTraining loss: 0.06019062013729047 \tValidation loss: 0.26143157724597466\n","Epoch #64\tTraining loss: 0.060270900166586004 \tValidation loss: 0.25376106098360857\n","Epoch #65\tTraining loss: 0.05963890739069637 \tValidation loss: 0.2409236127832801\n","Epoch #66\tTraining loss: 0.05917376788717332 \tValidation loss: 0.24211221819220188\n","Epoch #67\tTraining loss: 0.05851773998025227 \tValidation loss: 0.23762406535942596\n","Epoch #68\tTraining loss: 0.058761166213291374 \tValidation loss: 0.24196798194316907\n","Epoch #69\tTraining loss: 0.058376633417718184 \tValidation loss: 0.24228041243271367\n","Epoch #70\tTraining loss: 0.05839244831227974 \tValidation loss: 0.2481796860150221\n","Epoch #71\tTraining loss: 0.05754963078184948 \tValidation loss: 0.2363149899158785\n","Epoch #72\tTraining loss: 0.05774581397528957 \tValidation loss: 0.23365909148331854\n","Epoch #73\tTraining loss: 0.05763470656785128 \tValidation loss: 0.2338757823095407\n","Epoch #74\tTraining loss: 0.057409090128942 \tValidation loss: 0.23571976919443333\n","Epoch #75\tTraining loss: 0.056911542289688644 \tValidation loss: 0.22578647051112077\n","Epoch #76\tTraining loss: 0.05723249629771936 \tValidation loss: 0.25988083973857595\n","Epoch #77\tTraining loss: 0.05706472673305115 \tValidation loss: 0.2660552862882808\n","Epoch #78\tTraining loss: 0.05644039776098606 \tValidation loss: 0.25341620504808476\n","Epoch #79\tTraining loss: 0.05624603423379724 \tValidation loss: 0.2504325059786745\n","Epoch #80\tTraining loss: 0.05623301937764531 \tValidation loss: 0.25963553833387665\n","Epoch #81\tTraining loss: 0.05666751466002999 \tValidation loss: 0.26739543000194554\n","Epoch #82\tTraining loss: 0.055985612985818625 \tValidation loss: 0.26361448021928674\n","Epoch #83\tTraining loss: 0.056006632080396565 \tValidation loss: 0.26163003956875264\n","Epoch #84\tTraining loss: 0.05566640390980289 \tValidation loss: 0.2763047015828043\n","Epoch #85\tTraining loss: 0.05536585061237612 \tValidation loss: 0.2697718534104172\n","Epoch #86\tTraining loss: 0.05549876083407355 \tValidation loss: 0.2701881112005839\n","Epoch #87\tTraining loss: 0.056320999087068765 \tValidation loss: 0.2440048364594753\n","Epoch #88\tTraining loss: 0.055761669521307844 \tValidation loss: 0.26308703314611537\n","Epoch #89\tTraining loss: 0.05538180272619272 \tValidation loss: 0.2751791064787716\n","Epoch #90\tTraining loss: 0.055264266474923926 \tValidation loss: 0.2581177022449866\n","Epoch #91\tTraining loss: 0.05494185386157674 \tValidation loss: 0.2531357006825008\n","Epoch #92\tTraining loss: 0.0543041311918863 \tValidation loss: 0.28161878129355233\n","Epoch #93\tTraining loss: 0.054225716810766135 \tValidation loss: 0.26311292032536876\n","Epoch #94\tTraining loss: 0.054187002943750284 \tValidation loss: 0.23364685387148296\n","Epoch #95\tTraining loss: 0.053668414249909625 \tValidation loss: 0.2479815084040795\n","Epoch #96\tTraining loss: 0.05319493817252865 \tValidation loss: 0.25994000981277593\n","Epoch #97\tTraining loss: 0.05258865160989497 \tValidation loss: 0.2641817148386185\n","Epoch #98\tTraining loss: 0.052801866143513586 \tValidation loss: 0.26932907317443705\n","Epoch #99\tTraining loss: 0.0531360407420981 \tValidation loss: 0.2598855846127485\n","Epoch #100\tTraining loss: 0.0533440773939148 \tValidation loss: 0.2516866654500458\n","Epoch #101\tTraining loss: 0.053413570096472275 \tValidation loss: 0.2541634595448127\n","Epoch #102\tTraining loss: 0.05334153440625091 \tValidation loss: 0.25883640787210527\n","Epoch #103\tTraining loss: 0.05263337904404417 \tValidation loss: 0.2617139939497558\n","Epoch #104\tTraining loss: 0.052184884412376634 \tValidation loss: 0.27196664157758155\n","Epoch #105\tTraining loss: 0.05265179350800027 \tValidation loss: 0.2805931917953567\n","Epoch #106\tTraining loss: 0.0521197750045234 \tValidation loss: 0.18058180503624943\n","Epoch #107\tTraining loss: 0.051575714585525546 \tValidation loss: 0.18437635119357731\n","Epoch #108\tTraining loss: 0.051406866313713974 \tValidation loss: 0.21751930016079\n","Epoch #109\tTraining loss: 0.051069964331673504 \tValidation loss: 0.23324813669561228\n","Epoch #110\tTraining loss: 0.0511716922162745 \tValidation loss: 0.23907584350130923\n","Epoch #111\tTraining loss: 0.0514751480557948 \tValidation loss: 0.24717976447726797\n","Epoch #112\tTraining loss: 0.05140491792866016 \tValidation loss: 0.28693884822985205\n","Epoch #113\tTraining loss: 0.05082915659836791 \tValidation loss: 0.28001446106771766\n","Epoch #114\tTraining loss: 0.05030461215004428 \tValidation loss: 0.27720628806568365\n","Epoch #115\tTraining loss: 0.050799339284069676 \tValidation loss: 0.2775909076551172\n","Epoch #116\tTraining loss: 0.050093775012551335 \tValidation loss: 0.27929549333651976\n","Epoch #117\tTraining loss: 0.04941834044784603 \tValidation loss: 0.2768713185201851\n","Epoch #118\tTraining loss: 0.049185396370030124 \tValidation loss: 0.2683803610164076\n","Epoch #119\tTraining loss: 0.04943890569089056 \tValidation loss: 0.26311452667825797\n","Epoch #120\tTraining loss: 0.04955678540951356 \tValidation loss: 0.26036080741810436\n","Epoch #121\tTraining loss: 0.04992914989562245 \tValidation loss: 0.15631603248759757\n","Epoch #122\tTraining loss: 0.05054969293881156 \tValidation loss: 0.1883047516271399\n","Epoch #123\tTraining loss: 0.05041763140401427 \tValidation loss: 0.19742728761472667\n","Epoch #124\tTraining loss: 0.04952896502633961 \tValidation loss: 0.21110093196573726\n","Epoch #125\tTraining loss: 0.04918593157452143 \tValidation loss: 0.24049058175089735\n","Epoch #126\tTraining loss: 0.049082118639373465 \tValidation loss: 0.2474879633407524\n","Epoch #127\tTraining loss: 0.049072163805039745 \tValidation loss: 0.24767818743903294\n","Epoch #128\tTraining loss: 0.04904119690831983 \tValidation loss: 0.24069297595437564\n","Epoch #129\tTraining loss: 0.048588536308517755 \tValidation loss: 0.23329063285026566\n","Epoch #130\tTraining loss: 0.04890284298714578 \tValidation loss: 0.19873343749088276\n","Epoch #131\tTraining loss: 0.04875856466985337 \tValidation loss: 0.1827044957630545\n","Epoch #132\tTraining loss: 0.048014461865859184 \tValidation loss: 0.2236549402361426\n","Epoch #133\tTraining loss: 0.047601289146698304 \tValidation loss: 0.2544490379127243\n","Epoch #134\tTraining loss: 0.04740198783460574 \tValidation loss: 0.25841887186203577\n","Epoch #135\tTraining loss: 0.04727941132746655 \tValidation loss: 0.24590506216392535\n","Epoch #136\tTraining loss: 0.04691689565584253 \tValidation loss: 0.23538314153015233\n","Epoch #137\tTraining loss: 0.04758350517611872 \tValidation loss: 0.2383718247663707\n","Epoch #138\tTraining loss: 0.048264613089691974 \tValidation loss: 0.2311026949843216\n","Epoch #139\tTraining loss: 0.04737572538953692 \tValidation loss: 0.22985964560400468\n","Epoch #140\tTraining loss: 0.04707996489045304 \tValidation loss: 0.2306634478320076\n","Epoch #141\tTraining loss: 0.04608942247378314 \tValidation loss: 0.26276038594799395\n","Epoch #142\tTraining loss: 0.04668136352110578 \tValidation loss: 0.21554987722340155\n","Epoch #143\tTraining loss: 0.046214527761695955 \tValidation loss: 0.2259365325183158\n","Epoch #144\tTraining loss: 0.04619652879137607 \tValidation loss: 0.15671887266652712\n","Epoch #145\tTraining loss: 0.04690623283106954 \tValidation loss: 0.15037346579179134\n","Epoch #146\tTraining loss: 0.04661297523079788 \tValidation loss: 0.18583250450148414\n","Epoch #147\tTraining loss: 0.0458881665829213 \tValidation loss: 0.1921373269061794\n","Epoch #148\tTraining loss: 0.045857110089177366 \tValidation loss: 0.20564325303522468\n","Epoch #149\tTraining loss: 0.045928794417583484 \tValidation loss: 0.18113710626700474\n","Epoch #150\tTraining loss: 0.04574515836240889 \tValidation loss: 0.19102011527450713\n","Epoch #151\tTraining loss: 0.04571699811300024 \tValidation loss: 0.22307574531632085\n","Epoch #152\tTraining loss: 0.04549350535694978 \tValidation loss: 0.2249432969708565\n","Epoch #153\tTraining loss: 0.04471963768910735 \tValidation loss: 0.20986314536466913\n","Epoch #154\tTraining loss: 0.04462249110492662 \tValidation loss: 0.20606150604095158\n","Epoch #155\tTraining loss: 0.04546683501162803 \tValidation loss: 0.24029067793887993\n","Epoch #156\tTraining loss: 0.04535606294980063 \tValidation loss: 0.10419205872543937\n","Epoch #157\tTraining loss: 0.04482160857183154 \tValidation loss: 0.0734694358586719\n","Epoch #158\tTraining loss: 0.04445721449974023 \tValidation loss: 0.11729105235105428\n","Epoch #159\tTraining loss: 0.044250081098847385 \tValidation loss: 0.16186248920089744\n","Epoch #160\tTraining loss: 0.043725428460529854 \tValidation loss: 0.19300309997052065\n","Epoch #161\tTraining loss: 0.04369358494838699 \tValidation loss: 0.2040035021867229\n","Epoch #162\tTraining loss: 0.043431714712623457 \tValidation loss: 0.20035220450209662\n","Epoch #163\tTraining loss: 0.04374837560713707 \tValidation loss: 0.2019073976105163\n","Epoch #164\tTraining loss: 0.0439840676503661 \tValidation loss: 0.2022532765023262\n","Epoch #165\tTraining loss: 0.043312575992777304 \tValidation loss: 0.19738744451986276\n","Epoch #166\tTraining loss: 0.043062367660629916 \tValidation loss: 0.18255672235686324\n","Epoch #167\tTraining loss: 0.042980242361558736 \tValidation loss: 0.15657099926715728\n","Epoch #168\tTraining loss: 0.043051728093448746 \tValidation loss: 0.16452472666263487\n","Epoch #169\tTraining loss: 0.04278433546013439 \tValidation loss: 0.15616273587092874\n","Epoch #170\tTraining loss: 0.04284087022417167 \tValidation loss: 0.15061384495893854\n","Epoch #171\tTraining loss: 0.042626081037754684 \tValidation loss: 0.14732186718980944\n","Epoch #172\tTraining loss: 0.04254689443152222 \tValidation loss: 0.16444678197701418\n","Epoch #173\tTraining loss: 0.04276064279343923 \tValidation loss: 0.18453830065388618\n","Epoch #174\tTraining loss: 0.04288237833295984 \tValidation loss: 0.18255763402284408\n","Epoch #175\tTraining loss: 0.04242406962249803 \tValidation loss: 0.1690881888315135\n","Epoch #176\tTraining loss: 0.04221497435066202 \tValidation loss: 0.17070762520841615\n","Epoch #177\tTraining loss: 0.04251873100642981 \tValidation loss: 0.1940933612692067\n","Epoch #178\tTraining loss: 0.04251270656326001 \tValidation loss: 0.18126541311196026\n","Epoch #179\tTraining loss: 0.04234110435143627 \tValidation loss: 0.14522247662443083\n","Epoch #180\tTraining loss: 0.04239864155179905 \tValidation loss: 0.12185174592743582\n","Epoch #181\tTraining loss: 0.04253446794247436 \tValidation loss: 0.10841064133588463\n","Epoch #182\tTraining loss: 0.04228907044556322 \tValidation loss: 0.13802419878878522\n","Epoch #183\tTraining loss: 0.04171496373929555 \tValidation loss: 0.18613374687649029\n","Epoch #184\tTraining loss: 0.041250645599275854 \tValidation loss: 0.18023114725640466\n","Epoch #185\tTraining loss: 0.04213640165920039 \tValidation loss: 0.17869809370743553\n","Epoch #186\tTraining loss: 0.04104882591683865 \tValidation loss: 0.15588228412551736\n","Epoch #187\tTraining loss: 0.04049358968339783 \tValidation loss: 0.11064873458744755\n","Epoch #188\tTraining loss: 0.04071821292014816 \tValidation loss: 0.09838771357663649\n","Epoch #189\tTraining loss: 0.04099242320247235 \tValidation loss: 0.1111646670561581\n","Epoch #190\tTraining loss: 0.04069568365871407 \tValidation loss: 0.11523561046454575\n","Epoch #191\tTraining loss: 0.04015218869639823 \tValidation loss: 0.0874928881665771\n","Epoch #192\tTraining loss: 0.04043388004249271 \tValidation loss: 0.089342747560513\n","Epoch #193\tTraining loss: 0.04020513763918279 \tValidation loss: 0.08419288407398742\n","Epoch #194\tTraining loss: 0.04043074929086475 \tValidation loss: 0.08311870535659269\n","Epoch #195\tTraining loss: 0.04089757834095521 \tValidation loss: 0.08163303925951092\n","Epoch #196\tTraining loss: 0.041040122582676655 \tValidation loss: 0.14368445874977626\n","Epoch #197\tTraining loss: 0.04056720822362075 \tValidation loss: 0.12486845373723911\n","Epoch #198\tTraining loss: 0.040195845395257696 \tValidation loss: 0.09704391131062298\n","Epoch #199\tTraining loss: 0.04030621334742123 \tValidation loss: 0.08578630815245797\n","Epoch #200\tTraining loss: 0.04124767274372158 \tValidation loss: 0.09702565351405298\n","Epoch #201\tTraining loss: 0.04213300428667108 \tValidation loss: 0.11176497372842384\n","Epoch #202\tTraining loss: 0.04234071782662931 \tValidation loss: 0.05766761133540135\n","Epoch #203\tTraining loss: 0.04122099689460426 \tValidation loss: 0.058951826637439614\n","Epoch #204\tTraining loss: 0.04023516048109371 \tValidation loss: 0.06756285699591266\n","Epoch #205\tTraining loss: 0.039589887288421124 \tValidation loss: 0.062095637948176956\n","Epoch #206\tTraining loss: 0.03959536608194283 \tValidation loss: 0.0818537428709446\n","Epoch #207\tTraining loss: 0.03968072734333424 \tValidation loss: 0.08740276142535984\n","Epoch #208\tTraining loss: 0.03952073204406581 \tValidation loss: 0.09665344856045917\n","Epoch #209\tTraining loss: 0.03933395127678754 \tValidation loss: 0.08719261602463353\n","Epoch #210\tTraining loss: 0.03903229857966525 \tValidation loss: 0.08662845087989456\n","Epoch #211\tTraining loss: 0.038750030406030776 \tValidation loss: 0.08241684178102866\n","Epoch #212\tTraining loss: 0.03915292800065294 \tValidation loss: 0.10275349221432106\n","Epoch #213\tTraining loss: 0.03897167420663826 \tValidation loss: 0.09447867653315799\n","Epoch #214\tTraining loss: 0.03853489551140122 \tValidation loss: 0.08424179572782278\n","Epoch #215\tTraining loss: 0.03780596064395801 \tValidation loss: 0.07959531632232968\n","Epoch #216\tTraining loss: 0.03760483179167695 \tValidation loss: 0.08501981572690538\n","Epoch #217\tTraining loss: 0.037841498544294264 \tValidation loss: 0.09993122840291091\n","Epoch #218\tTraining loss: 0.037676110849198106 \tValidation loss: 0.09708478624223817\n","Epoch #219\tTraining loss: 0.038487747160261945 \tValidation loss: 0.10138156187279394\n","Epoch #220\tTraining loss: 0.037977009643791484 \tValidation loss: 0.08663386634729925\n","Epoch #221\tTraining loss: 0.03768062361693997 \tValidation loss: 0.06977591273326128\n","Epoch #222\tTraining loss: 0.03865090076151586 \tValidation loss: 0.06121387876202711\n","Epoch #223\tTraining loss: 0.03819918758974346 \tValidation loss: 0.06244884273451296\n","Epoch #224\tTraining loss: 0.03849834922849875 \tValidation loss: 0.06936877930145131\n","Epoch #225\tTraining loss: 0.03900223549897277 \tValidation loss: 0.061328574463004694\n","Epoch #226\tTraining loss: 0.03913307695482958 \tValidation loss: 0.06807123669848604\n","Epoch #227\tTraining loss: 0.03796016420525724 \tValidation loss: 0.06611822363556134\n","Epoch #228\tTraining loss: 0.03769678701810566 \tValidation loss: 0.064075845841965\n","Epoch #229\tTraining loss: 0.03763176928395525 \tValidation loss: 0.05748343374457535\n","Epoch #230\tTraining loss: 0.03784834083143894 \tValidation loss: 0.05785663771154747\n","Epoch #231\tTraining loss: 0.038383484112336254 \tValidation loss: 0.07426175191966329\n","Epoch #232\tTraining loss: 0.03938240671154742 \tValidation loss: 0.1253293630167825\n","Epoch #233\tTraining loss: 0.0389584364986042 \tValidation loss: 0.05393665089701201\n","Epoch #234\tTraining loss: 0.03812541427795787 \tValidation loss: 0.19922754709438126\n","Epoch #235\tTraining loss: 0.03747602590641447 \tValidation loss: 0.11238048981111749\n","Epoch #236\tTraining loss: 0.03672552280476099 \tValidation loss: 0.06367955318133357\n","Epoch #237\tTraining loss: 0.03639824494552356 \tValidation loss: 0.05665202021616521\n","Epoch #238\tTraining loss: 0.03591556832501645 \tValidation loss: 0.05506277943270878\n","Epoch #239\tTraining loss: 0.03597124435570607 \tValidation loss: 0.08601817596167771\n","Epoch #240\tTraining loss: 0.03648507172106017 \tValidation loss: 0.2753024829069472\n","Epoch #241\tTraining loss: 0.03628387396386008 \tValidation loss: 0.20337780326731045\n","Epoch #242\tTraining loss: 0.03675587917781023 \tValidation loss: 0.12642974715727057\n","Epoch #243\tTraining loss: 0.03631572298916956 \tValidation loss: 0.07365572749397754\n","Epoch #244\tTraining loss: 0.036954311223997574 \tValidation loss: 0.07004403176199928\n","Epoch #245\tTraining loss: 0.03690542637550409 \tValidation loss: 0.07821327430128525\n","Epoch #246\tTraining loss: 0.037496475451906014 \tValidation loss: 0.08198134910002311\n","Epoch #247\tTraining loss: 0.03869170239022518 \tValidation loss: 0.08923690866115583\n","Epoch #248\tTraining loss: 0.03747239969358818 \tValidation loss: 0.05335631853526345\n","Epoch #249\tTraining loss: 0.03684689699724762 \tValidation loss: 0.06152198733002593\n","Epoch #250\tTraining loss: 0.03662857165139601 \tValidation loss: 0.06621400521016843\n","Epoch #251\tTraining loss: 0.035992345440454585 \tValidation loss: 0.07476279739146595\n","Epoch #252\tTraining loss: 0.035917323991875014 \tValidation loss: 0.05724236972532705\n","Epoch #253\tTraining loss: 0.03564135018206405 \tValidation loss: 0.058458714452277225\n","Epoch #254\tTraining loss: 0.035524022937334025 \tValidation loss: 0.06824867207567274\n","Epoch #255\tTraining loss: 0.035678993363827564 \tValidation loss: 0.057384624344295684\n","Epoch #256\tTraining loss: 0.03498107795309256 \tValidation loss: 0.053971638027721335\n","Epoch #257\tTraining loss: 0.03491458374298133 \tValidation loss: 0.06573932655169441\n","Epoch #258\tTraining loss: 0.03487744652890662 \tValidation loss: 0.059124274350160315\n","Epoch #259\tTraining loss: 0.035178948145744925 \tValidation loss: 0.05280523799561947\n","Epoch #260\tTraining loss: 0.035125634149273874 \tValidation loss: 0.05788656965292474\n","Epoch #261\tTraining loss: 0.034664754286667376 \tValidation loss: 0.05850415027293849\n","Epoch #262\tTraining loss: 0.03493889705158607 \tValidation loss: 0.055120314312431626\n","Epoch #263\tTraining loss: 0.03506450404205771 \tValidation loss: 0.054721862944946055\n","Epoch #264\tTraining loss: 0.0352813036727956 \tValidation loss: 0.05190166405420998\n","Epoch #265\tTraining loss: 0.035593300662613715 \tValidation loss: 0.05497063604905444\n","Epoch #266\tTraining loss: 0.03565692253977714 \tValidation loss: 0.07035430706962016\n","Epoch #267\tTraining loss: 0.035054203570439445 \tValidation loss: 0.08545416864690059\n","Epoch #268\tTraining loss: 0.035086784964596725 \tValidation loss: 0.08151443799879252\n","Epoch #269\tTraining loss: 0.03477816361363769 \tValidation loss: 0.1064914727479915\n","Epoch #270\tTraining loss: 0.03460265302592053 \tValidation loss: 0.07399027750517292\n","Epoch #271\tTraining loss: 0.03449900113012882 \tValidation loss: 0.09692459821194567\n","Epoch #272\tTraining loss: 0.03509128257043244 \tValidation loss: 0.0780544672608028\n","Epoch #273\tTraining loss: 0.03502794038131869 \tValidation loss: 0.056486407565645366\n","Epoch #274\tTraining loss: 0.03565551522030707 \tValidation loss: 0.05848811789020238\n","Epoch #275\tTraining loss: 0.035881119586496105 \tValidation loss: 0.0576702978934699\n","Epoch #276\tTraining loss: 0.036991824944725325 \tValidation loss: 0.0783435718316121\n","Epoch #277\tTraining loss: 0.037027572904799355 \tValidation loss: 0.07661437501411206\n","Epoch #278\tTraining loss: 0.03480378011986792 \tValidation loss: 0.05148828737084168\n","Epoch #279\tTraining loss: 0.03407485223175012 \tValidation loss: 0.051864177271505454\n","Epoch #280\tTraining loss: 0.03386475415563967 \tValidation loss: 0.05479602401021743\n","Epoch #281\tTraining loss: 0.03348911856795571 \tValidation loss: 0.09091210296200335\n","Epoch #282\tTraining loss: 0.03412532754914651 \tValidation loss: 0.1441860826771061\n","Epoch #283\tTraining loss: 0.03376941885236079 \tValidation loss: 0.14255683916018977\n","Epoch #284\tTraining loss: 0.03424140924605148 \tValidation loss: 0.07775549355632387\n","Epoch #285\tTraining loss: 0.03456381452856468 \tValidation loss: 0.05238311878092223\n","Epoch #286\tTraining loss: 0.03395411837360397 \tValidation loss: 0.0589957893538624\n","Epoch #287\tTraining loss: 0.0342425740161907 \tValidation loss: 0.05596152995791688\n","Epoch #288\tTraining loss: 0.034186883699155665 \tValidation loss: 0.06212826333919098\n","Epoch #289\tTraining loss: 0.03410619277740567 \tValidation loss: 0.06273902794529124\n","Epoch #290\tTraining loss: 0.03416548697273011 \tValidation loss: 0.06478423837221821\n","Epoch #291\tTraining loss: 0.03417852691369551 \tValidation loss: 0.058832047924621836\n","Epoch #292\tTraining loss: 0.034176980730432724 \tValidation loss: 0.0555744679700389\n","Epoch #293\tTraining loss: 0.0344821088739927 \tValidation loss: 0.05737165149683839\n","Epoch #294\tTraining loss: 0.03438931605628308 \tValidation loss: 0.05654659419432727\n","Epoch #295\tTraining loss: 0.03540705748356603 \tValidation loss: 0.055985638149386854\n","Epoch #296\tTraining loss: 0.03452734862263638 \tValidation loss: 0.058075066401936035\n","Epoch #297\tTraining loss: 0.03419178136739236 \tValidation loss: 0.055600728385378655\n","Epoch #298\tTraining loss: 0.03423163095404433 \tValidation loss: 0.06683222055410543\n","Epoch #299\tTraining loss: 0.033786189081923435 \tValidation loss: 0.09900410159520663\n","Epoch #300\tTraining loss: 0.03348024020297007 \tValidation loss: 0.09707910054078124\n","Epoch #301\tTraining loss: 0.03331250977595291 \tValidation loss: 0.05856533673626686\n","Epoch #302\tTraining loss: 0.033083364670396034 \tValidation loss: 0.050756038004284264\n","Epoch #303\tTraining loss: 0.03268244665727752 \tValidation loss: 0.05361785131609134\n","Epoch #304\tTraining loss: 0.033311656491565575 \tValidation loss: 0.05116256208019567\n","Epoch #305\tTraining loss: 0.03369569137422576 \tValidation loss: 0.06303132093428565\n","Epoch #306\tTraining loss: 0.033861536856174934 \tValidation loss: 0.054404386763341174\n","Epoch #307\tTraining loss: 0.033286014510275776 \tValidation loss: 0.0690342580208297\n","Epoch #308\tTraining loss: 0.033294889214954486 \tValidation loss: 0.06323624191205096\n","Epoch #309\tTraining loss: 0.033997612658657035 \tValidation loss: 0.09623441309211721\n","Epoch #310\tTraining loss: 0.034517284082774165 \tValidation loss: 0.06800456118576434\n","Epoch #311\tTraining loss: 0.03351281021006194 \tValidation loss: 0.058800092644289155\n","Epoch #312\tTraining loss: 0.03424362786258356 \tValidation loss: 0.0859030821254085\n","Epoch #313\tTraining loss: 0.03396174012938817 \tValidation loss: 0.061575178064693835\n","Epoch #314\tTraining loss: 0.0336525998812923 \tValidation loss: 0.062378528561168635\n","Epoch #315\tTraining loss: 0.03332954997212052 \tValidation loss: 0.07310072437997066\n","Epoch #316\tTraining loss: 0.03323959236194338 \tValidation loss: 0.08299859048048658\n","Epoch #317\tTraining loss: 0.032566395377833034 \tValidation loss: 0.0698343432208768\n","Epoch #318\tTraining loss: 0.032082342464497926 \tValidation loss: 0.053916920441702376\n","Epoch #319\tTraining loss: 0.0320595399459309 \tValidation loss: 0.0542889334306731\n","Epoch #320\tTraining loss: 0.033008361302607245 \tValidation loss: 0.07793549461538633\n","Epoch #321\tTraining loss: 0.032353256669956774 \tValidation loss: 0.12219736370082207\n","Epoch #322\tTraining loss: 0.03204619883813745 \tValidation loss: 0.08967665712722399\n","Epoch #323\tTraining loss: 0.0330924765907614 \tValidation loss: 0.10022903291169703\n","Epoch #324\tTraining loss: 0.031825046383293996 \tValidation loss: 0.08636438991646866\n","Epoch #325\tTraining loss: 0.032502104636346854 \tValidation loss: 0.0604841294802476\n","Epoch #326\tTraining loss: 0.03270197162928999 \tValidation loss: 0.05240250080427422\n","Epoch #327\tTraining loss: 0.03238257845960467 \tValidation loss: 0.051934763233133084\n","Epoch #328\tTraining loss: 0.03292102525411507 \tValidation loss: 0.05529487073599885\n","Epoch #329\tTraining loss: 0.032883578511704176 \tValidation loss: 0.06871223479474341\n","Epoch #330\tTraining loss: 0.03292574324911414 \tValidation loss: 0.07090170181886574\n","Epoch #331\tTraining loss: 0.03386104636433043 \tValidation loss: 0.07161153489196294\n","Epoch #332\tTraining loss: 0.033382713292077246 \tValidation loss: 0.08369347957948434\n","Epoch #333\tTraining loss: 0.03284789060268794 \tValidation loss: 0.05900475609451338\n","Epoch #334\tTraining loss: 0.032750729993838115 \tValidation loss: 0.0538709314468031\n","Epoch #335\tTraining loss: 0.03306082847965781 \tValidation loss: 0.0873634512489029\n","Epoch #336\tTraining loss: 0.03298711503157655 \tValidation loss: 0.0511489867893426\n","Epoch #337\tTraining loss: 0.03262354093193868 \tValidation loss: 0.0495338012023822\n","Epoch #338\tTraining loss: 0.03234295859151641 \tValidation loss: 0.07653784385141123\n","Epoch #339\tTraining loss: 0.03163600391213029 \tValidation loss: 0.05079911422813091\n","Epoch #340\tTraining loss: 0.03160210806547776 \tValidation loss: 0.05362622136795304\n","Epoch #341\tTraining loss: 0.03148942341772483 \tValidation loss: 0.05161284993160992\n","Epoch #342\tTraining loss: 0.03144846610228069 \tValidation loss: 0.056437105471650005\n","Epoch #343\tTraining loss: 0.03152987251660411 \tValidation loss: 0.10204073924125166\n","Epoch #344\tTraining loss: 0.03192879753354953 \tValidation loss: 0.12361941594052304\n","Epoch #345\tTraining loss: 0.03184888323009981 \tValidation loss: 0.07635405931466287\n","Epoch #346\tTraining loss: 0.031594593274294404 \tValidation loss: 0.07382451451607557\n","Epoch #347\tTraining loss: 0.03140911084922217 \tValidation loss: 0.054525239682664074\n","Epoch #348\tTraining loss: 0.031230236575679284 \tValidation loss: 0.05406539102763137\n","Epoch #349\tTraining loss: 0.031017140063959454 \tValidation loss: 0.06877612884373048\n","Epoch #350\tTraining loss: 0.03147681395133448 \tValidation loss: 0.05199115343696742\n","Epoch #351\tTraining loss: 0.031346503106096914 \tValidation loss: 0.05396525300386016\n","Epoch #352\tTraining loss: 0.03116376984415361 \tValidation loss: 0.05299684621993518\n","Epoch #353\tTraining loss: 0.030882133957787893 \tValidation loss: 0.22291528207097291\n","Epoch #354\tTraining loss: 0.031238876572236157 \tValidation loss: 0.19705517537245534\n","Epoch #355\tTraining loss: 0.03192127251124799 \tValidation loss: 0.09616395148020809\n","Epoch #356\tTraining loss: 0.031618432969732733 \tValidation loss: 0.06006174945329703\n","Epoch #357\tTraining loss: 0.0324339204545671 \tValidation loss: 0.07284752384101455\n","Epoch #358\tTraining loss: 0.0325449783136129 \tValidation loss: 0.06331246566637533\n","Epoch #359\tTraining loss: 0.033139900162321 \tValidation loss: 0.0722915331717116\n","Epoch #360\tTraining loss: 0.03194903704291586 \tValidation loss: 0.06527303501259503\n","Epoch #361\tTraining loss: 0.03183337970986588 \tValidation loss: 0.05779788822971395\n","Epoch #362\tTraining loss: 0.03200654306032872 \tValidation loss: 0.06803765683341953\n","Epoch #363\tTraining loss: 0.03224236117208031 \tValidation loss: 0.05727174868371608\n","Epoch #364\tTraining loss: 0.03214457983416721 \tValidation loss: 0.0632931574592054\n","Epoch #365\tTraining loss: 0.03146090444651779 \tValidation loss: 0.05207103298109737\n","Epoch #366\tTraining loss: 0.031277513900140094 \tValidation loss: 0.050334639110111594\n","Epoch #367\tTraining loss: 0.03136428249566872 \tValidation loss: 0.0717820420670191\n","Epoch #368\tTraining loss: 0.03082453362350787 \tValidation loss: 0.05256218840626739\n","Epoch #369\tTraining loss: 0.030697437615896345 \tValidation loss: 0.05242651688100164\n","Epoch #370\tTraining loss: 0.03132399389751476 \tValidation loss: 0.0652089099172027\n","Epoch #371\tTraining loss: 0.030626271994505988 \tValidation loss: 0.058165509759454434\n","Epoch #372\tTraining loss: 0.030298490621269963 \tValidation loss: 0.05747084464350345\n","Epoch #373\tTraining loss: 0.030346394364963954 \tValidation loss: 0.05799087331603807\n","Epoch #374\tTraining loss: 0.029979431118504464 \tValidation loss: 0.05235375810784438\n","Epoch #375\tTraining loss: 0.030494699493266775 \tValidation loss: 0.050528148908546\n","Epoch #376\tTraining loss: 0.03075942744003805 \tValidation loss: 0.05842211969860467\n","Epoch #377\tTraining loss: 0.03129603785435487 \tValidation loss: 0.057712429059752636\n","Epoch #378\tTraining loss: 0.03111374043080854 \tValidation loss: 0.05017073235146821\n","Epoch #379\tTraining loss: 0.03222112853690752 \tValidation loss: 0.07224268578002878\n","Epoch #380\tTraining loss: 0.03135861499377872 \tValidation loss: 0.05643198334759473\n","Epoch #381\tTraining loss: 0.03143070031029408 \tValidation loss: 0.06415202812419027\n","Epoch #382\tTraining loss: 0.030205258608600637 \tValidation loss: 0.06437861307117124\n","Epoch #383\tTraining loss: 0.030829279208701477 \tValidation loss: 0.0823774551751946\n","Epoch #384\tTraining loss: 0.02996469830008322 \tValidation loss: 0.054324815173125256\n","Epoch #385\tTraining loss: 0.030292961759030047 \tValidation loss: 0.061414866666127395\n","Epoch #386\tTraining loss: 0.029621946486815758 \tValidation loss: 0.05566170420502014\n","Epoch #387\tTraining loss: 0.030036116394978617 \tValidation loss: 0.05882946190171908\n","Epoch #388\tTraining loss: 0.029462591747306737 \tValidation loss: 0.051370763705775306\n","Epoch #389\tTraining loss: 0.02994619723838609 \tValidation loss: 0.0529824835780496\n","Epoch #390\tTraining loss: 0.029729858477547298 \tValidation loss: 0.05032001277933376\n","Epoch #391\tTraining loss: 0.029863400577664824 \tValidation loss: 0.05122984603979447\n","Epoch #392\tTraining loss: 0.03016663514585931 \tValidation loss: 0.05990285397911055\n","Epoch #393\tTraining loss: 0.02958382199751198 \tValidation loss: 0.06013190982177594\n","Epoch #394\tTraining loss: 0.030054351141264656 \tValidation loss: 0.05031954478442578\n","Epoch #395\tTraining loss: 0.02990400948277061 \tValidation loss: 0.05112739491096296\n","Epoch #396\tTraining loss: 0.029736595070460597 \tValidation loss: 0.056766887775485934\n","Epoch #397\tTraining loss: 0.02995862822896315 \tValidation loss: 0.05061996171649139\n","Epoch #398\tTraining loss: 0.029596630402220443 \tValidation loss: 0.050936854261144825\n","Epoch #399\tTraining loss: 0.029839010979792413 \tValidation loss: 0.05237281661363253\n","Epoch #400\tTraining loss: 0.030024947979894367 \tValidation loss: 0.054360652769577654\n","Epoch #401\tTraining loss: 0.029536416785314722 \tValidation loss: 0.06323888671883153\n","Epoch #402\tTraining loss: 0.030137806192164295 \tValidation loss: 0.09316053873562359\n","Epoch #403\tTraining loss: 0.02909936435332421 \tValidation loss: 0.05666367250691815\n","Epoch #404\tTraining loss: 0.02957864566356948 \tValidation loss: 0.06141748055488989\n","Epoch #405\tTraining loss: 0.028867933207960772 \tValidation loss: 0.059940746686034524\n","Epoch #406\tTraining loss: 0.02956555228854888 \tValidation loss: 0.05753581247480846\n","Epoch #407\tTraining loss: 0.029993034245575523 \tValidation loss: 0.0778531189692755\n","Epoch #408\tTraining loss: 0.030209421169300792 \tValidation loss: 0.06032577487838292\n","Epoch #409\tTraining loss: 0.029858360042766853 \tValidation loss: 0.0550619839974796\n","Epoch #410\tTraining loss: 0.029702359709549375 \tValidation loss: 0.06625863800153395\n","Epoch #411\tTraining loss: 0.029219075418089258 \tValidation loss: 0.07536894965410121\n","Epoch #412\tTraining loss: 0.029564441824177716 \tValidation loss: 0.04961283145242796\n","Epoch #413\tTraining loss: 0.029437483113327924 \tValidation loss: 0.05319969932136721\n","Epoch #414\tTraining loss: 0.02945038945363434 \tValidation loss: 0.05963742182834277\n","Epoch #415\tTraining loss: 0.02943566471804298 \tValidation loss: 0.0646984844030225\n","Epoch #416\tTraining loss: 0.029364756442360827 \tValidation loss: 0.05688134523606469\n","Epoch #417\tTraining loss: 0.03019409660254161 \tValidation loss: 0.0959988121176496\n","Epoch #418\tTraining loss: 0.03028980273880525 \tValidation loss: 0.07147901349561296\n","Epoch #419\tTraining loss: 0.030852782626975994 \tValidation loss: 0.06318297565405878\n","Epoch #420\tTraining loss: 0.030682833345515144 \tValidation loss: 0.060408721344121216\n","Epoch #421\tTraining loss: 0.03130295299506493 \tValidation loss: 0.06088676855871554\n","Epoch #422\tTraining loss: 0.03256914400738623 \tValidation loss: 0.06296442404402365\n","Epoch #423\tTraining loss: 0.03193692616286062 \tValidation loss: 0.05672585900877298\n","Epoch #424\tTraining loss: 0.031242968431027383 \tValidation loss: 0.05900991784724195\n","Epoch #425\tTraining loss: 0.03213342212449471 \tValidation loss: 0.05033739391261899\n","Epoch #426\tTraining loss: 0.03147166457957686 \tValidation loss: 0.05232395487210395\n","Epoch #427\tTraining loss: 0.03260148936944651 \tValidation loss: 0.05409191473735938\n","Epoch #428\tTraining loss: 0.030920497862134237 \tValidation loss: 0.061921374608233375\n","Epoch #429\tTraining loss: 0.031224900583068073 \tValidation loss: 0.07345306148782868\n","Epoch #430\tTraining loss: 0.02898826480875365 \tValidation loss: 0.05016087048484089\n","Epoch #431\tTraining loss: 0.029821029377457377 \tValidation loss: 0.06061011869644647\n","Epoch #432\tTraining loss: 0.02855909218127013 \tValidation loss: 0.05243185095653496\n","Epoch #433\tTraining loss: 0.02944288708486249 \tValidation loss: 0.06977980377542815\n","Epoch #434\tTraining loss: 0.028877667861547218 \tValidation loss: 0.05339055087843973\n","Epoch #435\tTraining loss: 0.029281053528490635 \tValidation loss: 0.05571683572857156\n","Epoch #436\tTraining loss: 0.028444036879881084 \tValidation loss: 0.050236031992006866\n","Epoch #437\tTraining loss: 0.029203179102049524 \tValidation loss: 0.06495662214120725\n","Epoch #438\tTraining loss: 0.028363681161760208 \tValidation loss: 0.05306736667510398\n","Epoch #439\tTraining loss: 0.029981513392394205 \tValidation loss: 0.0889375707975776\n","Epoch #440\tTraining loss: 0.029682277822482978 \tValidation loss: 0.0745819983146807\n","Epoch #441\tTraining loss: 0.02918998586133197 \tValidation loss: 0.05238501683535966\n","Epoch #442\tTraining loss: 0.02895992634244629 \tValidation loss: 0.05301877696919429\n","Epoch #443\tTraining loss: 0.02851369313340567 \tValidation loss: 0.05303188925027127\n","Epoch #444\tTraining loss: 0.028727658954250473 \tValidation loss: 0.0518857213606337\n","Epoch #445\tTraining loss: 0.029261832278007972 \tValidation loss: 0.05024396768746967\n","Epoch #446\tTraining loss: 0.029981306957259174 \tValidation loss: 0.07132031888291665\n","Epoch #447\tTraining loss: 0.029035503947963566 \tValidation loss: 0.05122884489717297\n","Epoch #448\tTraining loss: 0.029825162187110933 \tValidation loss: 0.062329424915506804\n","Epoch #449\tTraining loss: 0.03168666225275114 \tValidation loss: 0.11547664280058179\n","Epoch #450\tTraining loss: 0.03399017630397767 \tValidation loss: 0.10006748231612624\n","Epoch #451\tTraining loss: 0.03524170851031748 \tValidation loss: 0.0628524404638282\n","Epoch #452\tTraining loss: 0.03792791649601766 \tValidation loss: 0.08960168169705592\n","Epoch #453\tTraining loss: 0.037692952941656685 \tValidation loss: 0.10553097260309993\n","Epoch #454\tTraining loss: 0.0357223330652936 \tValidation loss: 0.059148165630347686\n","Epoch #455\tTraining loss: 0.0345365962414466 \tValidation loss: 0.056503378651768806\n","Epoch #456\tTraining loss: 0.03407528541590446 \tValidation loss: 0.05860355806849534\n","Epoch #457\tTraining loss: 0.031191828414335116 \tValidation loss: 0.10293901261180514\n","Epoch #458\tTraining loss: 0.03006511403658365 \tValidation loss: 0.1757857079317987\n","Epoch #459\tTraining loss: 0.02997717663721073 \tValidation loss: 0.14398410798871866\n","Epoch #460\tTraining loss: 0.029651223797117286 \tValidation loss: 0.08680610392997841\n","Epoch #461\tTraining loss: 0.029134977830985658 \tValidation loss: 0.05940644715684743\n","Epoch #462\tTraining loss: 0.029865987878645178 \tValidation loss: 0.061805338038368594\n","Epoch #463\tTraining loss: 0.031338600837171514 \tValidation loss: 0.06312277066917063\n","Epoch #464\tTraining loss: 0.030964330340819822 \tValidation loss: 0.05678866437012609\n","Epoch #465\tTraining loss: 0.030949001947112382 \tValidation loss: 0.0696086436780664\n","Epoch #466\tTraining loss: 0.031004571093370632 \tValidation loss: 0.07982509842801126\n","Epoch #467\tTraining loss: 0.029244690285869856 \tValidation loss: 0.06917047651052494\n","Epoch #468\tTraining loss: 0.029157104530119732 \tValidation loss: 0.06736005927691265\n","Epoch #469\tTraining loss: 0.028518515424890867 \tValidation loss: 0.050119245814626665\n","Epoch #470\tTraining loss: 0.028143281832918254 \tValidation loss: 0.05727027000055793\n","Epoch #471\tTraining loss: 0.02820243318370715 \tValidation loss: 0.05888884156558302\n","Epoch #472\tTraining loss: 0.029111945117831994 \tValidation loss: 0.05218867241583387\n","Epoch #473\tTraining loss: 0.029097331014670137 \tValidation loss: 0.0485253013048206\n","Epoch #474\tTraining loss: 0.028912320375393652 \tValidation loss: 0.052650734456957435\n","Epoch #475\tTraining loss: 0.029139954504368938 \tValidation loss: 0.05744061666470429\n","Epoch #476\tTraining loss: 0.029218229516190786 \tValidation loss: 0.04912062840414459\n","Epoch #477\tTraining loss: 0.028367605825417204 \tValidation loss: 0.05386665902245058\n","Epoch #478\tTraining loss: 0.028262964935253466 \tValidation loss: 0.05540605263711825\n","Epoch #479\tTraining loss: 0.027972298543737822 \tValidation loss: 0.05599338655742053\n","Epoch #480\tTraining loss: 0.027665836599783424 \tValidation loss: 0.06396056063902679\n","Epoch #481\tTraining loss: 0.0277188671871889 \tValidation loss: 0.049102759471839084\n","Epoch #482\tTraining loss: 0.02683852500640338 \tValidation loss: 0.05988645090836933\n","Epoch #483\tTraining loss: 0.026540226700460308 \tValidation loss: 0.05086764699181558\n","Epoch #484\tTraining loss: 0.026516940217305024 \tValidation loss: 0.0486484412678859\n","Epoch #485\tTraining loss: 0.026540435054890503 \tValidation loss: 0.049747007268531505\n","Epoch #486\tTraining loss: 0.02631209670923441 \tValidation loss: 0.04837317079819531\n","Epoch #487\tTraining loss: 0.026454426273436322 \tValidation loss: 0.051531879494398254\n","Epoch #488\tTraining loss: 0.026702349962021366 \tValidation loss: 0.0504662872622296\n","Epoch #489\tTraining loss: 0.027007182345294155 \tValidation loss: 0.06302996283102819\n","Epoch #490\tTraining loss: 0.02700830147923155 \tValidation loss: 0.06985480790044288\n","Epoch #491\tTraining loss: 0.026556140741060136 \tValidation loss: 0.04961692734793948\n","Epoch #492\tTraining loss: 0.02655574522250508 \tValidation loss: 0.07191287086075626\n","Epoch #493\tTraining loss: 0.02617376880099827 \tValidation loss: 0.05884360128681222\n","Epoch #494\tTraining loss: 0.026700755439918938 \tValidation loss: 0.049031973532930914\n","Epoch #495\tTraining loss: 0.02654721102222791 \tValidation loss: 0.0504626354461806\n","Epoch #496\tTraining loss: 0.027558323257333494 \tValidation loss: 0.05375796196803216\n","Epoch #497\tTraining loss: 0.02778543168168359 \tValidation loss: 0.05164803075893255\n","Epoch #498\tTraining loss: 0.028363706498215145 \tValidation loss: 0.05263629052973365\n","Epoch #499\tTraining loss: 0.0282903303418019 \tValidation loss: 0.07388868304016806\n","Epoch #500\tTraining loss: 0.028762587944530543 \tValidation loss: 0.08419867237170647\n","Epoch #501\tTraining loss: 0.027858334424321295 \tValidation loss: 0.05647994242140341\n","Epoch #502\tTraining loss: 0.026978961866540346 \tValidation loss: 0.060152882650664104\n","Epoch #503\tTraining loss: 0.026898350785666 \tValidation loss: 0.04790923163345997\n","Epoch #504\tTraining loss: 0.02657949501049172 \tValidation loss: 0.049255815132659926\n","Epoch #505\tTraining loss: 0.02656593928680997 \tValidation loss: 0.04922829713977174\n","Epoch #506\tTraining loss: 0.02712033281528855 \tValidation loss: 0.05996040509315518\n","Epoch #507\tTraining loss: 0.02652041291303651 \tValidation loss: 0.0497051355577186\n","Epoch #508\tTraining loss: 0.026496445208931664 \tValidation loss: 0.06253371873714193\n","Epoch #509\tTraining loss: 0.026992882624099407 \tValidation loss: 0.09806403903433669\n","Epoch #510\tTraining loss: 0.026820188744224836 \tValidation loss: 0.05853175685568538\n","Epoch #511\tTraining loss: 0.026784177129303293 \tValidation loss: 0.05987958113667733\n","Epoch #512\tTraining loss: 0.026543791103793197 \tValidation loss: 0.05128906293703694\n","Epoch #513\tTraining loss: 0.026877848672291503 \tValidation loss: 0.052931792511873965\n","Epoch #514\tTraining loss: 0.02690446073119997 \tValidation loss: 0.06821557974036245\n","Epoch #515\tTraining loss: 0.027102040380929088 \tValidation loss: 0.07434908567812669\n","Epoch #516\tTraining loss: 0.026779081297440242 \tValidation loss: 0.054911439495479046\n","Epoch #517\tTraining loss: 0.027462204905184594 \tValidation loss: 0.05914931789777669\n","Epoch #518\tTraining loss: 0.028155886343866142 \tValidation loss: 0.05580707988613124\n","Epoch #519\tTraining loss: 0.02844215141148234 \tValidation loss: 0.05328732992210286\n","Epoch #520\tTraining loss: 0.02771345060064312 \tValidation loss: 0.05193283355487998\n","Epoch #521\tTraining loss: 0.028412506221720486 \tValidation loss: 0.04961223638177469\n","Epoch #522\tTraining loss: 0.029222729745980374 \tValidation loss: 0.06778916295945564\n","Epoch #523\tTraining loss: 0.029726300351649432 \tValidation loss: 0.06157562396390485\n","Epoch #524\tTraining loss: 0.02816343160038165 \tValidation loss: 0.06752705237541662\n","Epoch #525\tTraining loss: 0.027947204778988606 \tValidation loss: 0.06176393168156334\n","Epoch #526\tTraining loss: 0.0269661099514705 \tValidation loss: 0.07738680980057953\n","Epoch #527\tTraining loss: 0.026023797869342176 \tValidation loss: 0.06536221070907947\n","Epoch #528\tTraining loss: 0.026024842006926934 \tValidation loss: 0.0673004565066603\n","Epoch #529\tTraining loss: 0.02557307114371742 \tValidation loss: 0.07159231162887493\n","Epoch #530\tTraining loss: 0.026062747796801632 \tValidation loss: 0.09019768997727068\n","Epoch #531\tTraining loss: 0.02654527846873985 \tValidation loss: 0.07458917298468097\n","Epoch #532\tTraining loss: 0.0262712285084723 \tValidation loss: 0.07002189022948216\n","Epoch #533\tTraining loss: 0.026087551239318985 \tValidation loss: 0.07166692091392138\n","Epoch #534\tTraining loss: 0.026121983024619655 \tValidation loss: 0.09488706287976305\n","Epoch #535\tTraining loss: 0.026725789054562955 \tValidation loss: 0.11986807607384481\n","Epoch #536\tTraining loss: 0.029158973364675343 \tValidation loss: 0.07508627626481705\n","Epoch #537\tTraining loss: 0.02996681087188229 \tValidation loss: 0.09015498150092138\n","Epoch #538\tTraining loss: 0.030222831074768468 \tValidation loss: 0.050697369103677545\n","Epoch #539\tTraining loss: 0.03017923418908629 \tValidation loss: 0.05645506732709728\n","Epoch #540\tTraining loss: 0.029325582429611603 \tValidation loss: 0.056981423300849425\n","Epoch #541\tTraining loss: 0.029363428298400863 \tValidation loss: 0.06726915645755975\n","Epoch #542\tTraining loss: 0.028368378997961442 \tValidation loss: 0.07684410048857247\n","Epoch #543\tTraining loss: 0.028006150936866515 \tValidation loss: 0.06592654104198092\n","Epoch #544\tTraining loss: 0.028245073504289103 \tValidation loss: 0.12353577649615115\n","Epoch #545\tTraining loss: 0.027897317000602558 \tValidation loss: 0.14868802212671778\n","Epoch #546\tTraining loss: 0.027914200614170992 \tValidation loss: 0.10719351345513917\n","Epoch #547\tTraining loss: 0.028641486200990558 \tValidation loss: 0.10681319130814064\n","Epoch #548\tTraining loss: 0.029682886649108464 \tValidation loss: 0.04855785642124021\n","Epoch #549\tTraining loss: 0.029838134956795694 \tValidation loss: 0.04777203195292421\n","Epoch #550\tTraining loss: 0.03053747060612621 \tValidation loss: 0.0473586093988115\n","Epoch #551\tTraining loss: 0.03052983649080322 \tValidation loss: 0.06560544924337175\n","Epoch #552\tTraining loss: 0.029783813635074197 \tValidation loss: 0.066494829118367\n","Epoch #553\tTraining loss: 0.029555384144868118 \tValidation loss: 0.05806377662194739\n","Epoch #554\tTraining loss: 0.02835540556716673 \tValidation loss: 0.05848811712468486\n","Epoch #555\tTraining loss: 0.026986691559935806 \tValidation loss: 0.04812648928500161\n","Epoch #556\tTraining loss: 0.025892898058763603 \tValidation loss: 0.0555850747471964\n","Epoch #557\tTraining loss: 0.025411925884667452 \tValidation loss: 0.047976643449143125\n","Epoch #558\tTraining loss: 0.02534145583451275 \tValidation loss: 0.047915092141300755\n","Epoch #559\tTraining loss: 0.025135996193656036 \tValidation loss: 0.05926475951418726\n","Epoch #560\tTraining loss: 0.02608680489716756 \tValidation loss: 0.05562310507637993\n","Epoch #561\tTraining loss: 0.025810422356399486 \tValidation loss: 0.05245693166131273\n","Epoch #562\tTraining loss: 0.02552352961355863 \tValidation loss: 0.048817276035232844\n","Epoch #563\tTraining loss: 0.02540645689679607 \tValidation loss: 0.050030604732566625\n","Epoch #564\tTraining loss: 0.02574662259808011 \tValidation loss: 0.04786536975335632\n","Epoch #565\tTraining loss: 0.025738031484206066 \tValidation loss: 0.0493058783440767\n","Epoch #566\tTraining loss: 0.025774012070839077 \tValidation loss: 0.04756320073703203\n","Epoch #567\tTraining loss: 0.026066137083137486 \tValidation loss: 0.04895359932322647\n","Epoch #568\tTraining loss: 0.02664126066604543 \tValidation loss: 0.04659667651430708\n","Epoch #569\tTraining loss: 0.027292081246936394 \tValidation loss: 0.04891567321165265\n","Epoch #570\tTraining loss: 0.02751966099949734 \tValidation loss: 0.060166139685474164\n","Epoch #571\tTraining loss: 0.028168716707782558 \tValidation loss: 0.06756819015781737\n","Epoch #572\tTraining loss: 0.02794573429728885 \tValidation loss: 0.07846765150026537\n","Epoch #573\tTraining loss: 0.026226601279918278 \tValidation loss: 0.08456025065103569\n","Epoch #574\tTraining loss: 0.02600224923218956 \tValidation loss: 0.08931243823455824\n","Epoch #575\tTraining loss: 0.025931269571527242 \tValidation loss: 0.1242615215960183\n","Epoch #576\tTraining loss: 0.026006367099329297 \tValidation loss: 0.0907137540978618\n","Epoch #577\tTraining loss: 0.026478854151662855 \tValidation loss: 0.08106472066184162\n","Epoch #578\tTraining loss: 0.0272314469114196 \tValidation loss: 0.053967504246047265\n","Epoch #579\tTraining loss: 0.027396009360834563 \tValidation loss: 0.05821882306968051\n","Epoch #580\tTraining loss: 0.028246038618738277 \tValidation loss: 0.053242104370712286\n","Epoch #581\tTraining loss: 0.028490271001645712 \tValidation loss: 0.05297281060789936\n","Epoch #582\tTraining loss: 0.028136815532763836 \tValidation loss: 0.1278714525142535\n","Epoch #583\tTraining loss: 0.028443373406375226 \tValidation loss: 0.07023868952912724\n","Epoch #584\tTraining loss: 0.02840483152347815 \tValidation loss: 0.05149926913629889\n","Epoch #585\tTraining loss: 0.027654083642415693 \tValidation loss: 0.06878012153980285\n","Epoch #586\tTraining loss: 0.026679193532071828 \tValidation loss: 0.05106216827118794\n","Epoch #587\tTraining loss: 0.026301415483813345 \tValidation loss: 0.0642055960131265\n","Epoch #588\tTraining loss: 0.025891107544117813 \tValidation loss: 0.06361227611280819\n","Epoch #589\tTraining loss: 0.02607551773837067 \tValidation loss: 0.0644163167000173\n","Epoch #590\tTraining loss: 0.025946522235645652 \tValidation loss: 0.05423664120501634\n","Epoch #591\tTraining loss: 0.025423182180944127 \tValidation loss: 0.05169715823554325\n","Epoch #592\tTraining loss: 0.025375347565792194 \tValidation loss: 0.04978021469658196\n","Epoch #593\tTraining loss: 0.02498925184419872 \tValidation loss: 0.04764900845392773\n","Epoch #594\tTraining loss: 0.025065629020741914 \tValidation loss: 0.04971679670071488\n","Epoch #595\tTraining loss: 0.02543979873690495 \tValidation loss: 0.04939223847960668\n","Epoch #596\tTraining loss: 0.024850189530127013 \tValidation loss: 0.06995674436617522\n","Epoch #597\tTraining loss: 0.024996082018792116 \tValidation loss: 0.06363780510584541\n","Epoch #598\tTraining loss: 0.0255914209545057 \tValidation loss: 0.057059084809915846\n","Epoch #599\tTraining loss: 0.025475986579770876 \tValidation loss: 0.050785834877922126\n","Epoch #600\tTraining loss: 0.02575819028084399 \tValidation loss: 0.05601754240395536\n","Epoch #601\tTraining loss: 0.02591183281425723 \tValidation loss: 0.05486176156472647\n","Epoch #602\tTraining loss: 0.02533253861548395 \tValidation loss: 0.0470417001050118\n","Epoch #603\tTraining loss: 0.02467906662436826 \tValidation loss: 0.04933795150403246\n","Epoch #604\tTraining loss: 0.024437793725682895 \tValidation loss: 0.0493006619181235\n","Epoch #605\tTraining loss: 0.02481268204888551 \tValidation loss: 0.06112774044224212\n","Epoch #606\tTraining loss: 0.025034432130602698 \tValidation loss: 0.053334702309072855\n","Epoch #607\tTraining loss: 0.024998420599273104 \tValidation loss: 0.08200868001404941\n","Epoch #608\tTraining loss: 0.024992628394303648 \tValidation loss: 0.047695323825136755\n","Epoch #609\tTraining loss: 0.02479492244077125 \tValidation loss: 0.04684289248670429\n","Epoch #610\tTraining loss: 0.024792132151223657 \tValidation loss: 0.05944921269555303\n","Epoch #611\tTraining loss: 0.024747832364935912 \tValidation loss: 0.050579676018808244\n","Epoch #612\tTraining loss: 0.024501705205787157 \tValidation loss: 0.04647418373214584\n","Epoch #613\tTraining loss: 0.024481342786808757 \tValidation loss: 0.04883590087035852\n","Epoch #614\tTraining loss: 0.02449243490082063 \tValidation loss: 0.05226419736881734\n","Epoch #615\tTraining loss: 0.023849038191367987 \tValidation loss: 0.050397391028191525\n","Epoch #616\tTraining loss: 0.02358884011859482 \tValidation loss: 0.04844168947533202\n","Epoch #617\tTraining loss: 0.02337110174897778 \tValidation loss: 0.047209416755745026\n","Epoch #618\tTraining loss: 0.024512536283543673 \tValidation loss: 0.05084078145741954\n","Epoch #619\tTraining loss: 0.0247758741053385 \tValidation loss: 0.04941239725965772\n","Epoch #620\tTraining loss: 0.024616481866256756 \tValidation loss: 0.05044925271499477\n","Epoch #621\tTraining loss: 0.024966631125196474 \tValidation loss: 0.05353460782994635\n","Epoch #622\tTraining loss: 0.02498761483976357 \tValidation loss: 0.08302033386107488\n","Epoch #623\tTraining loss: 0.024534626344347343 \tValidation loss: 0.1354946542610491\n","Epoch #624\tTraining loss: 0.024460828801742076 \tValidation loss: 0.06279394226169535\n","Epoch #625\tTraining loss: 0.024863165968785655 \tValidation loss: 0.09399161079182085\n","Epoch #626\tTraining loss: 0.024458158710389595 \tValidation loss: 0.05456291456812126\n","Epoch #627\tTraining loss: 0.024236117903605713 \tValidation loss: 0.04801959958520547\n","Epoch #628\tTraining loss: 0.023971442852017348 \tValidation loss: 0.0588291227423709\n","Epoch #629\tTraining loss: 0.02373266532882473 \tValidation loss: 0.054527516087465866\n","Epoch #630\tTraining loss: 0.02337007318805625 \tValidation loss: 0.05120578318511065\n","Epoch #631\tTraining loss: 0.023775757321061484 \tValidation loss: 0.05149368836909948\n","Epoch #632\tTraining loss: 0.02401892686491773 \tValidation loss: 0.047917684775799704\n","Epoch #633\tTraining loss: 0.024319687577216444 \tValidation loss: 0.04771977698859112\n","Epoch #634\tTraining loss: 0.02488754478248572 \tValidation loss: 0.06978863519642516\n","Epoch #635\tTraining loss: 0.024832421641423106 \tValidation loss: 0.0884952425895775\n","Epoch #636\tTraining loss: 0.024770614589014743 \tValidation loss: 0.05709577540046236\n","Epoch #637\tTraining loss: 0.024429134634744176 \tValidation loss: 0.05174039223948882\n","Epoch #638\tTraining loss: 0.024793296751635142 \tValidation loss: 0.04762353826463899\n","Epoch #639\tTraining loss: 0.024254396664160526 \tValidation loss: 0.05150530052806592\n","Epoch #640\tTraining loss: 0.024088960385462096 \tValidation loss: 0.13980403422399607\n","Epoch #641\tTraining loss: 0.023863754358525117 \tValidation loss: 0.08268676660347707\n","Epoch #642\tTraining loss: 0.024111374021467725 \tValidation loss: 0.07010390178999386\n","Epoch #643\tTraining loss: 0.024253271959927367 \tValidation loss: 0.06697327972488556\n","Epoch #644\tTraining loss: 0.023295723209923985 \tValidation loss: 0.05027828510304055\n","Epoch #645\tTraining loss: 0.024009071918118706 \tValidation loss: 0.050400618356711155\n","Epoch #646\tTraining loss: 0.023808629269264667 \tValidation loss: 0.04522518506904006\n","Epoch #647\tTraining loss: 0.023887212026545562 \tValidation loss: 0.06485736601418818\n","Epoch #648\tTraining loss: 0.023685871104717565 \tValidation loss: 0.04924207861001021\n","Epoch #649\tTraining loss: 0.023569508724572846 \tValidation loss: 0.056057560873426115\n","Epoch #650\tTraining loss: 0.023462293639858123 \tValidation loss: 0.05320987760341055\n","Epoch #651\tTraining loss: 0.024312524442484877 \tValidation loss: 0.04878859778568821\n","Epoch #652\tTraining loss: 0.02485295771553751 \tValidation loss: 0.050970728705942846\n","Epoch #653\tTraining loss: 0.024509219012855413 \tValidation loss: 0.056265132754984036\n","Epoch #654\tTraining loss: 0.024643027121775372 \tValidation loss: 0.05387917569319177\n","Epoch #655\tTraining loss: 0.024230576442898936 \tValidation loss: 0.09202566252177681\n","Epoch #656\tTraining loss: 0.025485077673070212 \tValidation loss: 0.192080835166164\n","Epoch #657\tTraining loss: 0.026410978171482292 \tValidation loss: 0.20105161743068595\n","Epoch #658\tTraining loss: 0.025623722536410078 \tValidation loss: 0.09979938415232506\n","Epoch #659\tTraining loss: 0.025441323370617597 \tValidation loss: 0.049835804109985955\n","Epoch #660\tTraining loss: 0.025697700504807595 \tValidation loss: 0.08209813906659176\n","Epoch #661\tTraining loss: 0.02503409571761231 \tValidation loss: 0.058048443605996314\n","Epoch #662\tTraining loss: 0.02540493471320706 \tValidation loss: 0.05583199232658483\n","Epoch #663\tTraining loss: 0.025373857018275235 \tValidation loss: 0.052450415098331864\n","Epoch #664\tTraining loss: 0.024277309580810293 \tValidation loss: 0.04816664640962909\n","Epoch #665\tTraining loss: 0.02498931665320029 \tValidation loss: 0.046962068192694856\n","Epoch #666\tTraining loss: 0.02721387805229799 \tValidation loss: 0.05253415070682256\n","Epoch #667\tTraining loss: 0.02809653157223363 \tValidation loss: 0.07405228885657493\n","Epoch #668\tTraining loss: 0.026161967402828078 \tValidation loss: 0.06276609930862301\n","Epoch #669\tTraining loss: 0.026377584157148214 \tValidation loss: 0.06298864896642024\n","Epoch #670\tTraining loss: 0.026311623257633903 \tValidation loss: 0.06405748248843539\n","Epoch #671\tTraining loss: 0.025887013574753693 \tValidation loss: 0.05220484442328851\n","Epoch #672\tTraining loss: 0.027044850049146463 \tValidation loss: 0.05297728459116397\n","Epoch #673\tTraining loss: 0.026265776769180172 \tValidation loss: 0.05110830437068748\n","Epoch #674\tTraining loss: 0.025946247185505956 \tValidation loss: 0.047314741403092726\n","Epoch #675\tTraining loss: 0.02657322938531647 \tValidation loss: 0.07352785633709971\n","Epoch #676\tTraining loss: 0.026594245406522738 \tValidation loss: 0.047820102616411286\n","Epoch #677\tTraining loss: 0.027557456143497257 \tValidation loss: 0.062081321028924254\n","Epoch #678\tTraining loss: 0.02845823278926462 \tValidation loss: 0.0813362278574528\n","Epoch #679\tTraining loss: 0.02780678691326844 \tValidation loss: 0.07619309089490484\n","Epoch #680\tTraining loss: 0.026427291010064656 \tValidation loss: 0.05348114533687035\n","Epoch #681\tTraining loss: 0.024525620130709764 \tValidation loss: 0.05543166170696988\n","Epoch #682\tTraining loss: 0.023747690092311882 \tValidation loss: 0.05171697663711603\n","Epoch #683\tTraining loss: 0.024113415960184358 \tValidation loss: 0.04822929832896778\n","Epoch #684\tTraining loss: 0.023541069070287055 \tValidation loss: 0.04631023020902408\n","Epoch #685\tTraining loss: 0.02323795930281089 \tValidation loss: 0.04753360746433385\n","Epoch #686\tTraining loss: 0.024121266740912497 \tValidation loss: 0.048195850432159515\n","Epoch #687\tTraining loss: 0.02424508441901601 \tValidation loss: 0.051588823571016074\n","Epoch #688\tTraining loss: 0.02430287001947903 \tValidation loss: 0.051571107296679665\n","Epoch #689\tTraining loss: 0.02502150438615508 \tValidation loss: 0.05425254247795773\n","Epoch #690\tTraining loss: 0.025239808865969972 \tValidation loss: 0.057817571790468036\n","Epoch #691\tTraining loss: 0.02456976924111652 \tValidation loss: 0.07424442648374552\n","Epoch #692\tTraining loss: 0.023983044108054245 \tValidation loss: 0.12469685329925459\n","Epoch #693\tTraining loss: 0.02343776348667776 \tValidation loss: 0.14277731581539746\n","Epoch #694\tTraining loss: 0.02395472278682414 \tValidation loss: 0.11751641191918731\n","Epoch #695\tTraining loss: 0.02450090834427198 \tValidation loss: 0.07788735244169971\n","Epoch #696\tTraining loss: 0.02420278603204948 \tValidation loss: 0.053562042875755486\n","Epoch #697\tTraining loss: 0.02430243489701805 \tValidation loss: 0.06512641590949224\n","Epoch #698\tTraining loss: 0.024230574833065947 \tValidation loss: 0.05036506069902562\n","Epoch #699\tTraining loss: 0.02499127386131269 \tValidation loss: 0.0508708253430279\n","Epoch #700\tTraining loss: 0.024906090887660912 \tValidation loss: 0.05389532616212772\n","Epoch #701\tTraining loss: 0.02574035393346203 \tValidation loss: 0.05830038893312221\n","Epoch #702\tTraining loss: 0.025246913787367257 \tValidation loss: 0.1023190738854677\n","Epoch #703\tTraining loss: 0.024797604510361896 \tValidation loss: 0.0792769461522332\n","Epoch #704\tTraining loss: 0.024089304441346666 \tValidation loss: 0.051010966338293565\n","Epoch #705\tTraining loss: 0.024971042233675696 \tValidation loss: 0.05700725297202981\n","Epoch #706\tTraining loss: 0.024502555486714116 \tValidation loss: 0.05230568693729173\n","Epoch #707\tTraining loss: 0.02383886918696954 \tValidation loss: 0.051241807248401265\n","Epoch #708\tTraining loss: 0.024103084558746594 \tValidation loss: 0.05744070364516377\n","Epoch #709\tTraining loss: 0.024259444234691854 \tValidation loss: 0.052598408307097624\n","Epoch #710\tTraining loss: 0.024916931058825764 \tValidation loss: 0.06291100891401064\n","Epoch #711\tTraining loss: 0.02571651928101565 \tValidation loss: 0.05015302319031732\n","Epoch #712\tTraining loss: 0.026521912546194747 \tValidation loss: 0.06976648441260402\n","Epoch #713\tTraining loss: 0.02573335695753528 \tValidation loss: 0.05505383780234134\n","Epoch #714\tTraining loss: 0.024970890026855444 \tValidation loss: 0.056155279262709516\n","Epoch #715\tTraining loss: 0.023565330585146177 \tValidation loss: 0.045675510476522435\n","Epoch #716\tTraining loss: 0.023463203489499238 \tValidation loss: 0.06194338828314315\n","Epoch #717\tTraining loss: 0.022722026612189715 \tValidation loss: 0.057973519819720945\n","Epoch #718\tTraining loss: 0.022590145817587608 \tValidation loss: 0.055791546916165155\n","Epoch #719\tTraining loss: 0.02267270245578138 \tValidation loss: 0.047016191542781284\n","Epoch #720\tTraining loss: 0.02266455822279994 \tValidation loss: 0.06043139734357548\n","Epoch #721\tTraining loss: 0.022389619353044225 \tValidation loss: 0.051590276873368424\n","Epoch #722\tTraining loss: 0.022431584688625345 \tValidation loss: 0.045703649174493224\n","Epoch #723\tTraining loss: 0.022596791554864208 \tValidation loss: 0.0513809102181651\n","Epoch #724\tTraining loss: 0.022404974974506428 \tValidation loss: 0.055428369058117884\n","Epoch #725\tTraining loss: 0.02250504767858773 \tValidation loss: 0.05670154353203597\n","Epoch #726\tTraining loss: 0.023324897901254353 \tValidation loss: 0.051382705735420635\n","Epoch #727\tTraining loss: 0.02294391911583099 \tValidation loss: 0.05215991996684456\n","Epoch #728\tTraining loss: 0.023611283542674836 \tValidation loss: 0.051966975756742\n","Epoch #729\tTraining loss: 0.023365286582701626 \tValidation loss: 0.04789586286984813\n","Epoch #730\tTraining loss: 0.022626807478365464 \tValidation loss: 0.05389737908523948\n","Epoch #731\tTraining loss: 0.022215829958901777 \tValidation loss: 0.04892199234761967\n","Epoch #732\tTraining loss: 0.02251354437360139 \tValidation loss: 0.046114900892075386\n","Epoch #733\tTraining loss: 0.022019556573109382 \tValidation loss: 0.045386717314859854\n","Epoch #734\tTraining loss: 0.022102277415158363 \tValidation loss: 0.05282045961426208\n","Epoch #735\tTraining loss: 0.02230348375118794 \tValidation loss: 0.04777690907873043\n","Epoch #736\tTraining loss: 0.022523130556104113 \tValidation loss: 0.057811827379338925\n","Epoch #737\tTraining loss: 0.023203846586435976 \tValidation loss: 0.049015992251432924\n","Epoch #738\tTraining loss: 0.022914051310580153 \tValidation loss: 0.06984122087961678\n","Epoch #739\tTraining loss: 0.023026399508578838 \tValidation loss: 0.05795350915601468\n","Epoch #740\tTraining loss: 0.02315132379674422 \tValidation loss: 0.060200452858071914\n","Epoch #741\tTraining loss: 0.02244577032956486 \tValidation loss: 0.06528513913810603\n","Epoch #742\tTraining loss: 0.022243394144282294 \tValidation loss: 0.05733302009602598\n","Epoch #743\tTraining loss: 0.022344178139237583 \tValidation loss: 0.047257082560190095\n","Epoch #744\tTraining loss: 0.0228211168300309 \tValidation loss: 0.048791585243610304\n","Epoch #745\tTraining loss: 0.02326251302802773 \tValidation loss: 0.04872466792689649\n","Epoch #746\tTraining loss: 0.023393798573666403 \tValidation loss: 0.05283892369691669\n","Epoch #747\tTraining loss: 0.02318898167454353 \tValidation loss: 0.04894256557393163\n","Epoch #748\tTraining loss: 0.02307461882604521 \tValidation loss: 0.05192181770835288\n","Epoch #749\tTraining loss: 0.022476550502055187 \tValidation loss: 0.05157680971904644\n","Epoch #750\tTraining loss: 0.022810846896423608 \tValidation loss: 0.045723636532463086\n","Epoch #751\tTraining loss: 0.02233333610967122 \tValidation loss: 0.05297921549319537\n","Epoch #752\tTraining loss: 0.02212261468125712 \tValidation loss: 0.06866771242838837\n","Epoch #753\tTraining loss: 0.02253962176386067 \tValidation loss: 0.04812962691317036\n","Epoch #754\tTraining loss: 0.022385494979154492 \tValidation loss: 0.055701635733593324\n","Epoch #755\tTraining loss: 0.021636455423616873 \tValidation loss: 0.05038306933875373\n","Epoch #756\tTraining loss: 0.022174577393154482 \tValidation loss: 0.05138016540059467\n","Epoch #757\tTraining loss: 0.022076858589051375 \tValidation loss: 0.045708044865110965\n","Epoch #758\tTraining loss: 0.022728938192189487 \tValidation loss: 0.04690543639849636\n","Epoch #759\tTraining loss: 0.02235251174519092 \tValidation loss: 0.04446690189887291\n","Epoch #760\tTraining loss: 0.022481827320331483 \tValidation loss: 0.06024349743959466\n","Epoch #761\tTraining loss: 0.02221292532683476 \tValidation loss: 0.04703207320352126\n","Epoch #762\tTraining loss: 0.022850120039532875 \tValidation loss: 0.04981742340868605\n","Epoch #763\tTraining loss: 0.022646842160997126 \tValidation loss: 0.05071478433645852\n","Epoch #764\tTraining loss: 0.022514388213034933 \tValidation loss: 0.06405060392908331\n","Epoch #765\tTraining loss: 0.023502763717322142 \tValidation loss: 0.06032271453373538\n","Epoch #766\tTraining loss: 0.02301185185501038 \tValidation loss: 0.09652898664390364\n","Epoch #767\tTraining loss: 0.023469555741418527 \tValidation loss: 0.08776091520578976\n","Epoch #768\tTraining loss: 0.02275553859072817 \tValidation loss: 0.07112732132628492\n","Epoch #769\tTraining loss: 0.022985830540996316 \tValidation loss: 0.051531303747351036\n","Epoch #770\tTraining loss: 0.022693937965645603 \tValidation loss: 0.059041685929815034\n","Epoch #771\tTraining loss: 0.023533057616562762 \tValidation loss: 0.04964360237040991\n","Epoch #772\tTraining loss: 0.02294876317481358 \tValidation loss: 0.05971561010265167\n","Epoch #773\tTraining loss: 0.023025636014289218 \tValidation loss: 0.051836169709119403\n","Epoch #774\tTraining loss: 0.0230768909906298 \tValidation loss: 0.0510523355974394\n","Epoch #775\tTraining loss: 0.02212811971372216 \tValidation loss: 0.05240391075086197\n","Epoch #776\tTraining loss: 0.021813345844927744 \tValidation loss: 0.08502367867764615\n","Epoch #777\tTraining loss: 0.02286452431440566 \tValidation loss: 0.09737650155663344\n","Epoch #778\tTraining loss: 0.022750585788596962 \tValidation loss: 0.07194589596240764\n","Epoch #779\tTraining loss: 0.023040875911082778 \tValidation loss: 0.09900279879368654\n","Epoch #780\tTraining loss: 0.02257313129199113 \tValidation loss: 0.07624503472545292\n","Epoch #781\tTraining loss: 0.022878705899325585 \tValidation loss: 0.057908029997312836\n","Epoch #782\tTraining loss: 0.023507352723475722 \tValidation loss: 0.0476308566180194\n","Epoch #783\tTraining loss: 0.024200351554143407 \tValidation loss: 0.06108067555436155\n","Epoch #784\tTraining loss: 0.02326667016141898 \tValidation loss: 0.05351805149861988\n","Epoch #785\tTraining loss: 0.02343178074688044 \tValidation loss: 0.17155447818629224\n","Epoch #786\tTraining loss: 0.02224581559602239 \tValidation loss: 0.13588712132626657\n","Epoch #787\tTraining loss: 0.021580248075943217 \tValidation loss: 0.07639928189758956\n","Epoch #788\tTraining loss: 0.022668343731851958 \tValidation loss: 0.053829649005132825\n","Epoch #789\tTraining loss: 0.023298401796074498 \tValidation loss: 0.08992903889618241\n","Epoch #790\tTraining loss: 0.02253359305252188 \tValidation loss: 0.06344080993119831\n","Epoch #791\tTraining loss: 0.023722361813947105 \tValidation loss: 0.06150130483089818\n","Epoch #792\tTraining loss: 0.024641233694926205 \tValidation loss: 0.046640290932181444\n","Epoch #793\tTraining loss: 0.026119649957252845 \tValidation loss: 0.14325764174483363\n","Epoch #794\tTraining loss: 0.026189130661772253 \tValidation loss: 0.07014090608278245\n","Epoch #795\tTraining loss: 0.027068769782131668 \tValidation loss: 0.07456719245292577\n","Epoch #796\tTraining loss: 0.026463909998171747 \tValidation loss: 0.11598382630891063\n","Epoch #797\tTraining loss: 0.026273985287977315 \tValidation loss: 0.06798211112477782\n","Epoch #798\tTraining loss: 0.02415041821914904 \tValidation loss: 0.04937371216066214\n","Epoch #799\tTraining loss: 0.022605009728444488 \tValidation loss: 0.06390659052490981\n","Epoch #800\tTraining loss: 0.02118753841053851 \tValidation loss: 0.055366991958213915\n","Epoch #801\tTraining loss: 0.02073562144707764 \tValidation loss: 0.049888427351186065\n","Epoch #802\tTraining loss: 0.02027561474354471 \tValidation loss: 0.05697458921956017\n","Epoch #803\tTraining loss: 0.020455464922716417 \tValidation loss: 0.0533322552662233\n","Epoch #804\tTraining loss: 0.021131947554941386 \tValidation loss: 0.04913660016293108\n","Epoch #805\tTraining loss: 0.02186640577489727 \tValidation loss: 0.04495311488957204\n","Epoch #806\tTraining loss: 0.021907852417307205 \tValidation loss: 0.046575902967913224\n","Epoch #807\tTraining loss: 0.021894955700306813 \tValidation loss: 0.053032194873487835\n","Epoch #808\tTraining loss: 0.0220826973364458 \tValidation loss: 0.05799979287381296\n","Epoch #809\tTraining loss: 0.021826312349669058 \tValidation loss: 0.05118290752206764\n","Epoch #810\tTraining loss: 0.02100640388428602 \tValidation loss: 0.0600942880809984\n","Epoch #811\tTraining loss: 0.021556547268392685 \tValidation loss: 0.04510156267251986\n","Epoch #812\tTraining loss: 0.021652747059018655 \tValidation loss: 0.05781646337032215\n","Epoch #813\tTraining loss: 0.022443010474718562 \tValidation loss: 0.048944555014681576\n","Epoch #814\tTraining loss: 0.023289959377215445 \tValidation loss: 0.05613517397837724\n","Epoch #815\tTraining loss: 0.023987170621298202 \tValidation loss: 0.054767850112022794\n","Epoch #816\tTraining loss: 0.024071872214140123 \tValidation loss: 0.04929416861251714\n","Epoch #817\tTraining loss: 0.023445439357261698 \tValidation loss: 0.05691910469266568\n","Epoch #818\tTraining loss: 0.022910910955177548 \tValidation loss: 0.049076939468136876\n","Epoch #819\tTraining loss: 0.02222543117000949 \tValidation loss: 0.05846008569638519\n","Epoch #820\tTraining loss: 0.02209780225824824 \tValidation loss: 0.05770531182185665\n","Epoch #821\tTraining loss: 0.022266086605098778 \tValidation loss: 0.057619446511694095\n","Epoch #822\tTraining loss: 0.022251744519171664 \tValidation loss: 0.050699413634680286\n","Epoch #823\tTraining loss: 0.0234829156883287 \tValidation loss: 0.05588177913192364\n","Epoch #824\tTraining loss: 0.024307238946035894 \tValidation loss: 0.04468820870295555\n","Epoch #825\tTraining loss: 0.02558288262089604 \tValidation loss: 0.07320259741686345\n","Epoch #826\tTraining loss: 0.026007620498076884 \tValidation loss: 0.053212926131728934\n","Epoch #827\tTraining loss: 0.026397999358311747 \tValidation loss: 0.07332517111113185\n","Epoch #828\tTraining loss: 0.024615010463765394 \tValidation loss: 0.050297875557179926\n","Epoch #829\tTraining loss: 0.023887526170066543 \tValidation loss: 0.13141316661383912\n","Epoch #830\tTraining loss: 0.02311968001867825 \tValidation loss: 0.17219818771088446\n","Epoch #831\tTraining loss: 0.02228563934666822 \tValidation loss: 0.08928898483006141\n","Epoch #832\tTraining loss: 0.022633685990430925 \tValidation loss: 0.08892684449664216\n","Epoch #833\tTraining loss: 0.02138363437176693 \tValidation loss: 0.05202344487494178\n","Epoch #834\tTraining loss: 0.021217893417669966 \tValidation loss: 0.04622827049953407\n","Epoch #835\tTraining loss: 0.02086888003980823 \tValidation loss: 0.052110697870510744\n","Epoch #836\tTraining loss: 0.020809333569351735 \tValidation loss: 0.0508035990507871\n","Epoch #837\tTraining loss: 0.020532424571086276 \tValidation loss: 0.05169615244397374\n","Epoch #838\tTraining loss: 0.021016683566952424 \tValidation loss: 0.051425680890166074\n","Epoch #839\tTraining loss: 0.02090133547454725 \tValidation loss: 0.05049691736437105\n","Epoch #840\tTraining loss: 0.02188942420346313 \tValidation loss: 0.1553721347735078\n","Epoch #841\tTraining loss: 0.022555114208293287 \tValidation loss: 0.14546362699565554\n","Epoch #842\tTraining loss: 0.024217184188684737 \tValidation loss: 0.07702873218923294\n","Epoch #843\tTraining loss: 0.024450935868421807 \tValidation loss: 0.058925858549786825\n","Epoch #844\tTraining loss: 0.023574655685584586 \tValidation loss: 0.05216973680780445\n","Epoch #845\tTraining loss: 0.023442116900839815 \tValidation loss: 0.07293713417486936\n","Epoch #846\tTraining loss: 0.02261101937786744 \tValidation loss: 0.09218827314894672\n","Epoch #847\tTraining loss: 0.024847304966583477 \tValidation loss: 0.08894709824932118\n","Epoch #848\tTraining loss: 0.024564374559906045 \tValidation loss: 0.09486390593877976\n","Epoch #849\tTraining loss: 0.024150950750043816 \tValidation loss: 0.08323166608608883\n","Epoch #850\tTraining loss: 0.023617022270191544 \tValidation loss: 0.06213215393449108\n","Epoch #851\tTraining loss: 0.022345909168982228 \tValidation loss: 0.0607405978428887\n","Epoch #852\tTraining loss: 0.022539142681830382 \tValidation loss: 0.062140885989345925\n","Epoch #853\tTraining loss: 0.02358115993599316 \tValidation loss: 0.046778735902716695\n","Epoch #854\tTraining loss: 0.02353268861080444 \tValidation loss: 0.04756582380263383\n","Epoch #855\tTraining loss: 0.02350769635280025 \tValidation loss: 0.04771572643303612\n","Epoch #856\tTraining loss: 0.023225316613022316 \tValidation loss: 0.04488886409764514\n","Epoch #857\tTraining loss: 0.022949231037105285 \tValidation loss: 0.05401324406851499\n","Epoch #858\tTraining loss: 0.021959953834974583 \tValidation loss: 0.04815293980732566\n","Epoch #859\tTraining loss: 0.021274770942175832 \tValidation loss: 0.045400088576584015\n","Epoch #860\tTraining loss: 0.020574029577682614 \tValidation loss: 0.048961716297722134\n","Epoch #861\tTraining loss: 0.019506817433883664 \tValidation loss: 0.04850528591766525\n","Epoch #862\tTraining loss: 0.019566651159052408 \tValidation loss: 0.049439873334978846\n","Epoch #863\tTraining loss: 0.019807316357423707 \tValidation loss: 0.0507568041121152\n","Epoch #864\tTraining loss: 0.01988622327250688 \tValidation loss: 0.05334112534181869\n","Epoch #865\tTraining loss: 0.020325092324776715 \tValidation loss: 0.052864567820415966\n","Epoch #866\tTraining loss: 0.021045945929485416 \tValidation loss: 0.04930692205202385\n","Epoch #867\tTraining loss: 0.021590355667084865 \tValidation loss: 0.04591712268978895\n","Epoch #868\tTraining loss: 0.02250520192811522 \tValidation loss: 0.051148404315222926\n","Epoch #869\tTraining loss: 0.023117179625367492 \tValidation loss: 0.047213688381476805\n","Epoch #870\tTraining loss: 0.021989224043368522 \tValidation loss: 0.0582530013624798\n","Epoch #871\tTraining loss: 0.02084927694877193 \tValidation loss: 0.04802271805037798\n","Epoch #872\tTraining loss: 0.02047020987297383 \tValidation loss: 0.05999743790775961\n","Epoch #873\tTraining loss: 0.019836770533756744 \tValidation loss: 0.09834449930592484\n","Epoch #874\tTraining loss: 0.01963757456119764 \tValidation loss: 0.0630962061530355\n","Epoch #875\tTraining loss: 0.019944754107745037 \tValidation loss: 0.06463444018346136\n","Epoch #876\tTraining loss: 0.020278047580108042 \tValidation loss: 0.061767815824062135\n","Epoch #877\tTraining loss: 0.020467577193967485 \tValidation loss: 0.052496334141017224\n","Epoch #878\tTraining loss: 0.019864584430183335 \tValidation loss: 0.0513210667623666\n","Epoch #879\tTraining loss: 0.019825185502548876 \tValidation loss: 0.04726040889667951\n","Epoch #880\tTraining loss: 0.01967537383259971 \tValidation loss: 0.0477433645624827\n","Epoch #881\tTraining loss: 0.01977748085218465 \tValidation loss: 0.04459337084212314\n","Epoch #882\tTraining loss: 0.019549342527069212 \tValidation loss: 0.06664840827165962\n","Epoch #883\tTraining loss: 0.019650198758131236 \tValidation loss: 0.08160050521313157\n","Epoch #884\tTraining loss: 0.019506004730663547 \tValidation loss: 0.059279475684256795\n","Epoch #885\tTraining loss: 0.019725893768041205 \tValidation loss: 0.0529909306040398\n","Epoch #886\tTraining loss: 0.020200345429693962 \tValidation loss: 0.0523339662860421\n","Epoch #887\tTraining loss: 0.020613457489830057 \tValidation loss: 0.04936919407862403\n","Epoch #888\tTraining loss: 0.020617203245479554 \tValidation loss: 0.04388306261953386\n","Epoch #889\tTraining loss: 0.02007311501766528 \tValidation loss: 0.046070573479174\n","Epoch #890\tTraining loss: 0.01999277382882262 \tValidation loss: 0.04515260520858616\n","Epoch #891\tTraining loss: 0.020187573979232598 \tValidation loss: 0.04731541847803184\n","Epoch #892\tTraining loss: 0.020816219476894438 \tValidation loss: 0.050096191606001136\n","Epoch #893\tTraining loss: 0.022409828092780047 \tValidation loss: 0.11241376210701146\n","Epoch #894\tTraining loss: 0.02216461500031284 \tValidation loss: 0.05410435502095008\n","Epoch #895\tTraining loss: 0.021576454863688418 \tValidation loss: 0.06195714756590447\n","Epoch #896\tTraining loss: 0.02062574192777143 \tValidation loss: 0.06273680964301433\n","Epoch #897\tTraining loss: 0.019913547561045172 \tValidation loss: 0.0489334273733348\n","Epoch #898\tTraining loss: 0.020177174203793 \tValidation loss: 0.04838901546730235\n","Epoch #899\tTraining loss: 0.019721948624702158 \tValidation loss: 0.048898370538651315\n","Epoch #900\tTraining loss: 0.02061072399178166 \tValidation loss: 0.05693592517754434\n","Epoch #901\tTraining loss: 0.02024140667517047 \tValidation loss: 0.05040260865374302\n","Epoch #902\tTraining loss: 0.020448582005772556 \tValidation loss: 0.05224320030742768\n","Epoch #903\tTraining loss: 0.019794339355338503 \tValidation loss: 0.04508078027045633\n","Epoch #904\tTraining loss: 0.02103700316983753 \tValidation loss: 0.05836036816266238\n","Epoch #905\tTraining loss: 0.01989194134903397 \tValidation loss: 0.04487702083928586\n","Epoch #906\tTraining loss: 0.02059421305924382 \tValidation loss: 0.04551647648109371\n","Epoch #907\tTraining loss: 0.020941021176150786 \tValidation loss: 0.047640211580074344\n","Epoch #908\tTraining loss: 0.02078007171431711 \tValidation loss: 0.05223584440575221\n","Epoch #909\tTraining loss: 0.020815248631363817 \tValidation loss: 0.09225455534414063\n","Epoch #910\tTraining loss: 0.021142449400290413 \tValidation loss: 0.15719286937515936\n","Epoch #911\tTraining loss: 0.022039604470106798 \tValidation loss: 0.11070815701230123\n","Epoch #912\tTraining loss: 0.02274311089914859 \tValidation loss: 0.09737175560974799\n","Epoch #913\tTraining loss: 0.021951911540912628 \tValidation loss: 0.053809032225041874\n","Epoch #914\tTraining loss: 0.020436283774256984 \tValidation loss: 0.05536129110685861\n","Epoch #915\tTraining loss: 0.02041701088708705 \tValidation loss: 0.04684835957515635\n","Epoch #916\tTraining loss: 0.02010759825177331 \tValidation loss: 0.054210351265882584\n","Epoch #917\tTraining loss: 0.019858932233996894 \tValidation loss: 0.04928354553040033\n","Epoch #918\tTraining loss: 0.020355957656528055 \tValidation loss: 0.054269717817021716\n","Epoch #919\tTraining loss: 0.01977458899877483 \tValidation loss: 0.05664920246358495\n","Epoch #920\tTraining loss: 0.020414413833298305 \tValidation loss: 0.052828283896162245\n","Epoch #921\tTraining loss: 0.020526199729954953 \tValidation loss: 0.049862726134673206\n","Epoch #922\tTraining loss: 0.02178364289671561 \tValidation loss: 0.0531899944354895\n","Epoch #923\tTraining loss: 0.021081898862070285 \tValidation loss: 0.05094079895835454\n","Epoch #924\tTraining loss: 0.021600493923632353 \tValidation loss: 0.052825723685850576\n","Epoch #925\tTraining loss: 0.022072733931557873 \tValidation loss: 0.051984571467918995\n","Epoch #926\tTraining loss: 0.02139458659239914 \tValidation loss: 0.0824449740624702\n","Epoch #927\tTraining loss: 0.022317284617938905 \tValidation loss: 0.10292615431856357\n","Epoch #928\tTraining loss: 0.023263890789810518 \tValidation loss: 0.06052515456847238\n","Epoch #929\tTraining loss: 0.021787086584163527 \tValidation loss: 0.08003053514375594\n","Epoch #930\tTraining loss: 0.020629805501152603 \tValidation loss: 0.08187796229839192\n","Epoch #931\tTraining loss: 0.019698997892279878 \tValidation loss: 0.05146257746431566\n","Epoch #932\tTraining loss: 0.019411914044212518 \tValidation loss: 0.05609899225105076\n","Epoch #933\tTraining loss: 0.01974514990214313 \tValidation loss: 0.062485714329569786\n","Epoch #934\tTraining loss: 0.020064722652039402 \tValidation loss: 0.08597002678061168\n","Epoch #935\tTraining loss: 0.019705438349570826 \tValidation loss: 0.08866191214483475\n","Epoch #936\tTraining loss: 0.019491597912697032 \tValidation loss: 0.07502266302742736\n","Epoch #937\tTraining loss: 0.019433569352807163 \tValidation loss: 0.06603387003782957\n","Epoch #938\tTraining loss: 0.019633148529285136 \tValidation loss: 0.049897015929651206\n","Epoch #939\tTraining loss: 0.01967397789219425 \tValidation loss: 0.06122017042662511\n","Epoch #940\tTraining loss: 0.019312009918695004 \tValidation loss: 0.05060965956413983\n","Epoch #941\tTraining loss: 0.019286461580903313 \tValidation loss: 0.05045823339681728\n","Epoch #942\tTraining loss: 0.019265640430690047 \tValidation loss: 0.04833953273713693\n","Epoch #943\tTraining loss: 0.0203075849822344 \tValidation loss: 0.05533743743479832\n","Epoch #944\tTraining loss: 0.02000787253157161 \tValidation loss: 0.06860197792059902\n","Epoch #945\tTraining loss: 0.019963330658861182 \tValidation loss: 0.05539101635904889\n","Epoch #946\tTraining loss: 0.019973204915458072 \tValidation loss: 0.07255133863901048\n","Epoch #947\tTraining loss: 0.020749735182075842 \tValidation loss: 0.1013143347756263\n","Epoch #948\tTraining loss: 0.021401458321472933 \tValidation loss: 0.06943259255944119\n","Epoch #949\tTraining loss: 0.021917705389847504 \tValidation loss: 0.06842656740765177\n","Epoch #950\tTraining loss: 0.021693275001475755 \tValidation loss: 0.04500573337468677\n","Epoch #951\tTraining loss: 0.02031556352464602 \tValidation loss: 0.0446166813995935\n","Epoch #952\tTraining loss: 0.019049026104639 \tValidation loss: 0.05457123165383171\n","Epoch #953\tTraining loss: 0.018590262054251596 \tValidation loss: 0.04637364302178367\n","Epoch #954\tTraining loss: 0.018489065203705544 \tValidation loss: 0.051180646269150844\n","Epoch #955\tTraining loss: 0.01912952614625976 \tValidation loss: 0.05214872862199445\n","Epoch #956\tTraining loss: 0.019268069718625047 \tValidation loss: 0.04602403667636147\n","Epoch #957\tTraining loss: 0.018788528600471418 \tValidation loss: 0.048499369056108556\n","Epoch #958\tTraining loss: 0.01889672182265351 \tValidation loss: 0.04620172832162174\n","Epoch #959\tTraining loss: 0.018890521148485263 \tValidation loss: 0.046737731039669064\n","Epoch #960\tTraining loss: 0.01895401152617974 \tValidation loss: 0.05737895518953915\n","Epoch #961\tTraining loss: 0.019485292396961704 \tValidation loss: 0.0551057028906453\n","Epoch #962\tTraining loss: 0.019894054722248648 \tValidation loss: 0.04702855289224373\n","Epoch #963\tTraining loss: 0.020448018358605474 \tValidation loss: 0.055018894828270014\n","Epoch #964\tTraining loss: 0.01983560345669778 \tValidation loss: 0.05556115639857716\n","Epoch #965\tTraining loss: 0.019757586884104707 \tValidation loss: 0.04795355169469505\n","Epoch #966\tTraining loss: 0.020172640548367933 \tValidation loss: 0.06051861508676613\n","Epoch #967\tTraining loss: 0.01988048343559073 \tValidation loss: 0.05389556030527793\n","Epoch #968\tTraining loss: 0.019459717051054648 \tValidation loss: 0.08229945992430848\n","Epoch #969\tTraining loss: 0.0194509375965642 \tValidation loss: 0.05033122508604672\n","Epoch #970\tTraining loss: 0.01923238395828867 \tValidation loss: 0.053803694197336056\n","Epoch #971\tTraining loss: 0.01846197893609855 \tValidation loss: 0.05101644961584563\n","Epoch #972\tTraining loss: 0.01843804745104009 \tValidation loss: 0.047792127873215906\n","Epoch #973\tTraining loss: 0.018097191196155733 \tValidation loss: 0.04873686291640593\n","Epoch #974\tTraining loss: 0.01790624752530674 \tValidation loss: 0.04370622943161011\n","Epoch #975\tTraining loss: 0.018424613091580964 \tValidation loss: 0.04443875950372136\n","Epoch #976\tTraining loss: 0.01926828890595509 \tValidation loss: 0.04384928773741258\n","Epoch #977\tTraining loss: 0.018277094018820755 \tValidation loss: 0.05006946590341067\n","Epoch #978\tTraining loss: 0.01795340549913899 \tValidation loss: 0.04851889449448183\n","Epoch #979\tTraining loss: 0.01866811636132928 \tValidation loss: 0.047157767839022036\n","Epoch #980\tTraining loss: 0.0194173460433478 \tValidation loss: 0.04680646043314728\n","Epoch #981\tTraining loss: 0.01875663045785629 \tValidation loss: 0.047193311388784666\n","Epoch #982\tTraining loss: 0.01921753552188551 \tValidation loss: 0.049882760663476\n","Epoch #983\tTraining loss: 0.01970442265192663 \tValidation loss: 0.047328541535377196\n","Epoch #984\tTraining loss: 0.020530480677428086 \tValidation loss: 0.05885417948178666\n","Epoch #985\tTraining loss: 0.02051320027787973 \tValidation loss: 0.0631801064067727\n","Epoch #986\tTraining loss: 0.02019308539580708 \tValidation loss: 0.06606622783692588\n","Epoch #987\tTraining loss: 0.020100624609999878 \tValidation loss: 0.07606177484067607\n","Epoch #988\tTraining loss: 0.01975504587333655 \tValidation loss: 0.0751737912042454\n","Epoch #989\tTraining loss: 0.019748779459778536 \tValidation loss: 0.06190325606275323\n","Epoch #990\tTraining loss: 0.019047492127843006 \tValidation loss: 0.05739454405677421\n","Epoch #991\tTraining loss: 0.018640238383584327 \tValidation loss: 0.06949885346122678\n","Epoch #992\tTraining loss: 0.018837377699877363 \tValidation loss: 0.057660956499752086\n","Epoch #993\tTraining loss: 0.018863453164130954 \tValidation loss: 0.0461233159735044\n","Epoch #994\tTraining loss: 0.019505174807708067 \tValidation loss: 0.0494379432300309\n","Epoch #995\tTraining loss: 0.020035897608087192 \tValidation loss: 0.05150632507959732\n","Epoch #996\tTraining loss: 0.020516636772310057 \tValidation loss: 0.05439144456506207\n","Epoch #997\tTraining loss: 0.020007681724923225 \tValidation loss: 0.06540436578614728\n","Epoch #998\tTraining loss: 0.020263542086780904 \tValidation loss: 0.05460065591483767\n","Epoch #999\tTraining loss: 0.01954553011072855 \tValidation loss: 0.04535435500239288\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qWBGaZfwmkGZ","colab_type":"text"},"source":["# Train Data model"]},{"cell_type":"code","metadata":{"id":"fE4qPtvwmoJz","colab_type":"code","colab":{}},"source":["#!python3 train.py -m data -a train -d ../Images/o1_marked/"],"execution_count":0,"outputs":[]}]}