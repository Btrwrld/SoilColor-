{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CUDA_training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMd9PIMv2NSj26PsRm5Aewy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DNjRVavm5RGZ","colab_type":"text"},"source":["# Connect to colab"]},{"cell_type":"code","metadata":{"id":"zk26N0eZ5Xp7","colab_type":"code","outputId":"f9e59e47-e4cf-4707-b3f7-f31839b6bb59","executionInfo":{"status":"ok","timestamp":1591314185889,"user_tz":360,"elapsed":28524,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd ../gdrive/'My Drive'/PARMA/SoilColor/GitRepo"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive/My Drive/PARMA/SoilColor/GitRepo\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dZ3I_ve0mPGS","colab_type":"text"},"source":["# Train Image model"]},{"cell_type":"code","metadata":{"id":"bMKOFr_hmN4y","colab_type":"code","outputId":"0a922410-f78c-4f4e-8a94-dbe871e0bf94","executionInfo":{"status":"ok","timestamp":1591297761406,"user_tz":360,"elapsed":1259606,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python3 train.py -m image -a train -d ../Images/definitive/o1_fused/ -e Adam_MAE_dataset1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Selected image model for train, using the data stored in ../Images/definitive/o1_fused/\n","Ingrese el número de epochs que desea entrenar: 500\n","Ingrese el batch size: 64\n","Ingrese el lr: 1e-5\n","Image_Model(\n","  (loss): L1Loss()\n","  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n","  (relu1): ELU(alpha=1.0)\n","  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (relu2): ELU(alpha=1.0)\n","  (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=2048, out_features=500, bias=True)\n","  (sigm1): Tanh()\n","  (fc2): Linear(in_features=500, out_features=3, bias=True)\n","  (sigm2): Tanh()\n",")\n","Using 999 training samples\n","Using 285 validation samples\n","Executing model on: cuda:0\n","Epoch #0\tTraining loss: 0.32061525318664613 \tValidation loss: 0.47873385339758595\n","Epoch #1\tTraining loss: 0.1936943056205703 \tValidation loss: 0.4777911071700146\n","Epoch #2\tTraining loss: 0.16591840231489188 \tValidation loss: 0.4724583446171947\n","Epoch #3\tTraining loss: 0.15121492994508584 \tValidation loss: 0.45088063971675213\n","Epoch #4\tTraining loss: 0.13989526581450681 \tValidation loss: 0.3984789529106837\n","Epoch #5\tTraining loss: 0.12979953564589675 \tValidation loss: 0.29603667263322053\n","Epoch #6\tTraining loss: 0.12184370231139638 \tValidation loss: 0.19704055273894622\n","Epoch #7\tTraining loss: 0.11597351431304824 \tValidation loss: 0.16632595835074585\n","Epoch #8\tTraining loss: 0.1109699110458613 \tValidation loss: 0.1610201891188804\n","Epoch #9\tTraining loss: 0.10774493750492016 \tValidation loss: 0.1610084068872409\n","Epoch #10\tTraining loss: 0.10473354837528336 \tValidation loss: 0.1630656144179832\n","Epoch #11\tTraining loss: 0.10240496413112284 \tValidation loss: 0.16468780231933974\n","Epoch #12\tTraining loss: 0.10082468419988513 \tValidation loss: 0.16723589563307384\n","Epoch #13\tTraining loss: 0.09901864181432686 \tValidation loss: 0.16090398197789837\n","Epoch #14\tTraining loss: 0.09789089735535249 \tValidation loss: 0.16185523781445274\n","Epoch #15\tTraining loss: 0.09684201335233085 \tValidation loss: 0.16489406472082793\n","Epoch #16\tTraining loss: 0.09553134422157851 \tValidation loss: 0.165387865234261\n","Epoch #17\tTraining loss: 0.09461189149774782 \tValidation loss: 0.1659750904560545\n","Epoch #18\tTraining loss: 0.09357356683605136 \tValidation loss: 0.1654764967044557\n","Epoch #19\tTraining loss: 0.09238708508304674 \tValidation loss: 0.16196120040890752\n","Epoch #20\tTraining loss: 0.09160499579523215 \tValidation loss: 0.16154866662160766\n","Epoch #21\tTraining loss: 0.09070406019605777 \tValidation loss: 0.16102654001579192\n","Epoch #22\tTraining loss: 0.09047336465198304 \tValidation loss: 0.16187393532775657\n","Epoch #23\tTraining loss: 0.0894895591209815 \tValidation loss: 0.1578130366379355\n","Epoch #24\tTraining loss: 0.08839686915866696 \tValidation loss: 0.15684077990170997\n","Epoch #25\tTraining loss: 0.08819774622183227 \tValidation loss: 0.1616894311615199\n","Epoch #26\tTraining loss: 0.08714736621646847 \tValidation loss: 0.16264079606596432\n","Epoch #27\tTraining loss: 0.08662867004269016 \tValidation loss: 0.1640791678916025\n","Epoch #28\tTraining loss: 0.08615499999214979 \tValidation loss: 0.16136611995046785\n","Epoch #29\tTraining loss: 0.08570484188162662 \tValidation loss: 0.15789121989894517\n","Epoch #30\tTraining loss: 0.0851316955838446 \tValidation loss: 0.16397484487469302\n","Epoch #31\tTraining loss: 0.08439654023761546 \tValidation loss: 0.16335702264585927\n","Epoch #32\tTraining loss: 0.08397400210944946 \tValidation loss: 0.1637927932189459\n","Epoch #33\tTraining loss: 0.08336443880400507 \tValidation loss: 0.1627954562138188\n","Epoch #34\tTraining loss: 0.08332298777971196 \tValidation loss: 0.1627820465915642\n","Epoch #35\tTraining loss: 0.08291244502594981 \tValidation loss: 0.16372866602149844\n","Epoch #36\tTraining loss: 0.08235399320625966 \tValidation loss: 0.16315587650550353\n","Epoch #37\tTraining loss: 0.08202917765384046 \tValidation loss: 0.1616457278142175\n","Epoch #38\tTraining loss: 0.08155673934579366 \tValidation loss: 0.16246517338443991\n","Epoch #39\tTraining loss: 0.08103103949159589 \tValidation loss: 0.1630559861727982\n","Epoch #40\tTraining loss: 0.0807651205238454 \tValidation loss: 0.1630227251333228\n","Epoch #41\tTraining loss: 0.08026796935385777 \tValidation loss: 0.16108183819028907\n","Epoch #42\tTraining loss: 0.08082945428679181 \tValidation loss: 0.16399086517942588\n","Epoch #43\tTraining loss: 0.07953868244499236 \tValidation loss: 0.16028917114042188\n","Epoch #44\tTraining loss: 0.07945522111934557 \tValidation loss: 0.16138718758352674\n","Epoch #45\tTraining loss: 0.07943590466447839 \tValidation loss: 0.16138827236746225\n","Epoch #46\tTraining loss: 0.07868663470113296 \tValidation loss: 0.16073489442829006\n","Epoch #47\tTraining loss: 0.0785122806814396 \tValidation loss: 0.16272166183907935\n","Epoch #48\tTraining loss: 0.07836456950046174 \tValidation loss: 0.1640403278721417\n","Epoch #49\tTraining loss: 0.07746879668183408 \tValidation loss: 0.1637908893964742\n","Epoch #50\tTraining loss: 0.0777224857575002 \tValidation loss: 0.162884084935788\n","Epoch #51\tTraining loss: 0.07748806618464653 \tValidation loss: 0.16293145172716694\n","Epoch #52\tTraining loss: 0.07669171770792417 \tValidation loss: 0.16076936665406386\n","Epoch #53\tTraining loss: 0.07651493786286741 \tValidation loss: 0.15968014977370934\n","Epoch #54\tTraining loss: 0.0766697751492637 \tValidation loss: 0.16445224311881426\n","Epoch #55\tTraining loss: 0.07692091236259053 \tValidation loss: 0.1648565423106794\n","Epoch #56\tTraining loss: 0.07605019802205966 \tValidation loss: 0.16111569846774199\n","Epoch #57\tTraining loss: 0.07600959476458527 \tValidation loss: 0.16123481740591916\n","Epoch #58\tTraining loss: 0.07623893433687333 \tValidation loss: 0.16213754630790078\n","Epoch #59\tTraining loss: 0.07539194749643527 \tValidation loss: 0.1619759774010915\n","Epoch #60\tTraining loss: 0.07529286173097495 \tValidation loss: 0.16139197528806679\n","Epoch #61\tTraining loss: 0.07557432394629779 \tValidation loss: 0.16356783011687448\n","Epoch #62\tTraining loss: 0.07455949822282965 \tValidation loss: 0.16292300430131035\n","Epoch #63\tTraining loss: 0.07523288846859494 \tValidation loss: 0.16052207169071794\n","Epoch #64\tTraining loss: 0.07492812446048563 \tValidation loss: 0.16543422118841608\n","Epoch #65\tTraining loss: 0.07434440406244906 \tValidation loss: 0.16504894436271697\n","Epoch #66\tTraining loss: 0.07474280297173125 \tValidation loss: 0.16550501868551132\n","Epoch #67\tTraining loss: 0.07445767877613677 \tValidation loss: 0.172827767644961\n","Epoch #68\tTraining loss: 0.07404539044644798 \tValidation loss: 0.17244759896393755\n","Epoch #69\tTraining loss: 0.07322704179615512 \tValidation loss: 0.16681933088178438\n","Epoch #70\tTraining loss: 0.07304556275613577 \tValidation loss: 0.1669728382938868\n","Epoch #71\tTraining loss: 0.07363363692089715 \tValidation loss: 0.17082642639281637\n","Epoch #72\tTraining loss: 0.07202299491969003 \tValidation loss: 0.1688575373096737\n","Epoch #73\tTraining loss: 0.07305610930497286 \tValidation loss: 0.1446972718079115\n","Epoch #74\tTraining loss: 0.07169666408748285 \tValidation loss: 0.15366840883911753\n","Epoch #75\tTraining loss: 0.0729178736670192 \tValidation loss: 0.16655715435869903\n","Epoch #76\tTraining loss: 0.07169671522111498 \tValidation loss: 0.16538584814102397\n","Epoch #77\tTraining loss: 0.0716750647871787 \tValidation loss: 0.16499273126306566\n","Epoch #78\tTraining loss: 0.07084688649634806 \tValidation loss: 0.1620709699808415\n","Epoch #79\tTraining loss: 0.07205601996044765 \tValidation loss: 0.1693514826642915\n","Epoch #80\tTraining loss: 0.0706799480465408 \tValidation loss: 0.16368894810645662\n","Epoch #81\tTraining loss: 0.07159101333063794 \tValidation loss: 0.16527820576105567\n","Epoch #82\tTraining loss: 0.07073543616859825 \tValidation loss: 0.16100000867499084\n","Epoch #83\tTraining loss: 0.07176769704362584 \tValidation loss: 0.1670063208924468\n","Epoch #84\tTraining loss: 0.07043276209311879 \tValidation loss: 0.16376556519764734\n","Epoch #85\tTraining loss: 0.07104817922854396 \tValidation loss: 0.16744969359877418\n","Epoch #86\tTraining loss: 0.07006116183370296 \tValidation loss: 0.1607143527768503\n","Epoch #87\tTraining loss: 0.07085631749470958 \tValidation loss: 0.16825855445317645\n","Epoch #88\tTraining loss: 0.06988054352448492 \tValidation loss: 0.1654168582984223\n","Epoch #89\tTraining loss: 0.07051518549571639 \tValidation loss: 0.1674834680897315\n","Epoch #90\tTraining loss: 0.06923528474683611 \tValidation loss: 0.1584663740021157\n","Epoch #91\tTraining loss: 0.07036290358599917 \tValidation loss: 0.16550378762148954\n","Epoch #92\tTraining loss: 0.06858413679516827 \tValidation loss: 0.16162774415610726\n","Epoch #93\tTraining loss: 0.0699315372355555 \tValidation loss: 0.16367056552634285\n","Epoch #94\tTraining loss: 0.06837964179476366 \tValidation loss: 0.15793957621644644\n","Epoch #95\tTraining loss: 0.06908752875119986 \tValidation loss: 0.16290417190711254\n","Epoch #96\tTraining loss: 0.06804053159504082 \tValidation loss: 0.15721992575233754\n","Epoch #97\tTraining loss: 0.06869929832510431 \tValidation loss: 0.16228484629855724\n","Epoch #98\tTraining loss: 0.06790223550850172 \tValidation loss: 0.155790664591437\n","Epoch #99\tTraining loss: 0.06875489950922264 \tValidation loss: 0.160122829239734\n","Epoch #100\tTraining loss: 0.06740166170654532 \tValidation loss: 0.1592793429035906\n","Epoch #101\tTraining loss: 0.06824464524041904 \tValidation loss: 0.16376191797644804\n","Epoch #102\tTraining loss: 0.0678737074690845 \tValidation loss: 0.15377930905455342\n","Epoch #103\tTraining loss: 0.06845323913821856 \tValidation loss: 0.1605772103546684\n","Epoch #104\tTraining loss: 0.06747502577649782 \tValidation loss: 0.16173777016707705\n","Epoch #105\tTraining loss: 0.06855104279899574 \tValidation loss: 0.16172807417653823\n","Epoch #106\tTraining loss: 0.06710408292518458 \tValidation loss: 0.15641080263889712\n","Epoch #107\tTraining loss: 0.0681993232585253 \tValidation loss: 0.16389103142559347\n","Epoch #108\tTraining loss: 0.06661852501106942 \tValidation loss: 0.15075109154252878\n","Epoch #109\tTraining loss: 0.0680554789556804 \tValidation loss: 0.15728697435696207\n","Epoch #110\tTraining loss: 0.06611776711811566 \tValidation loss: 0.15161423547831285\n","Epoch #111\tTraining loss: 0.0676435755511952 \tValidation loss: 0.15986540905220042\n","Epoch #112\tTraining loss: 0.06558219683813911 \tValidation loss: 0.14950125978480225\n","Epoch #113\tTraining loss: 0.06709285374292691 \tValidation loss: 0.14896142095709727\n","Epoch #114\tTraining loss: 0.06567866605200885 \tValidation loss: 0.14530289000778018\n","Epoch #115\tTraining loss: 0.06661530379113088 \tValidation loss: 0.15034751850433045\n","Epoch #116\tTraining loss: 0.06516495710514578 \tValidation loss: 0.14767892629422824\n","Epoch #117\tTraining loss: 0.0666448322181053 \tValidation loss: 0.1499071926316393\n","Epoch #118\tTraining loss: 0.06531602579071095 \tValidation loss: 0.14315410319791738\n","Epoch #119\tTraining loss: 0.06541060258976852 \tValidation loss: 0.1402500334090623\n","Epoch #120\tTraining loss: 0.06532595353569637 \tValidation loss: 0.1435397147308503\n","Epoch #121\tTraining loss: 0.06537015517105497 \tValidation loss: 0.14603066908363635\n","Epoch #122\tTraining loss: 0.0645791016591071 \tValidation loss: 0.1455573282317996\n","Epoch #123\tTraining loss: 0.0648016525019624 \tValidation loss: 0.14350086581837038\n","Epoch #124\tTraining loss: 0.06429643783896573 \tValidation loss: 0.1405037810221914\n","Epoch #125\tTraining loss: 0.0647862757144448 \tValidation loss: 0.14172685457120482\n","Epoch #126\tTraining loss: 0.06441180670583842 \tValidation loss: 0.13913339331460015\n","Epoch #127\tTraining loss: 0.0639615942554265 \tValidation loss: 0.13833599695924267\n","Epoch #128\tTraining loss: 0.06410957636971235 \tValidation loss: 0.13842567172155978\n","Epoch #129\tTraining loss: 0.06423133772826511 \tValidation loss: 0.14098197790986003\n","Epoch #130\tTraining loss: 0.06374489778020374 \tValidation loss: 0.13819678608525396\n","Epoch #131\tTraining loss: 0.06395392157558964 \tValidation loss: 0.13149797872048352\n","Epoch #132\tTraining loss: 0.06335831215741385 \tValidation loss: 0.12785992927941822\n","Epoch #133\tTraining loss: 0.0639954551393513 \tValidation loss: 0.13152606767592978\n","Epoch #134\tTraining loss: 0.06323249055535869 \tValidation loss: 0.13394774216213728\n","Epoch #135\tTraining loss: 0.06342243966272769 \tValidation loss: 0.13588269626668273\n","Epoch #136\tTraining loss: 0.06323411350983431 \tValidation loss: 0.134587893937424\n","Epoch #137\tTraining loss: 0.06316808985330272 \tValidation loss: 0.12592040767647475\n","Epoch #138\tTraining loss: 0.06269226090839367 \tValidation loss: 0.12227561400942712\n","Epoch #139\tTraining loss: 0.06287805613011886 \tValidation loss: 0.12882959054050583\n","Epoch #140\tTraining loss: 0.06289773844140852 \tValidation loss: 0.13212691136415503\n","Epoch #141\tTraining loss: 0.06290399191182153 \tValidation loss: 0.1335597451139101\n","Epoch #142\tTraining loss: 0.06281956294158676 \tValidation loss: 0.12797082333792617\n","Epoch #143\tTraining loss: 0.06226648054139689 \tValidation loss: 0.12798224243472903\n","Epoch #144\tTraining loss: 0.06250949325844342 \tValidation loss: 0.12256567883914511\n","Epoch #145\tTraining loss: 0.06173573560970234 \tValidation loss: 0.12058434900449859\n","Epoch #146\tTraining loss: 0.06277942978928945 \tValidation loss: 0.12687352646791894\n","Epoch #147\tTraining loss: 0.06182823498892223 \tValidation loss: 0.12054903072986126\n","Epoch #148\tTraining loss: 0.06237422101804567 \tValidation loss: 0.11968581226641116\n","Epoch #149\tTraining loss: 0.061708158952466646 \tValidation loss: 0.1168795838264404\n","Epoch #150\tTraining loss: 0.06189049907885306 \tValidation loss: 0.11365025222816012\n","Epoch #151\tTraining loss: 0.061607992639705206 \tValidation loss: 0.11343525387860161\n","Epoch #152\tTraining loss: 0.060862906480307906 \tValidation loss: 0.1180427316922395\n","Epoch #153\tTraining loss: 0.06169030309111236 \tValidation loss: 0.1223588996257117\n","Epoch #154\tTraining loss: 0.061276646566314256 \tValidation loss: 0.11618818811639475\n","Epoch #155\tTraining loss: 0.0606080642554082 \tValidation loss: 0.11244138572968443\n","Epoch #156\tTraining loss: 0.061038232857682825 \tValidation loss: 0.11182936147733667\n","Epoch #157\tTraining loss: 0.06075264018084122 \tValidation loss: 0.1044742698805743\n","Epoch #158\tTraining loss: 0.060975297960253506 \tValidation loss: 0.10801451270198138\n","Epoch #159\tTraining loss: 0.06110168384784033 \tValidation loss: 0.11072893833984365\n","Epoch #160\tTraining loss: 0.060288212506327446 \tValidation loss: 0.11061603569723083\n","Epoch #161\tTraining loss: 0.06078050864993876 \tValidation loss: 0.11207568413225101\n","Epoch #162\tTraining loss: 0.06065221467277152 \tValidation loss: 0.11120850426190573\n","Epoch #163\tTraining loss: 0.060405823528083706 \tValidation loss: 0.109816462960373\n","Epoch #164\tTraining loss: 0.06029118883780737 \tValidation loss: 0.10734418239688812\n","Epoch #165\tTraining loss: 0.059935994865817764 \tValidation loss: 0.11082222636911603\n","Epoch #166\tTraining loss: 0.05975098082432749 \tValidation loss: 0.10585313163183846\n","Epoch #167\tTraining loss: 0.05990141490028138 \tValidation loss: 0.10019147482418399\n","Epoch #168\tTraining loss: 0.06091816679180613 \tValidation loss: 0.1003805455529432\n","Epoch #169\tTraining loss: 0.05964259661435991 \tValidation loss: 0.09623136745151627\n","Epoch #170\tTraining loss: 0.05999287740199297 \tValidation loss: 0.1022608851691894\n","Epoch #171\tTraining loss: 0.05901380059070251 \tValidation loss: 0.09695347252040365\n","Epoch #172\tTraining loss: 0.05922732990829663 \tValidation loss: 0.0980200861671459\n","Epoch #173\tTraining loss: 0.05903013023652212 \tValidation loss: 0.09125784375796622\n","Epoch #174\tTraining loss: 0.059444108226593635 \tValidation loss: 0.10534061591232428\n","Epoch #175\tTraining loss: 0.05880841972886855 \tValidation loss: 0.10013060446695236\n","Epoch #176\tTraining loss: 0.05915186222962968 \tValidation loss: 0.10129067126815659\n","Epoch #177\tTraining loss: 0.059150171910554473 \tValidation loss: 0.09548628372842632\n","Epoch #178\tTraining loss: 0.058794419872753315 \tValidation loss: 0.09644443582631688\n","Epoch #179\tTraining loss: 0.05842357790813977 \tValidation loss: 0.09334400003235024\n","Epoch #180\tTraining loss: 0.058742105209994684 \tValidation loss: 0.09552822672005232\n","Epoch #181\tTraining loss: 0.058104770585003074 \tValidation loss: 0.09372832307778778\n","Epoch #182\tTraining loss: 0.05839363024637492 \tValidation loss: 0.09500037724492615\n","Epoch #183\tTraining loss: 0.057863582231272956 \tValidation loss: 0.09357682248262435\n","Epoch #184\tTraining loss: 0.058060357022214196 \tValidation loss: 0.09680929797544989\n","Epoch #185\tTraining loss: 0.05843986155509444 \tValidation loss: 0.09236294236577258\n","Epoch #186\tTraining loss: 0.05780355650753937 \tValidation loss: 0.09405534239134265\n","Epoch #187\tTraining loss: 0.05821892801070246 \tValidation loss: 0.09211401959154682\n","Epoch #188\tTraining loss: 0.0581517706336094 \tValidation loss: 0.09089069046052624\n","Epoch #189\tTraining loss: 0.05767668465115018 \tValidation loss: 0.09530870620231549\n","Epoch #190\tTraining loss: 0.057115877213641704 \tValidation loss: 0.09575681058671655\n","Epoch #191\tTraining loss: 0.05732072204621303 \tValidation loss: 0.08573932084893057\n","Epoch #192\tTraining loss: 0.057584599910213814 \tValidation loss: 0.09063083865666914\n","Epoch #193\tTraining loss: 0.0574679261013495 \tValidation loss: 0.08539551889376686\n","Epoch #194\tTraining loss: 0.0572189461126551 \tValidation loss: 0.08255381643948352\n","Epoch #195\tTraining loss: 0.05755993207006826 \tValidation loss: 0.08763512464965931\n","Epoch #196\tTraining loss: 0.057369759259648524 \tValidation loss: 0.09086220894878645\n","Epoch #197\tTraining loss: 0.05729296015604587 \tValidation loss: 0.08880477787365622\n","Epoch #198\tTraining loss: 0.0569274289806446 \tValidation loss: 0.09093330203587041\n","Epoch #199\tTraining loss: 0.0569803245639626 \tValidation loss: 0.084519216892181\n","Epoch #200\tTraining loss: 0.05723739647751084 \tValidation loss: 0.08319619366562778\n","Epoch #201\tTraining loss: 0.05733803077149889 \tValidation loss: 0.08686294489974118\n","Epoch #202\tTraining loss: 0.05686636844117336 \tValidation loss: 0.08556752458153707\n","Epoch #203\tTraining loss: 0.05699320845747492 \tValidation loss: 0.08346706116169227\n","Epoch #204\tTraining loss: 0.05693757476882021 \tValidation loss: 0.0835949977976047\n","Epoch #205\tTraining loss: 0.05680067904919904 \tValidation loss: 0.08873663211663887\n","Epoch #206\tTraining loss: 0.05645958073367929 \tValidation loss: 0.08562151457370719\n","Epoch #207\tTraining loss: 0.05630061217047357 \tValidation loss: 0.09566072520829623\n","Epoch #208\tTraining loss: 0.05621048227471519 \tValidation loss: 0.06796801684724761\n","Epoch #209\tTraining loss: 0.055783273106882775 \tValidation loss: 0.0741959056749366\n","Epoch #210\tTraining loss: 0.05570484926875042 \tValidation loss: 0.07508734978287966\n","Epoch #211\tTraining loss: 0.05568870359502636 \tValidation loss: 0.0753425746243672\n","Epoch #212\tTraining loss: 0.05618306518302656 \tValidation loss: 0.07772834857998764\n","Epoch #213\tTraining loss: 0.055944727338014914 \tValidation loss: 0.07808784871142398\n","Epoch #214\tTraining loss: 0.05607545273261698 \tValidation loss: 0.0749153078904452\n","Epoch #215\tTraining loss: 0.05552264308396014 \tValidation loss: 0.07276554525879209\n","Epoch #216\tTraining loss: 0.05643905166778303 \tValidation loss: 0.0686462927745485\n","Epoch #217\tTraining loss: 0.05665462261077259 \tValidation loss: 0.07315271766960697\n","Epoch #218\tTraining loss: 0.05644149294315222 \tValidation loss: 0.07438619990866986\n","Epoch #219\tTraining loss: 0.055767824344693157 \tValidation loss: 0.07483404310646868\n","Epoch #220\tTraining loss: 0.05586791477633038 \tValidation loss: 0.07252494641180443\n","Epoch #221\tTraining loss: 0.05583440318352945 \tValidation loss: 0.07271176892714142\n","Epoch #222\tTraining loss: 0.05581534566979547 \tValidation loss: 0.07039557163555189\n","Epoch #223\tTraining loss: 0.055426155800560324 \tValidation loss: 0.07567897785762416\n","Epoch #224\tTraining loss: 0.05512660256858973 \tValidation loss: 0.07572818755605841\n","Epoch #225\tTraining loss: 0.05492727570440572 \tValidation loss: 0.07591320138100478\n","Epoch #226\tTraining loss: 0.05468667547445747 \tValidation loss: 0.07500334192719355\n","Epoch #227\tTraining loss: 0.054844801105776005 \tValidation loss: 0.07112698107032371\n","Epoch #228\tTraining loss: 0.05466861933889571 \tValidation loss: 0.07250432604573462\n","Epoch #229\tTraining loss: 0.05526023769044601 \tValidation loss: 0.069284714066674\n","Epoch #230\tTraining loss: 0.05512728555259943 \tValidation loss: 0.07550716587537887\n","Epoch #231\tTraining loss: 0.05474936489872071 \tValidation loss: 0.07703114695442052\n","Epoch #232\tTraining loss: 0.05458319828493336 \tValidation loss: 0.07703224956010661\n","Epoch #233\tTraining loss: 0.05470324387320769 \tValidation loss: 0.07429531023429772\n","Epoch #234\tTraining loss: 0.05506171588063273 \tValidation loss: 0.07223364822713703\n","Epoch #235\tTraining loss: 0.05486216563142647 \tValidation loss: 0.0717537198258805\n","Epoch #236\tTraining loss: 0.054894419370506604 \tValidation loss: 0.07180513945209692\n","Epoch #237\tTraining loss: 0.05491451120341448 \tValidation loss: 0.06919332075435132\n","Epoch #238\tTraining loss: 0.05534605291922543 \tValidation loss: 0.07007455939510872\n","Epoch #239\tTraining loss: 0.05447508143846883 \tValidation loss: 0.0716680602057989\n","Epoch #240\tTraining loss: 0.05413681631978344 \tValidation loss: 0.07211285242741795\n","Epoch #241\tTraining loss: 0.05408561127561435 \tValidation loss: 0.07395065679887101\n","Epoch #242\tTraining loss: 0.05428540554190204 \tValidation loss: 0.0684597540417842\n","Epoch #243\tTraining loss: 0.054513684743164224 \tValidation loss: 0.06870321446774873\n","Epoch #244\tTraining loss: 0.054223019043138296 \tValidation loss: 0.07458111473687058\n","Epoch #245\tTraining loss: 0.053934739979003894 \tValidation loss: 0.07834474329009708\n","Epoch #246\tTraining loss: 0.053947221439102216 \tValidation loss: 0.07359076082600341\n","Epoch #247\tTraining loss: 0.054118527458159364 \tValidation loss: 0.07138094044379284\n","Epoch #248\tTraining loss: 0.05428431297924151 \tValidation loss: 0.07016263487432822\n","Epoch #249\tTraining loss: 0.05440895666957439 \tValidation loss: 0.0718144705010177\n","Epoch #250\tTraining loss: 0.054829676885942306 \tValidation loss: 0.06899615630757545\n","Epoch #251\tTraining loss: 0.05420908093860297 \tValidation loss: 0.07320222439999562\n","Epoch #252\tTraining loss: 0.053889578074783326 \tValidation loss: 0.07454804822109302\n","Epoch #253\tTraining loss: 0.053947015648818604 \tValidation loss: 0.07257821012459299\n","Epoch #254\tTraining loss: 0.05385473160320959 \tValidation loss: 0.07224352569643686\n","Epoch #255\tTraining loss: 0.05376412060783584 \tValidation loss: 0.07067774741298152\n","Epoch #256\tTraining loss: 0.053562848560099 \tValidation loss: 0.07145532286238564\n","Epoch #257\tTraining loss: 0.05377820512500042 \tValidation loss: 0.07333871832794246\n","Epoch #258\tTraining loss: 0.05354190047741365 \tValidation loss: 0.0751404904753981\n","Epoch #259\tTraining loss: 0.053443716162561085 \tValidation loss: 0.07366151290029685\n","Epoch #260\tTraining loss: 0.053066543257465806 \tValidation loss: 0.07549187029917342\n","Epoch #261\tTraining loss: 0.05298224474585269 \tValidation loss: 0.07617664785823829\n","Epoch #262\tTraining loss: 0.05352392617427953 \tValidation loss: 0.0756776220054564\n","Epoch #263\tTraining loss: 0.05339232773964715 \tValidation loss: 0.07424304347319581\n","Epoch #264\tTraining loss: 0.053123501461574116 \tValidation loss: 0.07229204854474484\n","Epoch #265\tTraining loss: 0.0529904870015685 \tValidation loss: 0.06972580685796524\n","Epoch #266\tTraining loss: 0.05339402745413059 \tValidation loss: 0.07097880916231829\n","Epoch #267\tTraining loss: 0.053489880280638315 \tValidation loss: 0.07091986845994681\n","Epoch #268\tTraining loss: 0.0534622313689851 \tValidation loss: 0.0704327892040067\n","Epoch #269\tTraining loss: 0.053198138572900545 \tValidation loss: 0.07096357543435866\n","Epoch #270\tTraining loss: 0.05342087599344187 \tValidation loss: 0.0748141274725952\n","Epoch #271\tTraining loss: 0.053059881532259844 \tValidation loss: 0.07539535461121204\n","Epoch #272\tTraining loss: 0.052984773069934175 \tValidation loss: 0.07504856247590799\n","Epoch #273\tTraining loss: 0.052780645724465884 \tValidation loss: 0.08308632743057595\n","Epoch #274\tTraining loss: 0.05251027399802274 \tValidation loss: 0.07535433222129242\n","Epoch #275\tTraining loss: 0.05242159385224133 \tValidation loss: 0.07141257368307477\n","Epoch #276\tTraining loss: 0.05246402164584243 \tValidation loss: 0.07164221678769318\n","Epoch #277\tTraining loss: 0.052728858881196494 \tValidation loss: 0.07163005530878999\n","Epoch #278\tTraining loss: 0.05231019854960772 \tValidation loss: 0.07427139086059853\n","Epoch #279\tTraining loss: 0.05219807317563765 \tValidation loss: 0.07401404837070566\n","Epoch #280\tTraining loss: 0.05222593929847276 \tValidation loss: 0.07595080160889121\n","Epoch #281\tTraining loss: 0.052055274345317024 \tValidation loss: 0.0764815787996526\n","Epoch #282\tTraining loss: 0.051998598816994375 \tValidation loss: 0.07754614162622565\n","Epoch #283\tTraining loss: 0.0519553403231945 \tValidation loss: 0.07757945025307099\n","Epoch #284\tTraining loss: 0.05170617964691225 \tValidation loss: 0.07298803153280742\n","Epoch #285\tTraining loss: 0.05197286799539713 \tValidation loss: 0.0732566145204819\n","Epoch #286\tTraining loss: 0.051878913590196404 \tValidation loss: 0.07351155106808187\n","Epoch #287\tTraining loss: 0.0520815673308301 \tValidation loss: 0.07188766651402966\n","Epoch #288\tTraining loss: 0.05235593564048205 \tValidation loss: 0.07444806168183156\n","Epoch #289\tTraining loss: 0.05207193089001418 \tValidation loss: 0.0732421782643123\n","Epoch #290\tTraining loss: 0.05166205992985587 \tValidation loss: 0.07572552653784947\n","Epoch #291\tTraining loss: 0.051711799991942094 \tValidation loss: 0.0742171347907796\n","Epoch #292\tTraining loss: 0.05152515105452126 \tValidation loss: 0.07965451262809065\n","Epoch #293\tTraining loss: 0.051581545235871884 \tValidation loss: 0.0780309341000597\n","Epoch #294\tTraining loss: 0.051593556443607295 \tValidation loss: 0.0756140990212877\n","Epoch #295\tTraining loss: 0.05213911192173771 \tValidation loss: 0.0759482959643298\n","Epoch #296\tTraining loss: 0.052692692585510396 \tValidation loss: 0.07286759054524655\n","Epoch #297\tTraining loss: 0.05228705662239304 \tValidation loss: 0.08153917565662265\n","Epoch #298\tTraining loss: 0.05159499865303817 \tValidation loss: 0.07647208422146974\n","Epoch #299\tTraining loss: 0.05162507440050165 \tValidation loss: 0.08281415114877962\n","Epoch #300\tTraining loss: 0.05180477169394885 \tValidation loss: 0.0712540668119249\n","Epoch #301\tTraining loss: 0.05181527251078641 \tValidation loss: 0.07411275072279229\n","Epoch #302\tTraining loss: 0.051901036279448004 \tValidation loss: 0.07844503895249288\n","Epoch #303\tTraining loss: 0.05141439023341666 \tValidation loss: 0.07989122433464214\n","Epoch #304\tTraining loss: 0.05122730187595322 \tValidation loss: 0.0816685343161828\n","Epoch #305\tTraining loss: 0.05104538507707277 \tValidation loss: 0.07999487903650174\n","Epoch #306\tTraining loss: 0.050991434445788034 \tValidation loss: 0.0796878845514166\n","Epoch #307\tTraining loss: 0.050948504978887775 \tValidation loss: 0.07802417928443588\n","Epoch #308\tTraining loss: 0.05104530241129747 \tValidation loss: 0.0804493336331539\n","Epoch #309\tTraining loss: 0.0508484726774026 \tValidation loss: 0.07476324927382963\n","Epoch #310\tTraining loss: 0.05131778490933548 \tValidation loss: 0.07594024431620636\n","Epoch #311\tTraining loss: 0.05127577669453485 \tValidation loss: 0.07688313719661652\n","Epoch #312\tTraining loss: 0.05088802327693633 \tValidation loss: 0.0814302258930765\n","Epoch #313\tTraining loss: 0.05065716184871631 \tValidation loss: 0.08514068171604952\n","Epoch #314\tTraining loss: 0.05062847171885147 \tValidation loss: 0.08716492377134528\n","Epoch #315\tTraining loss: 0.05086734105368919 \tValidation loss: 0.08498155295683814\n","Epoch #316\tTraining loss: 0.050866251633337425 \tValidation loss: 0.0805544401525611\n","Epoch #317\tTraining loss: 0.05138600081295738 \tValidation loss: 0.08044152963891683\n","Epoch #318\tTraining loss: 0.05117750965418048 \tValidation loss: 0.09167329865792172\n","Epoch #319\tTraining loss: 0.050910553644191066 \tValidation loss: 0.0865553852915172\n","Epoch #320\tTraining loss: 0.05092097227078045 \tValidation loss: 0.08895367178532385\n","Epoch #321\tTraining loss: 0.0504635870507874 \tValidation loss: 0.08629231397140735\n","Epoch #322\tTraining loss: 0.0505506993463533 \tValidation loss: 0.08636837693655655\n","Epoch #323\tTraining loss: 0.05038345037047946 \tValidation loss: 0.0821447953163747\n","Epoch #324\tTraining loss: 0.050469057751882054 \tValidation loss: 0.08451597517114033\n","Epoch #325\tTraining loss: 0.050438373173080316 \tValidation loss: 0.08312078047470171\n","Epoch #326\tTraining loss: 0.05071416437442275 \tValidation loss: 0.08702652027302427\n","Epoch #327\tTraining loss: 0.05060356981365903 \tValidation loss: 0.08066996098839467\n","Epoch #328\tTraining loss: 0.05072424618605274 \tValidation loss: 0.08513090378316514\n","Epoch #329\tTraining loss: 0.050658934628261584 \tValidation loss: 0.08150989015927364\n","Epoch #330\tTraining loss: 0.05093269412715989 \tValidation loss: 0.08567756678408515\n","Epoch #331\tTraining loss: 0.05104741890396072 \tValidation loss: 0.07986800011688931\n","Epoch #332\tTraining loss: 0.05082049822863821 \tValidation loss: 0.0806238851965992\n","Epoch #333\tTraining loss: 0.05099957443154625 \tValidation loss: 0.08150861182837349\n","Epoch #334\tTraining loss: 0.05035333160215138 \tValidation loss: 0.08166455816431717\n","Epoch #335\tTraining loss: 0.0503275560218194 \tValidation loss: 0.0849292365468092\n","Epoch #336\tTraining loss: 0.05037006687338089 \tValidation loss: 0.10102778317348479\n","Epoch #337\tTraining loss: 0.049940557114505145 \tValidation loss: 0.08843105099976085\n","Epoch #338\tTraining loss: 0.049909993100278736 \tValidation loss: 0.08608945488124901\n","Epoch #339\tTraining loss: 0.04955345994268838 \tValidation loss: 0.08669360298205421\n","Epoch #340\tTraining loss: 0.04979553190594006 \tValidation loss: 0.08751983045080228\n","Epoch #341\tTraining loss: 0.04955730022504708 \tValidation loss: 0.0853208854485739\n","Epoch #342\tTraining loss: 0.04999889251512909 \tValidation loss: 0.08589352947414855\n","Epoch #343\tTraining loss: 0.050097876953864105 \tValidation loss: 0.08366582543833366\n","Epoch #344\tTraining loss: 0.05058399713613518 \tValidation loss: 0.0863238469836931\n","Epoch #345\tTraining loss: 0.05012483583597766 \tValidation loss: 0.0860233491795567\n","Epoch #346\tTraining loss: 0.04964620722547918 \tValidation loss: 0.09390154656844109\n","Epoch #347\tTraining loss: 0.049747219455398416 \tValidation loss: 0.1067457677447908\n","Epoch #348\tTraining loss: 0.04974195182682206 \tValidation loss: 0.09457053214154784\n","Epoch #349\tTraining loss: 0.04961021025594672 \tValidation loss: 0.09588342922919987\n","Epoch #350\tTraining loss: 0.04978149671832439 \tValidation loss: 0.08925484934551377\n","Epoch #351\tTraining loss: 0.04987788068889073 \tValidation loss: 0.09145668253345665\n","Epoch #352\tTraining loss: 0.05016236652174308 \tValidation loss: 0.09067125866936208\n","Epoch #353\tTraining loss: 0.05002769819427235 \tValidation loss: 0.09083465402963349\n","Epoch #354\tTraining loss: 0.0495511913039135 \tValidation loss: 0.08817233541279196\n","Epoch #355\tTraining loss: 0.049564614394926786 \tValidation loss: 0.09986628109694096\n","Epoch #356\tTraining loss: 0.04958132867293156 \tValidation loss: 0.09644795724671056\n","Epoch #357\tTraining loss: 0.04915444942031257 \tValidation loss: 0.08697927075844332\n","Epoch #358\tTraining loss: 0.04933303961772954 \tValidation loss: 0.0878701490487257\n","Epoch #359\tTraining loss: 0.04952106300717389 \tValidation loss: 0.08486527248542365\n","Epoch #360\tTraining loss: 0.051174809508060554 \tValidation loss: 0.0879899380633272\n","Epoch #361\tTraining loss: 0.05046122505913489 \tValidation loss: 0.08567496650806036\n","Epoch #362\tTraining loss: 0.0498476921454926 \tValidation loss: 0.09008421566625567\n","Epoch #363\tTraining loss: 0.04963425265277573 \tValidation loss: 0.09129672397021357\n","Epoch #364\tTraining loss: 0.04988954538166042 \tValidation loss: 0.09586653407978256\n","Epoch #365\tTraining loss: 0.049290457870137284 \tValidation loss: 0.08805362981850053\n","Epoch #366\tTraining loss: 0.04929266381409427 \tValidation loss: 0.09277789068521414\n","Epoch #367\tTraining loss: 0.049754058063680134 \tValidation loss: 0.08680139353554574\n","Epoch #368\tTraining loss: 0.04949350162443658 \tValidation loss: 0.08974019496233049\n","Epoch #369\tTraining loss: 0.049465760431814686 \tValidation loss: 0.08579072795685222\n","Epoch #370\tTraining loss: 0.04932018096786458 \tValidation loss: 0.08948485064152767\n","Epoch #371\tTraining loss: 0.049463252477971745 \tValidation loss: 0.0943524784724842\n","Epoch #372\tTraining loss: 0.049365667526351166 \tValidation loss: 0.09544640060476019\n","Epoch #373\tTraining loss: 0.04897204249767115 \tValidation loss: 0.09267879385657429\n","Epoch #374\tTraining loss: 0.04922473040892877 \tValidation loss: 0.09234588905465743\n","Epoch #375\tTraining loss: 0.04925299435211635 \tValidation loss: 0.09573966582661393\n","Epoch #376\tTraining loss: 0.04886417295584478 \tValidation loss: 0.09178353527660656\n","Epoch #377\tTraining loss: 0.04861417511268487 \tValidation loss: 0.0916147471909249\n","Epoch #378\tTraining loss: 0.04876668082334944 \tValidation loss: 0.09330388203386933\n","Epoch #379\tTraining loss: 0.04852509308402508 \tValidation loss: 0.09357916287359429\n","Epoch #380\tTraining loss: 0.04856582444464118 \tValidation loss: 0.09009111419197272\n","Epoch #381\tTraining loss: 0.0486233920068896 \tValidation loss: 0.08604108093662079\n","Epoch #382\tTraining loss: 0.04871297166833628 \tValidation loss: 0.09004429241163588\n","Epoch #383\tTraining loss: 0.04873285426949167 \tValidation loss: 0.08999326019063203\n","Epoch #384\tTraining loss: 0.04943871405550793 \tValidation loss: 0.09278259124552582\n","Epoch #385\tTraining loss: 0.049413430782988495 \tValidation loss: 0.08569680177460527\n","Epoch #386\tTraining loss: 0.04922741805552431 \tValidation loss: 0.0975273428625839\n","Epoch #387\tTraining loss: 0.04924806337299162 \tValidation loss: 0.09528571445342152\n","Epoch #388\tTraining loss: 0.04923022995408825 \tValidation loss: 0.09821574166996323\n","Epoch #389\tTraining loss: 0.04908165098708701 \tValidation loss: 0.0990442607506643\n","Epoch #390\tTraining loss: 0.049229272128615276 \tValidation loss: 0.09374349208834998\n","Epoch #391\tTraining loss: 0.049206566007721934 \tValidation loss: 0.09117901607582116\n","Epoch #392\tTraining loss: 0.05030100505741873 \tValidation loss: 0.11539147162644794\n","Epoch #393\tTraining loss: 0.05003069630262253 \tValidation loss: 0.0933500675698188\n","Epoch #394\tTraining loss: 0.04941395340162022 \tValidation loss: 0.08609074889395818\n","Epoch #395\tTraining loss: 0.04925888900306849 \tValidation loss: 0.08700664592509186\n","Epoch #396\tTraining loss: 0.04959466217538941 \tValidation loss: 0.08709947811016959\n","Epoch #397\tTraining loss: 0.0497209398956465 \tValidation loss: 0.08719354102547307\n","Epoch #398\tTraining loss: 0.051004607778671954 \tValidation loss: 0.08445011318092299\n","Epoch #399\tTraining loss: 0.05092524533843735 \tValidation loss: 0.09650598082037391\n","Epoch #400\tTraining loss: 0.05057166363625399 \tValidation loss: 0.14241294160451134\n","Epoch #401\tTraining loss: 0.050616753246096484 \tValidation loss: 0.0800535775753744\n","Epoch #402\tTraining loss: 0.0517316212493874 \tValidation loss: 0.08057466641580205\n","Epoch #403\tTraining loss: 0.05255453249145178 \tValidation loss: 0.08581130585933558\n","Epoch #404\tTraining loss: 0.052706495116385456 \tValidation loss: 0.08330258126318414\n","Epoch #405\tTraining loss: 0.05232896702682328 \tValidation loss: 0.08062253274816429\n","Epoch #406\tTraining loss: 0.05249626697119848 \tValidation loss: 0.07909679051979497\n","Epoch #407\tTraining loss: 0.052285449224220376 \tValidation loss: 0.08325291496579175\n","Epoch #408\tTraining loss: 0.05191970932494842 \tValidation loss: 0.09216581772172172\n","Epoch #409\tTraining loss: 0.05178752535500517 \tValidation loss: 0.10166912394888443\n","Epoch #410\tTraining loss: 0.0512123147581456 \tValidation loss: 0.08961274660301964\n","Epoch #411\tTraining loss: 0.05095959478300309 \tValidation loss: 0.07141664627662983\n","Epoch #412\tTraining loss: 0.0506982009621662 \tValidation loss: 0.0770297349496363\n","Epoch #413\tTraining loss: 0.050268155943515275 \tValidation loss: 0.07728004998325752\n","Epoch #414\tTraining loss: 0.0500877923246595 \tValidation loss: 0.07962690250340843\n","Epoch #415\tTraining loss: 0.04952617008669017 \tValidation loss: 0.07655778408407608\n","Epoch #416\tTraining loss: 0.04996561395447492 \tValidation loss: 0.07774826293897726\n","Epoch #417\tTraining loss: 0.04961356300606317 \tValidation loss: 0.07655545301136644\n","Epoch #418\tTraining loss: 0.049315739431972136 \tValidation loss: 0.08150313434785442\n","Epoch #419\tTraining loss: 0.04884027196179865 \tValidation loss: 0.08511628622075242\n","Epoch #420\tTraining loss: 0.048759639407366225 \tValidation loss: 0.08786114837457067\n","Epoch #421\tTraining loss: 0.048756297650966916 \tValidation loss: 0.08332958974392135\n","Epoch #422\tTraining loss: 0.04864594057038526 \tValidation loss: 0.07664898514394507\n","Epoch #423\tTraining loss: 0.04851734902118419 \tValidation loss: 0.0846132016848984\n","Epoch #424\tTraining loss: 0.0487987205428606 \tValidation loss: 0.06840312402672485\n","Epoch #425\tTraining loss: 0.048641260708785855 \tValidation loss: 0.06853745420211728\n","Epoch #426\tTraining loss: 0.048270759346114404 \tValidation loss: 0.07303851298402589\n","Epoch #427\tTraining loss: 0.04820140596528896 \tValidation loss: 0.08152394296006196\n","Epoch #428\tTraining loss: 0.048174033105720285 \tValidation loss: 0.08144572798441564\n","Epoch #429\tTraining loss: 0.04872331675999644 \tValidation loss: 0.08918012282143585\n","Epoch #430\tTraining loss: 0.04839512187475252 \tValidation loss: 0.07894174540960362\n","Epoch #431\tTraining loss: 0.04803039438620487 \tValidation loss: 0.08237307037163837\n","Epoch #432\tTraining loss: 0.04793111184778064 \tValidation loss: 0.08083239627938611\n","Epoch #433\tTraining loss: 0.048096234854080405 \tValidation loss: 0.08487473616686976\n","Epoch #434\tTraining loss: 0.048343818312123824 \tValidation loss: 0.08329633870154532\n","Epoch #435\tTraining loss: 0.04819522424251653 \tValidation loss: 0.08047715376998389\n","Epoch #436\tTraining loss: 0.048977901977876374 \tValidation loss: 0.08294720020885012\n","Epoch #437\tTraining loss: 0.04878010550415192 \tValidation loss: 0.07869415680167478\n","Epoch #438\tTraining loss: 0.048283601168581554 \tValidation loss: 0.08114630545761896\n","Epoch #439\tTraining loss: 0.048045812276262104 \tValidation loss: 0.08113894895557858\n","Epoch #440\tTraining loss: 0.048290629956781274 \tValidation loss: 0.08427283840363683\n","Epoch #441\tTraining loss: 0.04813437735654895 \tValidation loss: 0.08726823522216055\n","Epoch #442\tTraining loss: 0.047943795124755974 \tValidation loss: 0.07838358633126301\n","Epoch #443\tTraining loss: 0.04789967606292449 \tValidation loss: 0.08111981305374245\n","Epoch #444\tTraining loss: 0.048271958003387135 \tValidation loss: 0.08422715989696497\n","Epoch #445\tTraining loss: 0.04818070252491557 \tValidation loss: 0.07926162547823451\n","Epoch #446\tTraining loss: 0.04902762111313644 \tValidation loss: 0.0737667591126826\n","Epoch #447\tTraining loss: 0.04900430373083761 \tValidation loss: 0.07630778636143526\n","Epoch #448\tTraining loss: 0.048550672311034715 \tValidation loss: 0.07549751061157069\n","Epoch #449\tTraining loss: 0.04816989280949655 \tValidation loss: 0.07308279542731208\n","Epoch #450\tTraining loss: 0.04808742363226164 \tValidation loss: 0.07619398329993879\n","Epoch #451\tTraining loss: 0.0479736603773058 \tValidation loss: 0.08111769380924141\n","Epoch #452\tTraining loss: 0.048303621372533016 \tValidation loss: 0.07650765509232337\n","Epoch #453\tTraining loss: 0.04828378355507982 \tValidation loss: 0.0820221689556571\n","Epoch #454\tTraining loss: 0.048395180591232365 \tValidation loss: 0.0791797433979065\n","Epoch #455\tTraining loss: 0.04855409320015076 \tValidation loss: 0.07580679588118172\n","Epoch #456\tTraining loss: 0.04850468440824055 \tValidation loss: 0.08400594508631473\n","Epoch #457\tTraining loss: 0.04800505775599812 \tValidation loss: 0.08347141554390881\n","Epoch #458\tTraining loss: 0.04824625517069325 \tValidation loss: 0.08184469985629932\n","Epoch #459\tTraining loss: 0.048635434927370456 \tValidation loss: 0.07693861984667237\n","Epoch #460\tTraining loss: 0.04832124675929853 \tValidation loss: 0.07416723349542564\n","Epoch #461\tTraining loss: 0.04825468025709275 \tValidation loss: 0.07144553947379587\n","Epoch #462\tTraining loss: 0.04807407878206838 \tValidation loss: 0.0839563226691748\n","Epoch #463\tTraining loss: 0.04834277880902247 \tValidation loss: 0.08197214147277006\n","Epoch #464\tTraining loss: 0.04836177078828334 \tValidation loss: 0.08127766470912802\n","Epoch #465\tTraining loss: 0.04804889172848791 \tValidation loss: 0.08640799602805306\n","Epoch #466\tTraining loss: 0.04833870689997796 \tValidation loss: 0.0872815002169925\n","Epoch #467\tTraining loss: 0.047878057996186836 \tValidation loss: 0.08043259193902336\n","Epoch #468\tTraining loss: 0.04792163543016399 \tValidation loss: 0.08461277391194504\n","Epoch #469\tTraining loss: 0.04812340067910854 \tValidation loss: 0.09141998712295037\n","Epoch #470\tTraining loss: 0.048015443296659084 \tValidation loss: 0.08201762897779784\n","Epoch #471\tTraining loss: 0.048329362799122214 \tValidation loss: 0.08019795265344079\n","Epoch #472\tTraining loss: 0.048345147096965684 \tValidation loss: 0.07501392471127823\n","Epoch #473\tTraining loss: 0.04763491980876524 \tValidation loss: 0.07424478299547814\n","Epoch #474\tTraining loss: 0.04777461417918316 \tValidation loss: 0.07263039905532256\n","Epoch #475\tTraining loss: 0.0475889649223919 \tValidation loss: 0.08147122943242026\n","Epoch #476\tTraining loss: 0.04785783758297492 \tValidation loss: 0.07549391367690589\n","Epoch #477\tTraining loss: 0.04806444426235548 \tValidation loss: 0.07178245698061574\n","Epoch #478\tTraining loss: 0.047380892918961755 \tValidation loss: 0.06841613220821241\n","Epoch #479\tTraining loss: 0.04775461602371753 \tValidation loss: 0.0729997624578801\n","Epoch #480\tTraining loss: 0.04776009490738722 \tValidation loss: 0.08628106700676885\n","Epoch #481\tTraining loss: 0.047571296507421076 \tValidation loss: 0.07319676813542982\n","Epoch #482\tTraining loss: 0.047886104356123685 \tValidation loss: 0.06528123676632955\n","Epoch #483\tTraining loss: 0.04833941986099285 \tValidation loss: 0.0639067917129937\n","Epoch #484\tTraining loss: 0.04838953298821776 \tValidation loss: 0.06390625997752206\n","Epoch #485\tTraining loss: 0.04839774472753193 \tValidation loss: 0.06333013391583317\n","Epoch #486\tTraining loss: 0.04833615237557489 \tValidation loss: 0.06163804020365905\n","Epoch #487\tTraining loss: 0.04766390535214902 \tValidation loss: 0.06301068482312394\n","Epoch #488\tTraining loss: 0.047758089156909954 \tValidation loss: 0.06390369467148885\n","Epoch #489\tTraining loss: 0.04767252780472201 \tValidation loss: 0.06453167747488184\n","Epoch #490\tTraining loss: 0.047960348966656116 \tValidation loss: 0.0646182524867038\n","Epoch #491\tTraining loss: 0.04811284506424957 \tValidation loss: 0.06257294810773854\n","Epoch #492\tTraining loss: 0.04775973065729471 \tValidation loss: 0.06049510118024264\n","Epoch #493\tTraining loss: 0.04772280432418536 \tValidation loss: 0.06286362916730034\n","Epoch #494\tTraining loss: 0.04772369213190675 \tValidation loss: 0.06432727880611362\n","Epoch #495\tTraining loss: 0.047714646188071135 \tValidation loss: 0.06440678228325797\n","Epoch #496\tTraining loss: 0.04812762144515162 \tValidation loss: 0.062378309847886025\n","Epoch #497\tTraining loss: 0.04779134986800356 \tValidation loss: 0.0673217078188669\n","Epoch #498\tTraining loss: 0.04767806826337582 \tValidation loss: 0.060683020181917346\n","Epoch #499\tTraining loss: 0.04838990824063619 \tValidation loss: 0.05923647087688576\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sFORGeOcP3cS","colab_type":"code","outputId":"342e172f-d56e-44a4-aedd-f734961c854d","executionInfo":{"status":"ok","timestamp":1591314276302,"user_tz":360,"elapsed":86528,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["!python3 train.py -m checkpoints/04-06-2020_18:59:30_image_e500_bs64_lr1e-05_Adam_MAE_dataset1/images.pth -a infer -d ../Images/definitive/o1_fused/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Selected checkpoints/04-06-2020_18:59:30_image_e500_bs64_lr1e-05_Adam_MAE_dataset1/images.pth model for infer, using the data stored in ../Images/definitive/o1_fused/\n","Image_Model(\n","  (loss): L1Loss()\n","  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n","  (relu1): ELU(alpha=1.0)\n","  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (relu2): ELU(alpha=1.0)\n","  (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=2048, out_features=500, bias=True)\n","  (sigm1): Tanh()\n","  (fc2): Linear(in_features=500, out_features=3, bias=True)\n","  (sigm2): Tanh()\n",")\n","Mean_loss: 0.06452096861183006\n","Accuracy: 0.4375\n","Hue_accuracy: 0.5694444444444444\n","Chroma_accuracy: 0.7638888888888888\n","Value_accuracy: 0.9444444444444444\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qWBGaZfwmkGZ","colab_type":"text"},"source":["# Train Data model"]},{"cell_type":"code","metadata":{"id":"fE4qPtvwmoJz","colab_type":"code","colab":{}},"source":["#!python3 train.py -m data -a train -d ../Images/o1_marked/"],"execution_count":0,"outputs":[]}]}