{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CUDA_training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPI62AnU1cM+jq+hXQGuuWq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DNjRVavm5RGZ","colab_type":"text"},"source":["# Connect to colab"]},{"cell_type":"code","metadata":{"id":"zk26N0eZ5Xp7","colab_type":"code","outputId":"0112cdf4-9dad-4360-f740-06d0ad3612b3","executionInfo":{"status":"ok","timestamp":1589398838874,"user_tz":360,"elapsed":20657,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd ../gdrive/'My Drive'/PARMA/SoilColor/GitRepo"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive/My Drive/PARMA/SoilColor/GitRepo\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dZ3I_ve0mPGS","colab_type":"text"},"source":["# Train Image model"]},{"cell_type":"code","metadata":{"id":"bMKOFr_hmN4y","colab_type":"code","outputId":"934ba559-5bed-4ea7-e33d-035555038215","executionInfo":{"status":"ok","timestamp":1589400215951,"user_tz":360,"elapsed":1397712,"user":{"displayName":"Erick Alejandro Muñoz Alvarado","photoUrl":"","userId":"06421202750453488421"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python3 train.py -m image -a train -d ../Images/o1_fused/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Selected image model for train, using the data stored in ../Images/o1_fused/\n","Ingrese el número de epochs que desea entrenar: 500\n","Ingrese el batch size: 64\n","Ingrese el lr: 0.0001\n","Image_Model(\n","  (loss): MSELoss()\n","  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n","  (relu1): ELU(alpha=1.0)\n","  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (relu2): ELU(alpha=1.0)\n","  (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=2048, out_features=500, bias=True)\n","  (sigm1): Sigmoid()\n","  (fc2): Linear(in_features=500, out_features=3, bias=True)\n","  (sigm2): Sigmoid()\n",")\n","Executing model on: cuda:0\n","Epoch #0\tTraining loss: 0.07009554829353977 \tValidation loss: 0.10888370344399888\n","Epoch #1\tTraining loss: 0.03828889267163192 \tValidation loss: 0.1067280063006532\n","Epoch #2\tTraining loss: 0.023910819457423994 \tValidation loss: 0.10481092193867844\n","Epoch #3\tTraining loss: 0.018997387519523887 \tValidation loss: 0.10036454865803492\n","Epoch #4\tTraining loss: 0.01740059734593873 \tValidation loss: 0.08857613159557061\n","Epoch #5\tTraining loss: 0.01655016858014604 \tValidation loss: 0.07571239240760425\n","Epoch #6\tTraining loss: 0.015926033369690666 \tValidation loss: 0.07064041884086286\n","Epoch #7\tTraining loss: 0.015328526812719208 \tValidation loss: 0.06954811194451688\n","Epoch #8\tTraining loss: 0.014781730170666425 \tValidation loss: 0.07172591705157377\n","Epoch #9\tTraining loss: 0.014259517990501906 \tValidation loss: 0.07404159735069578\n","Epoch #10\tTraining loss: 0.01377461001389743 \tValidation loss: 0.07840677689851656\n","Epoch #11\tTraining loss: 0.01333572867752141 \tValidation loss: 0.08244374445477191\n","Epoch #12\tTraining loss: 0.01293526418667167 \tValidation loss: 0.08973253982033719\n","Epoch #13\tTraining loss: 0.012577498512466753 \tValidation loss: 0.09432028763227943\n","Epoch #14\tTraining loss: 0.01226338194372025 \tValidation loss: 0.09690428680985812\n","Epoch #15\tTraining loss: 0.011982653111532434 \tValidation loss: 0.10166202161556\n","Epoch #16\tTraining loss: 0.011737913971382942 \tValidation loss: 0.10430101918006437\n","Epoch #17\tTraining loss: 0.01152339538557421 \tValidation loss: 0.10933800994677488\n","Epoch #18\tTraining loss: 0.011334304495141228 \tValidation loss: 0.11513785132687046\n","Epoch #19\tTraining loss: 0.011170609115679965 \tValidation loss: 0.11502444923850333\n","Epoch #20\tTraining loss: 0.011025162566203627 \tValidation loss: 0.11845554064054843\n","Epoch #21\tTraining loss: 0.010888674795863777 \tValidation loss: 0.12083238242780577\n","Epoch #22\tTraining loss: 0.01075299322073257 \tValidation loss: 0.11273067171968347\n","Epoch #23\tTraining loss: 0.0106134900244839 \tValidation loss: 0.11996811162262187\n","Epoch #24\tTraining loss: 0.01046963980580464 \tValidation loss: 0.12383601025054655\n","Epoch #25\tTraining loss: 0.010329947185280753 \tValidation loss: 0.1330381014628346\n","Epoch #26\tTraining loss: 0.010205506267923343 \tValidation loss: 0.13467921938206048\n","Epoch #27\tTraining loss: 0.010097977516725832 \tValidation loss: 0.133647261433185\n","Epoch #28\tTraining loss: 0.010004062664095927 \tValidation loss: 0.13201016377824187\n","Epoch #29\tTraining loss: 0.009923080714350463 \tValidation loss: 0.13511589418457262\n","Epoch #30\tTraining loss: 0.009851208423444103 \tValidation loss: 0.13392482002477632\n","Epoch #31\tTraining loss: 0.009783173233842046 \tValidation loss: 0.13356221905810373\n","Epoch #32\tTraining loss: 0.009717442585785628 \tValidation loss: 0.1337494482255942\n","Epoch #33\tTraining loss: 0.009654715151206734 \tValidation loss: 0.13377423382174838\n","Epoch #34\tTraining loss: 0.009589671650019501 \tValidation loss: 0.12912537298417381\n","Epoch #35\tTraining loss: 0.009524556034724211 \tValidation loss: 0.1307890744511391\n","Epoch #36\tTraining loss: 0.009457559100426864 \tValidation loss: 0.12746681160356915\n","Epoch #37\tTraining loss: 0.009392841926045883 \tValidation loss: 0.12956602798495995\n","Epoch #38\tTraining loss: 0.009332949213632997 \tValidation loss: 0.12303993207334935\n","Epoch #39\tTraining loss: 0.009278630683018247 \tValidation loss: 0.12759553437551327\n","Epoch #40\tTraining loss: 0.00922917923127432 \tValidation loss: 0.12904691340364516\n","Epoch #41\tTraining loss: 0.009183859421516784 \tValidation loss: 0.12914560963585764\n","Epoch #42\tTraining loss: 0.009142445225143911 \tValidation loss: 0.12283498598229313\n","Epoch #43\tTraining loss: 0.009103429853044026 \tValidation loss: 0.12426295481445825\n","Epoch #44\tTraining loss: 0.009067272840795811 \tValidation loss: 0.11843176679395373\n","Epoch #45\tTraining loss: 0.009032994188431117 \tValidation loss: 0.12247999237582574\n","Epoch #46\tTraining loss: 0.009001157000965605 \tValidation loss: 0.12300682175076762\n","Epoch #47\tTraining loss: 0.00897177146657477 \tValidation loss: 0.12213176849750133\n","Epoch #48\tTraining loss: 0.008941816938181274 \tValidation loss: 0.1163333142594587\n","Epoch #49\tTraining loss: 0.008915721882863914 \tValidation loss: 0.11438549507089622\n","Epoch #50\tTraining loss: 0.008889400532502674 \tValidation loss: 0.11533632079973369\n","Epoch #51\tTraining loss: 0.008864160464789836 \tValidation loss: 0.11291160026547287\n","Epoch #52\tTraining loss: 0.00883922860171649 \tValidation loss: 0.11261485120020977\n","Epoch #53\tTraining loss: 0.008811983285386604 \tValidation loss: 0.10784983186639703\n","Epoch #54\tTraining loss: 0.008783074321127978 \tValidation loss: 0.10721418736684071\n","Epoch #55\tTraining loss: 0.00875053557706342 \tValidation loss: 0.10257741752242458\n","Epoch #56\tTraining loss: 0.008714933816302608 \tValidation loss: 0.10252344344774994\n","Epoch #57\tTraining loss: 0.008677884503321966 \tValidation loss: 0.09832673083476198\n","Epoch #58\tTraining loss: 0.008639069379177414 \tValidation loss: 0.09932234153434738\n","Epoch #59\tTraining loss: 0.008603925102359121 \tValidation loss: 0.0875527851686606\n","Epoch #60\tTraining loss: 0.008572021689529462 \tValidation loss: 0.10575437502879505\n","Epoch #61\tTraining loss: 0.008544776889615554 \tValidation loss: 0.09964714846641363\n","Epoch #62\tTraining loss: 0.008521278779486921 \tValidation loss: 0.08635739566848653\n","Epoch #63\tTraining loss: 0.008500515776210275 \tValidation loss: 0.08072384868594136\n","Epoch #64\tTraining loss: 0.00847847032733351 \tValidation loss: 0.0966687469567594\n","Epoch #65\tTraining loss: 0.008452150696758866 \tValidation loss: 0.0881792883230039\n","Epoch #66\tTraining loss: 0.00842114958990711 \tValidation loss: 0.083262868636597\n","Epoch #67\tTraining loss: 0.008385390257969695 \tValidation loss: 0.07632163502759458\n","Epoch #68\tTraining loss: 0.008346952832790624 \tValidation loss: 0.07647860400045542\n","Epoch #69\tTraining loss: 0.008309016514986478 \tValidation loss: 0.067649481023524\n","Epoch #70\tTraining loss: 0.008273268321449345 \tValidation loss: 0.05614833387316644\n","Epoch #71\tTraining loss: 0.008237966248238682 \tValidation loss: 0.09052946451064425\n","Epoch #72\tTraining loss: 0.008204233458394165 \tValidation loss: 0.08297930629586937\n","Epoch #73\tTraining loss: 0.00817103039584229 \tValidation loss: 0.08253620669249398\n","Epoch #74\tTraining loss: 0.008136150462785114 \tValidation loss: 0.0711933692829564\n","Epoch #75\tTraining loss: 0.008100998907710518 \tValidation loss: 0.06364273116903069\n","Epoch #76\tTraining loss: 0.008063136348357404 \tValidation loss: 0.06022125333453951\n","Epoch #77\tTraining loss: 0.008024715123222285 \tValidation loss: 0.054198334776894744\n","Epoch #78\tTraining loss: 0.007986809647505949 \tValidation loss: 0.060968151332640455\n","Epoch #79\tTraining loss: 0.007949881812040755 \tValidation loss: 0.06681995955915662\n","Epoch #80\tTraining loss: 0.007914305585410102 \tValidation loss: 0.05903024389593091\n","Epoch #81\tTraining loss: 0.007881419124763589 \tValidation loss: 0.06347091608827714\n","Epoch #82\tTraining loss: 0.007849799691424244 \tValidation loss: 0.055963262654268695\n","Epoch #83\tTraining loss: 0.007819746150877915 \tValidation loss: 0.06663126508203386\n","Epoch #84\tTraining loss: 0.007790825052203838 \tValidation loss: 0.04677852145318427\n","Epoch #85\tTraining loss: 0.007761965107837319 \tValidation loss: 0.057193000130758134\n","Epoch #86\tTraining loss: 0.0077345271198758555 \tValidation loss: 0.028095497345511155\n","Epoch #87\tTraining loss: 0.007707511547323218 \tValidation loss: 0.09243193726806982\n","Epoch #88\tTraining loss: 0.007681433597342582 \tValidation loss: 0.061894595299365966\n","Epoch #89\tTraining loss: 0.007655595178955828 \tValidation loss: 0.06431268548774158\n","Epoch #90\tTraining loss: 0.007630028142682831 \tValidation loss: 0.0477993989380735\n","Epoch #91\tTraining loss: 0.007605143062526764 \tValidation loss: 0.04195082883365917\n","Epoch #92\tTraining loss: 0.007581913711545526 \tValidation loss: 0.043809898110092574\n","Epoch #93\tTraining loss: 0.007558687667365258 \tValidation loss: 0.043043150771995435\n","Epoch #94\tTraining loss: 0.007536960231464522 \tValidation loss: 0.04592916944596391\n","Epoch #95\tTraining loss: 0.00751528371862872 \tValidation loss: 0.04282076656793067\n","Epoch #96\tTraining loss: 0.00749387693986304 \tValidation loss: 0.04055197492095393\n","Epoch #97\tTraining loss: 0.007472953238150737 \tValidation loss: 0.03870475366689445\n","Epoch #98\tTraining loss: 0.007451579671358013 \tValidation loss: 0.03150851653354046\n","Epoch #99\tTraining loss: 0.007429596369868467 \tValidation loss: 0.04077223914696239\n","Epoch #100\tTraining loss: 0.007408575217584967 \tValidation loss: 0.02338480256253443\n","Epoch #101\tTraining loss: 0.007387714180518771 \tValidation loss: 0.0196164128738172\n","Epoch #102\tTraining loss: 0.007367024260145891 \tValidation loss: 0.027233750388946917\n","Epoch #103\tTraining loss: 0.007347702929019882 \tValidation loss: 0.037434933164442505\n","Epoch #104\tTraining loss: 0.007328841613037625 \tValidation loss: 0.033187320948254266\n","Epoch #105\tTraining loss: 0.007309568302265835 \tValidation loss: 0.01878720292431187\n","Epoch #106\tTraining loss: 0.007292093713826787 \tValidation loss: 0.02564802098122903\n","Epoch #107\tTraining loss: 0.00727354300069333 \tValidation loss: 0.032439113528044614\n","Epoch #108\tTraining loss: 0.007255654275319227 \tValidation loss: 0.028035141741827547\n","Epoch #109\tTraining loss: 0.0072380431077938715 \tValidation loss: 0.022519849108482242\n","Epoch #110\tTraining loss: 0.007219341285835358 \tValidation loss: 0.02764477411154389\n","Epoch #111\tTraining loss: 0.0072014180606902786 \tValidation loss: 0.022119609358039766\n","Epoch #112\tTraining loss: 0.007182893839789305 \tValidation loss: 0.015145154142940377\n","Epoch #113\tTraining loss: 0.007165106568961342 \tValidation loss: 0.02487056585269322\n","Epoch #114\tTraining loss: 0.007147951292313239 \tValidation loss: 0.026528806904611294\n","Epoch #115\tTraining loss: 0.00713083030830659 \tValidation loss: 0.025003187401997955\n","Epoch #116\tTraining loss: 0.007113812583333467 \tValidation loss: 0.012265063201303402\n","Epoch #117\tTraining loss: 0.00709764857172196 \tValidation loss: 0.018019846103033563\n","Epoch #118\tTraining loss: 0.0070820387672623775 \tValidation loss: 0.020001151396595265\n","Epoch #119\tTraining loss: 0.007066556409384924 \tValidation loss: 0.015255600509544325\n","Epoch #120\tTraining loss: 0.007051050084836152 \tValidation loss: 0.013449585125823385\n","Epoch #121\tTraining loss: 0.007035252090828461 \tValidation loss: 0.021738078959540184\n","Epoch #122\tTraining loss: 0.007018682436904384 \tValidation loss: 0.020130350419405824\n","Epoch #123\tTraining loss: 0.007002818480375039 \tValidation loss: 0.015249442320299598\n","Epoch #124\tTraining loss: 0.0069861692382957976 \tValidation loss: 0.016346057171184702\n","Epoch #125\tTraining loss: 0.0069706968434211074 \tValidation loss: 0.021969540365139293\n","Epoch #126\tTraining loss: 0.006954789039751903 \tValidation loss: 0.011023946074330816\n","Epoch #127\tTraining loss: 0.006938688675770368 \tValidation loss: 0.019777034539239482\n","Epoch #128\tTraining loss: 0.006922656982920459 \tValidation loss: 0.010251096326104558\n","Epoch #129\tTraining loss: 0.006906970743840697 \tValidation loss: 0.015262371421230452\n","Epoch #130\tTraining loss: 0.0068915414922671335 \tValidation loss: 0.01729497315594426\n","Epoch #131\tTraining loss: 0.00687655839898725 \tValidation loss: 0.009934977086604036\n","Epoch #132\tTraining loss: 0.006861212020202247 \tValidation loss: 0.00926096137540243\n","Epoch #133\tTraining loss: 0.00684674790985266 \tValidation loss: 0.021608127583978936\n","Epoch #134\tTraining loss: 0.006832815046808821 \tValidation loss: 0.010143102908973082\n","Epoch #135\tTraining loss: 0.00681811671960385 \tValidation loss: 0.009287427582973413\n","Epoch #136\tTraining loss: 0.006803771638312177 \tValidation loss: 0.009133495205091059\n","Epoch #137\tTraining loss: 0.006789850249059692 \tValidation loss: 0.008075199795744453\n","Epoch #138\tTraining loss: 0.006775078951903978 \tValidation loss: 0.01872180798488506\n","Epoch #139\tTraining loss: 0.006761252806123281 \tValidation loss: 0.010601527148044342\n","Epoch #140\tTraining loss: 0.006746521370605831 \tValidation loss: 0.017309232038942836\n","Epoch #141\tTraining loss: 0.006733696781571263 \tValidation loss: 0.012669268363769087\n","Epoch #142\tTraining loss: 0.0067198806842394 \tValidation loss: 0.013386343685361523\n","Epoch #143\tTraining loss: 0.006706677616786296 \tValidation loss: 0.02800780256657879\n","Epoch #144\tTraining loss: 0.006691879392742019 \tValidation loss: 0.00878814508251442\n","Epoch #145\tTraining loss: 0.006679289739137245 \tValidation loss: 0.00776795353551549\n","Epoch #146\tTraining loss: 0.0066649043131222835 \tValidation loss: 0.010487196033336227\n","Epoch #147\tTraining loss: 0.006652064235302641 \tValidation loss: 0.010283686578675745\n","Epoch #148\tTraining loss: 0.006638483987833723 \tValidation loss: 0.019124721474403767\n","Epoch #149\tTraining loss: 0.006626642891335127 \tValidation loss: 0.00761555753880189\n","Epoch #150\tTraining loss: 0.006612106560579531 \tValidation loss: 0.012142938605882841\n","Epoch #151\tTraining loss: 0.006599102387280957 \tValidation loss: 0.008411393847424793\n","Epoch #152\tTraining loss: 0.006584429561811139 \tValidation loss: 0.019439239542114243\n","Epoch #153\tTraining loss: 0.006573996606892947 \tValidation loss: 0.01709253429533539\n","Epoch #154\tTraining loss: 0.006561365072926459 \tValidation loss: 0.025614543493897846\n","Epoch #155\tTraining loss: 0.006549277888838352 \tValidation loss: 0.02329624704284481\n","Epoch #156\tTraining loss: 0.006535022232744923 \tValidation loss: 0.011806841384563024\n","Epoch #157\tTraining loss: 0.006524221484460728 \tValidation loss: 0.010005628882449501\n","Epoch #158\tTraining loss: 0.006510736412462432 \tValidation loss: 0.017325183824321765\n","Epoch #159\tTraining loss: 0.006500712716062838 \tValidation loss: 0.02099768612543453\n","Epoch #160\tTraining loss: 0.00648643157346663 \tValidation loss: 0.02609376212257698\n","Epoch #161\tTraining loss: 0.006474866048796571 \tValidation loss: 0.019406194166809995\n","Epoch #162\tTraining loss: 0.006460935257020453 \tValidation loss: 0.0449745429383248\n","Epoch #163\tTraining loss: 0.0064506057636310255 \tValidation loss: 0.012282895756093416\n","Epoch #164\tTraining loss: 0.006436985175693141 \tValidation loss: 0.014033976656225005\n","Epoch #165\tTraining loss: 0.006425895323265359 \tValidation loss: 0.01113861949271224\n","Epoch #166\tTraining loss: 0.006412898951078771 \tValidation loss: 0.008256984324789685\n","Epoch #167\tTraining loss: 0.006401113775021689 \tValidation loss: 0.011211372686027593\n","Epoch #168\tTraining loss: 0.006388403145187192 \tValidation loss: 0.009806927673654669\n","Epoch #169\tTraining loss: 0.006376640541280502 \tValidation loss: 0.01477124470207031\n","Epoch #170\tTraining loss: 0.006363967557116878 \tValidation loss: 0.008389864698004307\n","Epoch #171\tTraining loss: 0.006352025541805807 \tValidation loss: 0.008993193769644976\n","Epoch #172\tTraining loss: 0.006340690071965264 \tValidation loss: 0.007977827251480307\n","Epoch #173\tTraining loss: 0.006328162510899494 \tValidation loss: 0.012715141742413125\n","Epoch #174\tTraining loss: 0.006318178681710323 \tValidation loss: 0.021843739469971313\n","Epoch #175\tTraining loss: 0.006306835940899354 \tValidation loss: 0.009357217178750916\n","Epoch #176\tTraining loss: 0.006294725367493334 \tValidation loss: 0.018218622174350538\n","Epoch #177\tTraining loss: 0.006283469244849962 \tValidation loss: 0.018330824859064544\n","Epoch #178\tTraining loss: 0.006272811205274318 \tValidation loss: 0.026511319826916638\n","Epoch #179\tTraining loss: 0.006263459991540514 \tValidation loss: 0.009643380950995858\n","Epoch #180\tTraining loss: 0.006253545990831714 \tValidation loss: 0.010606572130105997\n","Epoch #181\tTraining loss: 0.006243940563036764 \tValidation loss: 0.01944884507407027\n","Epoch #182\tTraining loss: 0.006235296612468758 \tValidation loss: 0.013502336877885684\n","Epoch #183\tTraining loss: 0.006225962669195025 \tValidation loss: 0.009380309760272069\n","Epoch #184\tTraining loss: 0.006216315012081876 \tValidation loss: 0.01710134758566669\n","Epoch #185\tTraining loss: 0.00620752836652716 \tValidation loss: 0.010259125041794381\n","Epoch #186\tTraining loss: 0.006199621427380407 \tValidation loss: 0.01823813860685002\n","Epoch #187\tTraining loss: 0.006191919522411497 \tValidation loss: 0.007230869624120921\n","Epoch #188\tTraining loss: 0.006184581955901569 \tValidation loss: 0.015095738054612843\n","Epoch #189\tTraining loss: 0.006178580765926754 \tValidation loss: 0.0077321871558578515\n","Epoch #190\tTraining loss: 0.006172143433109295 \tValidation loss: 0.023730805351528956\n","Epoch #191\tTraining loss: 0.006165744331687687 \tValidation loss: 0.02064232972978588\n","Epoch #192\tTraining loss: 0.0061610084493523695 \tValidation loss: 0.01149288081450044\n","Epoch #193\tTraining loss: 0.006156867169864682 \tValidation loss: 0.030226852745353635\n","Epoch #194\tTraining loss: 0.006151316506935922 \tValidation loss: 0.012652353477486545\n","Epoch #195\tTraining loss: 0.006147669020604479 \tValidation loss: 0.01368443705993396\n","Epoch #196\tTraining loss: 0.006141671864070656 \tValidation loss: 0.01962772649290019\n","Epoch #197\tTraining loss: 0.0061365310483581055 \tValidation loss: 0.008238442811068766\n","Epoch #198\tTraining loss: 0.0061284714665421815 \tValidation loss: 0.02953177837252344\n","Epoch #199\tTraining loss: 0.006121496125187222 \tValidation loss: 0.03908241236352896\n","Epoch #200\tTraining loss: 0.006108795165252756 \tValidation loss: 0.026125023266288638\n","Epoch #201\tTraining loss: 0.006094642599599574 \tValidation loss: 0.011425812013188812\n","Epoch #202\tTraining loss: 0.006079480660145287 \tValidation loss: 0.008222230330379159\n","Epoch #203\tTraining loss: 0.006062801499292935 \tValidation loss: 0.00730675743730104\n","Epoch #204\tTraining loss: 0.006046079140892209 \tValidation loss: 0.00983239001308215\n","Epoch #205\tTraining loss: 0.006029003594232006 \tValidation loss: 0.009979403254864282\n","Epoch #206\tTraining loss: 0.006016102100565548 \tValidation loss: 0.013161222976482262\n","Epoch #207\tTraining loss: 0.00600522641253807 \tValidation loss: 0.01066736811995499\n","Epoch #208\tTraining loss: 0.005998067428022471 \tValidation loss: 0.012248486148060124\n","Epoch #209\tTraining loss: 0.0059947182984659945 \tValidation loss: 0.015529569327191262\n","Epoch #210\tTraining loss: 0.005995962658062632 \tValidation loss: 0.007959292926170066\n","Epoch #211\tTraining loss: 0.0060028471756590025 \tValidation loss: 0.023425204043278844\n","Epoch #212\tTraining loss: 0.006021311000229856 \tValidation loss: 0.09412071709120051\n","Epoch #213\tTraining loss: 0.006057032473859347 \tValidation loss: 0.022778388807761004\n","Epoch #214\tTraining loss: 0.006126763059567916 \tValidation loss: 0.10353009747648279\n","Epoch #215\tTraining loss: 0.006246691871473872 \tValidation loss: 0.027300793379850467\n","Epoch #216\tTraining loss: 0.006382559074871619 \tValidation loss: 0.007497176962534906\n","Epoch #217\tTraining loss: 0.00635393987398419 \tValidation loss: 0.03343827372307215\n","Epoch #218\tTraining loss: 0.006168734646831506 \tValidation loss: 0.03116854214023925\n","Epoch #219\tTraining loss: 0.006039877070327157 \tValidation loss: 0.025303141076732708\n","Epoch #220\tTraining loss: 0.005963483789217514 \tValidation loss: 0.012728105960964993\n","Epoch #221\tTraining loss: 0.005909495917695335 \tValidation loss: 0.008255492981391235\n","Epoch #222\tTraining loss: 0.005875325062177793 \tValidation loss: 0.010284486710408231\n","Epoch #223\tTraining loss: 0.005846676009563254 \tValidation loss: 0.015477573255476138\n","Epoch #224\tTraining loss: 0.005822796058511032 \tValidation loss: 0.015507317568016977\n","Epoch #225\tTraining loss: 0.005802273734732639 \tValidation loss: 0.009834905709080211\n","Epoch #226\tTraining loss: 0.00578429561483825 \tValidation loss: 0.014299482807431887\n","Epoch #227\tTraining loss: 0.005768092165554782 \tValidation loss: 0.011625373142154744\n","Epoch #228\tTraining loss: 0.0057546894307466965 \tValidation loss: 0.010419864083667652\n","Epoch #229\tTraining loss: 0.005743714418888475 \tValidation loss: 0.008091180056424637\n","Epoch #230\tTraining loss: 0.005733820226848769 \tValidation loss: 0.022255012007930137\n","Epoch #231\tTraining loss: 0.005723516238158028 \tValidation loss: 0.007403031127388244\n","Epoch #232\tTraining loss: 0.005714363712300102 \tValidation loss: 0.015570447803836809\n","Epoch #233\tTraining loss: 0.005703816835795894 \tValidation loss: 0.020625723382961984\n","Epoch #234\tTraining loss: 0.005694449190745537 \tValidation loss: 0.02028185949621235\n","Epoch #235\tTraining loss: 0.005684701342796302 \tValidation loss: 0.013154266148438657\n","Epoch #236\tTraining loss: 0.005677018119653312 \tValidation loss: 0.01941244864896646\n","Epoch #237\tTraining loss: 0.005667674070450001 \tValidation loss: 0.007783251931530541\n","Epoch #238\tTraining loss: 0.00565895679536793 \tValidation loss: 0.008176417308644572\n","Epoch #239\tTraining loss: 0.00565037884004348 \tValidation loss: 0.009059535285131171\n","Epoch #240\tTraining loss: 0.0056411975653194195 \tValidation loss: 0.010000754881425579\n","Epoch #241\tTraining loss: 0.005633548561172625 \tValidation loss: 0.008081342765179288\n","Epoch #242\tTraining loss: 0.0056246683367509745 \tValidation loss: 0.007925536960611398\n","Epoch #243\tTraining loss: 0.005615960765279596 \tValidation loss: 0.012525139445942074\n","Epoch #244\tTraining loss: 0.005608909275597231 \tValidation loss: 0.007763032153168565\n","Epoch #245\tTraining loss: 0.005600381836934931 \tValidation loss: 0.02027132369573362\n","Epoch #246\tTraining loss: 0.005592397833157108 \tValidation loss: 0.010485758007076237\n","Epoch #247\tTraining loss: 0.005585018656370197 \tValidation loss: 0.008774752973403759\n","Epoch #248\tTraining loss: 0.005575753831559024 \tValidation loss: 0.017599211403214066\n","Epoch #249\tTraining loss: 0.005570141751799456 \tValidation loss: 0.016969916861244275\n","Epoch #250\tTraining loss: 0.005560560684553931 \tValidation loss: 0.020562849497465046\n","Epoch #251\tTraining loss: 0.0055539716688717735 \tValidation loss: 0.021112596208529028\n","Epoch #252\tTraining loss: 0.0055468231718879685 \tValidation loss: 0.008660883173643883\n","Epoch #253\tTraining loss: 0.0055367227218398925 \tValidation loss: 0.008805739471120963\n","Epoch #254\tTraining loss: 0.005532155000782425 \tValidation loss: 0.016254298999037145\n","Epoch #255\tTraining loss: 0.005523260760503998 \tValidation loss: 0.013536031336597592\n","Epoch #256\tTraining loss: 0.005515187408186836 \tValidation loss: 0.04452951725616774\n","Epoch #257\tTraining loss: 0.005510575280491241 \tValidation loss: 0.007370841744719868\n","Epoch #258\tTraining loss: 0.005500051950953068 \tValidation loss: 0.01699608924391829\n","Epoch #259\tTraining loss: 0.005494755695747012 \tValidation loss: 0.023067854804211613\n","Epoch #260\tTraining loss: 0.005484920694247882 \tValidation loss: 0.04469344686647166\n","Epoch #261\tTraining loss: 0.005479788620151794 \tValidation loss: 0.03162941853877459\n","Epoch #262\tTraining loss: 0.005472437881502986 \tValidation loss: 0.01597459300989871\n","Epoch #263\tTraining loss: 0.005462680282717246 \tValidation loss: 0.015042099356466904\n","Epoch #264\tTraining loss: 0.0054604856154090555 \tValidation loss: 0.013240733639506367\n","Epoch #265\tTraining loss: 0.005449084203582577 \tValidation loss: 0.008896292342670056\n","Epoch #266\tTraining loss: 0.005443902138693757 \tValidation loss: 0.01049562780762768\n","Epoch #267\tTraining loss: 0.0054377553703030745 \tValidation loss: 0.033282142318391654\n","Epoch #268\tTraining loss: 0.005427443803637032 \tValidation loss: 0.0103756114528543\n","Epoch #269\tTraining loss: 0.005423084802474407 \tValidation loss: 0.00900200789937368\n","Epoch #270\tTraining loss: 0.005414405696603433 \tValidation loss: 0.015351246168466668\n","Epoch #271\tTraining loss: 0.005407939354486097 \tValidation loss: 0.02549031372082671\n","Epoch #272\tTraining loss: 0.005399874125909522 \tValidation loss: 0.026766332315653646\n","Epoch #273\tTraining loss: 0.005393869273079194 \tValidation loss: 0.0088232881539028\n","Epoch #274\tTraining loss: 0.005386024590366698 \tValidation loss: 0.0218023747094843\n","Epoch #275\tTraining loss: 0.00537966850005932 \tValidation loss: 0.008764168920001638\n","Epoch #276\tTraining loss: 0.00537235663186869 \tValidation loss: 0.023162863417617887\n","Epoch #277\tTraining loss: 0.005365346689257521 \tValidation loss: 0.020444814729258284\n","Epoch #278\tTraining loss: 0.005359984029106907 \tValidation loss: 0.014079477623539772\n","Epoch #279\tTraining loss: 0.005350302834806075 \tValidation loss: 0.015110684906805087\n","Epoch #280\tTraining loss: 0.005347999688426037 \tValidation loss: 0.016924986060923977\n","Epoch #281\tTraining loss: 0.005336424959474308 \tValidation loss: 0.0071207627617892785\n","Epoch #282\tTraining loss: 0.005334940879047051 \tValidation loss: 0.010995823578609006\n","Epoch #283\tTraining loss: 0.0053225377102814764 \tValidation loss: 0.008087477299259538\n","Epoch #284\tTraining loss: 0.005321686293880194 \tValidation loss: 0.027999169347842495\n","Epoch #285\tTraining loss: 0.005312708285266868 \tValidation loss: 0.007204414020466533\n","Epoch #286\tTraining loss: 0.005302141145726022 \tValidation loss: 0.007411310445026533\n","Epoch #287\tTraining loss: 0.00530055724528005 \tValidation loss: 0.00954496863935337\n","Epoch #288\tTraining loss: 0.0052922419339865795 \tValidation loss: 0.027243283451026043\n","Epoch #289\tTraining loss: 0.005282071126686037 \tValidation loss: 0.028604613140953902\n","Epoch #290\tTraining loss: 0.005279444392369616 \tValidation loss: 0.03290871871501817\n","Epoch #291\tTraining loss: 0.005269159527580841 \tValidation loss: 0.008780668142878185\n","Epoch #292\tTraining loss: 0.005266604958164781 \tValidation loss: 0.009347389240834775\n","Epoch #293\tTraining loss: 0.005259469932417142 \tValidation loss: 0.012399841613964985\n","Epoch #294\tTraining loss: 0.005248315233834761 \tValidation loss: 0.01270176636935416\n","Epoch #295\tTraining loss: 0.005246208412508233 \tValidation loss: 0.02247514999025343\n","Epoch #296\tTraining loss: 0.005241467056747711 \tValidation loss: 0.015948343587940072\n","Epoch #297\tTraining loss: 0.005227868412697648 \tValidation loss: 0.016105618726186207\n","Epoch #298\tTraining loss: 0.00522566712941586 \tValidation loss: 0.00777941711013915\n","Epoch #299\tTraining loss: 0.005221224165181648 \tValidation loss: 0.03377336232680019\n","Epoch #300\tTraining loss: 0.005208504852384356 \tValidation loss: 0.011030880664725833\n","Epoch #301\tTraining loss: 0.005207474399120519 \tValidation loss: 0.05494685728380921\n","Epoch #302\tTraining loss: 0.005200690171850083 \tValidation loss: 0.034601677260926826\n","Epoch #303\tTraining loss: 0.0051885346293452485 \tValidation loss: 0.011088678625532918\n","Epoch #304\tTraining loss: 0.005186584224567859 \tValidation loss: 0.06042952608993205\n","Epoch #305\tTraining loss: 0.005182079695480949 \tValidation loss: 0.00832776983513341\n","Epoch #306\tTraining loss: 0.005169170155408905 \tValidation loss: 0.0258783052211636\n","Epoch #307\tTraining loss: 0.005168089495117124 \tValidation loss: 0.022400289039827104\n","Epoch #308\tTraining loss: 0.005162093002547546 \tValidation loss: 0.021163167965484744\n","Epoch #309\tTraining loss: 0.005151185594569119 \tValidation loss: 0.02090819815497602\n","Epoch #310\tTraining loss: 0.0051498559058802415 \tValidation loss: 0.021410411856584734\n","Epoch #311\tTraining loss: 0.0051434630929792285 \tValidation loss: 0.03270569150325314\n","Epoch #312\tTraining loss: 0.0051300155830195085 \tValidation loss: 0.014868662766721977\n","Epoch #313\tTraining loss: 0.005130499836287536 \tValidation loss: 0.006807920000151739\n","Epoch #314\tTraining loss: 0.005119204539761591 \tValidation loss: 0.08240359378135766\n","Epoch #315\tTraining loss: 0.0051221746352008764 \tValidation loss: 0.025158311660900094\n","Epoch #316\tTraining loss: 0.005105680073585963 \tValidation loss: 0.010957433090056354\n","Epoch #317\tTraining loss: 0.005118756438273271 \tValidation loss: 0.01135087528163461\n","Epoch #318\tTraining loss: 0.005102420202535397 \tValidation loss: 0.01216766902657865\n","Epoch #319\tTraining loss: 0.005093942190344347 \tValidation loss: 0.012313063303867482\n","Epoch #320\tTraining loss: 0.005082347549080291 \tValidation loss: 0.03318970754925024\n","Epoch #321\tTraining loss: 0.0050889939277719435 \tValidation loss: 0.04189452075014144\n","Epoch #322\tTraining loss: 0.0050757005395307715 \tValidation loss: 0.012238463552941023\n","Epoch #323\tTraining loss: 0.0050932822286673075 \tValidation loss: 0.019621181540932554\n","Epoch #324\tTraining loss: 0.00512389600190101 \tValidation loss: 0.018234230239956926\n","Epoch #325\tTraining loss: 0.0052085892986250175 \tValidation loss: 0.02879379648081799\n","Epoch #326\tTraining loss: 0.005171653952859787 \tValidation loss: 0.14882854722658634\n","Epoch #327\tTraining loss: 0.005173052539713434 \tValidation loss: 0.16248588415995915\n","Epoch #328\tTraining loss: 0.005135743276856864 \tValidation loss: 0.05733955364963306\n","Epoch #329\tTraining loss: 0.00507368821347141 \tValidation loss: 0.014759317432922324\n","Epoch #330\tTraining loss: 0.005087413318911383 \tValidation loss: 0.012398428707998644\n","Epoch #331\tTraining loss: 0.005079676600012383 \tValidation loss: 0.036034570327160606\n","Epoch #332\tTraining loss: 0.005016421119987927 \tValidation loss: 0.007558311399694663\n","Epoch #333\tTraining loss: 0.005015239935161775 \tValidation loss: 0.008180441527026058\n","Epoch #334\tTraining loss: 0.004982132909938706 \tValidation loss: 0.046431257063522205\n","Epoch #335\tTraining loss: 0.005015450701645339 \tValidation loss: 0.036806288649658216\n","Epoch #336\tTraining loss: 0.005041451681812576 \tValidation loss: 0.01230838225012581\n","Epoch #337\tTraining loss: 0.005024893235590877 \tValidation loss: 0.026140437500020916\n","Epoch #338\tTraining loss: 0.004981134779210935 \tValidation loss: 0.009294740809203582\n","Epoch #339\tTraining loss: 0.005010904197527916 \tValidation loss: 0.025271423731925095\n","Epoch #340\tTraining loss: 0.004965525038625453 \tValidation loss: 0.04577455879645267\n","Epoch #341\tTraining loss: 0.005003292664739664 \tValidation loss: 0.04439887998200154\n","Epoch #342\tTraining loss: 0.005051848903738976 \tValidation loss: 0.03477181726669227\n","Epoch #343\tTraining loss: 0.005022705846402619 \tValidation loss: 0.02367496854798127\n","Epoch #344\tTraining loss: 0.004955641550495797 \tValidation loss: 0.0860973177801264\n","Epoch #345\tTraining loss: 0.005028734441507319 \tValidation loss: 0.021431579658275017\n","Epoch #346\tTraining loss: 0.004968615256538175 \tValidation loss: 0.019718565891764683\n","Epoch #347\tTraining loss: 0.005000097259664954 \tValidation loss: 0.04378499335897014\n","Epoch #348\tTraining loss: 0.005000987941953214 \tValidation loss: 0.007690437928905311\n","Epoch #349\tTraining loss: 0.004955119080343948 \tValidation loss: 0.019353634592015855\n","Epoch #350\tTraining loss: 0.004898959072231813 \tValidation loss: 0.030692320637657547\n","Epoch #351\tTraining loss: 0.004915829576674436 \tValidation loss: 0.0194317603369937\n","Epoch #352\tTraining loss: 0.004918172657372381 \tValidation loss: 0.05879046276657854\n","Epoch #353\tTraining loss: 0.004962364458394698 \tValidation loss: 0.014633599247490293\n","Epoch #354\tTraining loss: 0.004935498156521929 \tValidation loss: 0.010721307563297259\n","Epoch #355\tTraining loss: 0.004906498215263344 \tValidation loss: 0.009568146315286219\n","Epoch #356\tTraining loss: 0.004874400738862025 \tValidation loss: 0.015926501323727485\n","Epoch #357\tTraining loss: 0.004875063379774382 \tValidation loss: 0.021041870243206133\n","Epoch #358\tTraining loss: 0.0049067042165136234 \tValidation loss: 0.04489395485809841\n","Epoch #359\tTraining loss: 0.004946251916210846 \tValidation loss: 0.008078892489607067\n","Epoch #360\tTraining loss: 0.004880962980059047 \tValidation loss: 0.02777789121091732\n","Epoch #361\tTraining loss: 0.004855511669178208 \tValidation loss: 0.049870053068674355\n","Epoch #362\tTraining loss: 0.004830553596606501 \tValidation loss: 0.04700555510702724\n","Epoch #363\tTraining loss: 0.004843887477264145 \tValidation loss: 0.008622815121435678\n","Epoch #364\tTraining loss: 0.0049039283162825706 \tValidation loss: 0.041320244372523386\n","Epoch #365\tTraining loss: 0.00493439959090814 \tValidation loss: 0.013898626998067376\n","Epoch #366\tTraining loss: 0.0048443177926778625 \tValidation loss: 0.03790194802911173\n","Epoch #367\tTraining loss: 0.0048346637012545175 \tValidation loss: 0.008271711447695906\n","Epoch #368\tTraining loss: 0.004807208821610658 \tValidation loss: 0.03513570886932766\n","Epoch #369\tTraining loss: 0.004820579040129863 \tValidation loss: 0.04618681577961616\n","Epoch #370\tTraining loss: 0.004892615652124558 \tValidation loss: 0.01110751050169929\n","Epoch #371\tTraining loss: 0.004913226085610412 \tValidation loss: 0.03918809723650924\n","Epoch #372\tTraining loss: 0.0048086881098535796 \tValidation loss: 0.0075712256798936865\n","Epoch #373\tTraining loss: 0.004803527652402671 \tValidation loss: 0.04394337200034993\n","Epoch #374\tTraining loss: 0.004782261017710981 \tValidation loss: 0.05016845267084866\n","Epoch #375\tTraining loss: 0.004804778038835184 \tValidation loss: 0.08737497056695444\n","Epoch #376\tTraining loss: 0.004864443353806562 \tValidation loss: 0.04950654733845854\n","Epoch #377\tTraining loss: 0.00485060637014592 \tValidation loss: 0.02620830440897811\n","Epoch #378\tTraining loss: 0.0047623858115025544 \tValidation loss: 0.03027342839373973\n","Epoch #379\tTraining loss: 0.004747235353435595 \tValidation loss: 0.010649141625748437\n","Epoch #380\tTraining loss: 0.004743833316929256 \tValidation loss: 0.009851646582320826\n","Epoch #381\tTraining loss: 0.004779778285165921 \tValidation loss: 0.009834734685646596\n","Epoch #382\tTraining loss: 0.004800064748984628 \tValidation loss: 0.028959114271797055\n","Epoch #383\tTraining loss: 0.0047946872092977134 \tValidation loss: 0.009692596453506609\n","Epoch #384\tTraining loss: 0.004731860058605265 \tValidation loss: 0.03904183223866734\n","Epoch #385\tTraining loss: 0.00471047898210352 \tValidation loss: 0.03221070654682927\n","Epoch #386\tTraining loss: 0.004719547343610717 \tValidation loss: 0.025881380995671742\n","Epoch #387\tTraining loss: 0.004740130567459237 \tValidation loss: 0.007117800115375961\n","Epoch #388\tTraining loss: 0.004779788423446694 \tValidation loss: 0.025539444223442568\n","Epoch #389\tTraining loss: 0.004756827532403646 \tValidation loss: 0.0194226927252654\n","Epoch #390\tTraining loss: 0.004702924539493604 \tValidation loss: 0.0369874943828287\n","Epoch #391\tTraining loss: 0.004671952519959711 \tValidation loss: 0.0068787401507660615\n","Epoch #392\tTraining loss: 0.0046830150592579636 \tValidation loss: 0.011216850492359362\n","Epoch #393\tTraining loss: 0.0046890612830615816 \tValidation loss: 0.016159673912116708\n","Epoch #394\tTraining loss: 0.004776937092712878 \tValidation loss: 0.022226322625411497\n","Epoch #395\tTraining loss: 0.004714745161959898 \tValidation loss: 0.03213464217232639\n","Epoch #396\tTraining loss: 0.004666192088017757 \tValidation loss: 0.00924831297972046\n","Epoch #397\tTraining loss: 0.004659148223948506 \tValidation loss: 0.019817728915086708\n","Epoch #398\tTraining loss: 0.004664008127408307 \tValidation loss: 0.05156882978035248\n","Epoch #399\tTraining loss: 0.0046935145850140695 \tValidation loss: 0.008146699582989771\n","Epoch #400\tTraining loss: 0.004776764607871478 \tValidation loss: 0.007338542342034024\n","Epoch #401\tTraining loss: 0.004676491655911609 \tValidation loss: 0.06831683507446978\n","Epoch #402\tTraining loss: 0.004611896048631092 \tValidation loss: 0.01697175876878525\n","Epoch #403\tTraining loss: 0.004634101085068297 \tValidation loss: 0.017262439024118066\n","Epoch #404\tTraining loss: 0.004650623365595981 \tValidation loss: 0.0360152587651384\n","Epoch #405\tTraining loss: 0.004701234766017226 \tValidation loss: 0.013416800480200292\n","Epoch #406\tTraining loss: 0.004640455666643125 \tValidation loss: 0.035960156982045896\n","Epoch #407\tTraining loss: 0.004613014240850325 \tValidation loss: 0.008481578965085645\n","Epoch #408\tTraining loss: 0.004656032320884476 \tValidation loss: 0.05725252272523388\n","Epoch #409\tTraining loss: 0.0046460269708887576 \tValidation loss: 0.03810373572869493\n","Epoch #410\tTraining loss: 0.004665736841363909 \tValidation loss: 0.0068606529445630554\n","Epoch #411\tTraining loss: 0.004673450924112675 \tValidation loss: 0.039954780922720465\n","Epoch #412\tTraining loss: 0.004700368664775274 \tValidation loss: 0.014359070614410437\n","Epoch #413\tTraining loss: 0.004742428288448681 \tValidation loss: 0.008357031056908335\n","Epoch #414\tTraining loss: 0.004823485176197591 \tValidation loss: 0.035288911612217394\n","Epoch #415\tTraining loss: 0.004909420943506677 \tValidation loss: 0.030054946025240364\n","Epoch #416\tTraining loss: 0.00499973572321379 \tValidation loss: 0.052930799900488976\n","Epoch #417\tTraining loss: 0.005394043941992022 \tValidation loss: 0.025978313752071762\n","Epoch #418\tTraining loss: 0.006446759862796401 \tValidation loss: 0.13953436898373403\n","Epoch #419\tTraining loss: 0.007462293790740139 \tValidation loss: 0.0301104609541283\n","Epoch #420\tTraining loss: 0.007268325136578177 \tValidation loss: 0.012769725318780387\n","Epoch #421\tTraining loss: 0.006443210811817549 \tValidation loss: 0.050675302078455636\n","Epoch #422\tTraining loss: 0.005457146269726529 \tValidation loss: 0.06331079837243882\n","Epoch #423\tTraining loss: 0.0049377331395231185 \tValidation loss: 0.05505374941660057\n","Epoch #424\tTraining loss: 0.004773076615522716 \tValidation loss: 0.016161676036044244\n","Epoch #425\tTraining loss: 0.004702677219405742 \tValidation loss: 0.024924154006714622\n","Epoch #426\tTraining loss: 0.004683531610441267 \tValidation loss: 0.018308997193643164\n","Epoch #427\tTraining loss: 0.004646826703343154 \tValidation loss: 0.0671667589046414\n","Epoch #428\tTraining loss: 0.004634149362979095 \tValidation loss: 0.02004842394610114\n","Epoch #429\tTraining loss: 0.004637721101547977 \tValidation loss: 0.022730527737930595\n","Epoch #430\tTraining loss: 0.00464423259402893 \tValidation loss: 0.01003722170543747\n","Epoch #431\tTraining loss: 0.00464421080740442 \tValidation loss: 0.025480433045597252\n","Epoch #432\tTraining loss: 0.004636021915540407 \tValidation loss: 0.012031828688654234\n","Epoch #433\tTraining loss: 0.0046194374611006036 \tValidation loss: 0.2338576983403009\n","Epoch #434\tTraining loss: 0.004596632382497314 \tValidation loss: 0.1459909628173917\n","Epoch #435\tTraining loss: 0.004571434259335529 \tValidation loss: 0.04121277733893092\n","Epoch #436\tTraining loss: 0.004548050003475678 \tValidation loss: 0.006557370171820268\n","Epoch #437\tTraining loss: 0.00452916981373354 \tValidation loss: 0.0455187893583025\n","Epoch #438\tTraining loss: 0.004509896794278144 \tValidation loss: 0.023431358346864977\n","Epoch #439\tTraining loss: 0.0044936798000305925 \tValidation loss: 0.015375253886111393\n","Epoch #440\tTraining loss: 0.004479297437658065 \tValidation loss: 0.012267373707088422\n","Epoch #441\tTraining loss: 0.00446542455246448 \tValidation loss: 0.041153022384735\n","Epoch #442\tTraining loss: 0.004455882135199518 \tValidation loss: 0.023265462217613057\n","Epoch #443\tTraining loss: 0.004450544924754682 \tValidation loss: 0.010775762178193088\n","Epoch #444\tTraining loss: 0.004455630896615831 \tValidation loss: 0.010556507897019984\n","Epoch #445\tTraining loss: 0.004464832518135268 \tValidation loss: 0.023974458224119696\n","Epoch #446\tTraining loss: 0.004485509450822835 \tValidation loss: 0.010425123213712802\n","Epoch #447\tTraining loss: 0.004495739302442784 \tValidation loss: 0.009536491603265553\n","Epoch #448\tTraining loss: 0.004484908810158623 \tValidation loss: 0.01940670757453469\n","Epoch #449\tTraining loss: 0.004455863035725538 \tValidation loss: 0.006559050583858733\n","Epoch #450\tTraining loss: 0.004444733373517398 \tValidation loss: 0.012110797724709124\n","Epoch #451\tTraining loss: 0.004466212675967789 \tValidation loss: 0.00913691668977879\n","Epoch #452\tTraining loss: 0.004553988326031298 \tValidation loss: 0.02814304430337468\n","Epoch #453\tTraining loss: 0.004719404945336256 \tValidation loss: 0.036476561842200765\n","Epoch #454\tTraining loss: 0.004791554008957282 \tValidation loss: 0.11275320606977894\n","Epoch #455\tTraining loss: 0.004938452259319095 \tValidation loss: 0.08993466186897625\n","Epoch #456\tTraining loss: 0.004947331126967541 \tValidation loss: 0.01082684184375802\n","Epoch #457\tTraining loss: 0.004910687391267468 \tValidation loss: 0.02808788329811797\n","Epoch #458\tTraining loss: 0.004825978840739061 \tValidation loss: 0.05151668122787413\n","Epoch #459\tTraining loss: 0.004622249799612969 \tValidation loss: 0.00953931011783321\n","Epoch #460\tTraining loss: 0.00467181865593188 \tValidation loss: 0.05480610622760779\n","Epoch #461\tTraining loss: 0.004571668190124611 \tValidation loss: 0.015365459503963336\n","Epoch #462\tTraining loss: 0.004425509420752046 \tValidation loss: 0.011915426112857555\n","Epoch #463\tTraining loss: 0.004342303810118598 \tValidation loss: 0.015651690134137024\n","Epoch #464\tTraining loss: 0.004331081763620057 \tValidation loss: 0.017078072395171146\n","Epoch #465\tTraining loss: 0.004353908769869482 \tValidation loss: 0.05737171629850231\n","Epoch #466\tTraining loss: 0.004386320791925819 \tValidation loss: 0.05761386080657824\n","Epoch #467\tTraining loss: 0.004389804132452083 \tValidation loss: 0.027255846539001486\n","Epoch #468\tTraining loss: 0.004371949439928456 \tValidation loss: 0.007117695495159352\n","Epoch #469\tTraining loss: 0.00434128315298021 \tValidation loss: 0.008132070138964442\n","Epoch #470\tTraining loss: 0.004330937600154199 \tValidation loss: 0.015085167837969935\n","Epoch #471\tTraining loss: 0.004373697123475446 \tValidation loss: 0.03300409184834084\n","Epoch #472\tTraining loss: 0.004402904941504452 \tValidation loss: 0.019588900469003696\n","Epoch #473\tTraining loss: 0.004404723797942903 \tValidation loss: 0.03221761539667824\n","Epoch #474\tTraining loss: 0.0043845288777092616 \tValidation loss: 0.013005460456716878\n","Epoch #475\tTraining loss: 0.004386882791899723 \tValidation loss: 0.008761075279568181\n","Epoch #476\tTraining loss: 0.004478957024875452 \tValidation loss: 0.019535900830140052\n","Epoch #477\tTraining loss: 0.0046591117192142486 \tValidation loss: 0.015491241393846509\n","Epoch #478\tTraining loss: 0.004657671196796315 \tValidation loss: 0.007387033562572971\n","Epoch #479\tTraining loss: 0.004545124258817459 \tValidation loss: 0.00875040341120149\n","Epoch #480\tTraining loss: 0.004387477538611181 \tValidation loss: 0.03396878177390751\n","Epoch #481\tTraining loss: 0.004279110053931317 \tValidation loss: 0.01238255976172242\n","Epoch #482\tTraining loss: 0.00425418934565939 \tValidation loss: 0.016192630377628834\n","Epoch #483\tTraining loss: 0.004240738892594316 \tValidation loss: 0.036022648941441984\n","Epoch #484\tTraining loss: 0.004223961492188307 \tValidation loss: 0.04354546748500625\n","Epoch #485\tTraining loss: 0.004217306713206093 \tValidation loss: 0.038668691319554985\n","Epoch #486\tTraining loss: 0.004217364466921795 \tValidation loss: 0.029166375237396815\n","Epoch #487\tTraining loss: 0.004223302088460893 \tValidation loss: 0.015929336414529403\n","Epoch #488\tTraining loss: 0.004242925657070332 \tValidation loss: 0.016176585251978726\n","Epoch #489\tTraining loss: 0.004265499316568722 \tValidation loss: 0.02072336413701021\n","Epoch #490\tTraining loss: 0.004294158417023353 \tValidation loss: 0.014601332879437365\n","Epoch #491\tTraining loss: 0.004301695121426737 \tValidation loss: 0.01604280575013486\n","Epoch #492\tTraining loss: 0.004263667015083425 \tValidation loss: 0.02346520021517126\n","Epoch #493\tTraining loss: 0.00422668838407588 \tValidation loss: 0.0240562619449383\n","Epoch #494\tTraining loss: 0.004194958536402249 \tValidation loss: 0.016574437543797574\n","Epoch #495\tTraining loss: 0.0042076839895658445 \tValidation loss: 0.01684149642875734\n","Epoch #496\tTraining loss: 0.00427345165805638 \tValidation loss: 0.05607227014751208\n","Epoch #497\tTraining loss: 0.00442727257297158 \tValidation loss: 0.011924322496385344\n","Epoch #498\tTraining loss: 0.004524038338083484 \tValidation loss: 0.07332810567607803\n","Epoch #499\tTraining loss: 0.004740578039493271 \tValidation loss: 0.014363761759114302\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qWBGaZfwmkGZ","colab_type":"text"},"source":["# Train Data model"]},{"cell_type":"code","metadata":{"id":"fE4qPtvwmoJz","colab_type":"code","colab":{}},"source":["#!python3 train.py -m data -a train -d ../Images/o1_marked/"],"execution_count":0,"outputs":[]}]}